# Discrete Random Variables
As behavioral researchers, we are often studying a population of something. We might be interested in the percentage of individuals who like cats or the number of cats owned by the average individual. We might be interested in the average happiness level of a population of individuals and/or what makes them happy. The populations we study are often vaguely defined, e.g. as in the population of people who have received some experimental treatment. Here, we will begin to take these somewhat vague notions and make them concrete. It turns out that, when we are studying a population, we are really studying what statisticians call a *random variable*. In this chapter, we will pay particular attention to the *discrete random variable*. 

To develop an initial understanding of random variables, it will be useful to start with an example. When we are in the process of conducting quantitative research (and statistics), we measure things in numbers. You may have noticed that our previous discussions of discrete sample spaces used non-numeric descriptions - e.g., $S = \{H, T\}$ in the case of a coin flip, etc. In practice, we shall map the values in a discrete sample space to numbers. This makes them easier to work with and describe. Moving back to our frequently referenced example of a coin flip, we might map heads to the value of 1 and tails to the value of 0. Our set of outcomes would then be described as $S = \{1,0\}$. On a given random experiment within this space, the outcome may therefore be a 1 or 0 with each number corresponding to heads or tails, respectively. 

Plainly speaking, what we have just described is a *discrete random variable* - a variable that assumes a _number_ as a result of randomness. We shall map the members of our sample space to numbers. The set of these numbers is called the _space_ of the random variable, and like the discrete sample space, the outcomes in the space of a discrete random variable are either finite or countable. Oftentimes, discrete random variables - and random variables more generally - are referred to by the capital letter $X$. This variable will assume an outcome at the resolution of some random process, and for the sake of clarity, a potential _outcome_ of this randomness is referred to symbolically by a lower case $x$. 

Indeed, because $X$ is a random variable, we shall need a concrete way of describing it's randomness. Recall in the last chapter, we discussed the probability set operator - $P()$. This operator assigns the probability to an outcome - $x$ - on the basis of a set of rules. At the time, I told you that we would discuss this "set of rules" later. Later is now. As we noted, the set of rules for the assignment of probability is called a probability distribution. In the case of a discrete random variable, this distribution is called a *probability mass function* (pmf) - $p(x)$ - where $x$ is used to reference a particular outcome in the space of $X$. 

Suppose we are looking to determine the probability of random variable $X$ attaining a specific value - $x$ - let's say, for example, $\{x|X = 1\}$. Here, $\{x|X = 1\}$ describes the set of outcomes ($x$) in the space of $X$ where the random experiment results in 1. Indeed, it's a small set composed of exactly one value - $\{1\}$. With regards to the probability set operator, we might write $P(\{x|X = x\})$, but that's a pain. Instead, we will use the shorthand $P(X = 1)$, or more generally $P(X = x)$. Moreover, you might also be interested in a larger set described by an inequality - e.g. $P(X < 2)$ - the probability of all the outcomes in the space of $X$ where our random experiment results in an outcome less than 2. 

The set of rules the probability set operator uses (a.k.a, a probability mass function) are frequently described like this:

$$
P(X=x) = p(x) = 
\begin{cases}
\frac{1}{2} & x = 1 \\
\frac{1}{2} & x = 0
\end{cases}
$$

Note that $P(X = x)$ and $p(x)$ mean the same thing - hence the equal sign. After the large curly-brace, you will see a row for each outcome in the space of $X$. In that row you will find the probability for that outcome. For example, $P(X = 1) = p(1) = 1/2$. The values of $x$ for which $p(x) > 0$ are called the *support* of $X$. The pmf of $X$ has two important properties. Specifically:

1. All the outcomes $x$ in the space of $X$ have a probability $0 \leq p(x) \leq 1$. That is, all the outcomes in the space of $X$ must have a probability from 0 to 1. 

2. Given the space $S$ of a random variable, $P(S)$ = 1. In other words, if we are conducting a random experiment by drawing a value from the space of $X$ (the set of all numeric outcomes for $X$), the probability that it lands in $S$ is 1. This hearkens back to \@ref(eq:setprop) in the previous section.

In plain language these two properties mean that we can define the pmf of a random variable by taking a total of one unit of probability and spreading it across a set of outcomes (the support of $X$). For example, you might give .25 probability to a value of 1, .5 to a value of 2, and .25 to a value of 3. Each outcome would have a value from 0 to 1, and their respective probabilities would sum to 1. 

As previously stated, in this chapter we will dig into discrete random variables:
1. We shall start by discussing the independence of random variables - with the goal of framing an intuition for the concept.
2. We will then turn our attention to things researchers might want to know about discrete random variables (and random variables more generally)
3. And finally, we will discuss some random variables that are frequently found in behavioral research.

## Independence of discrete random variables
The concept of independence is found frequently within the field of statistics. As you read on, you will find that it is essential to the derivation, understanding, and validity of some of the most fundamental things we shall learn in statistics. Therefore, it is nice to have a deeper understanding of it. The goal if this section is to frame an intuition of the independence of random variables. A deeper mathematical understanding shall be saved for your inevitable foray into mathematical statistics (no doubt after being inspired by this book to master the topic).

In the last chapter, we discussed the concept of the independence of _events_ in a random experiment. We shall now discuss independence as it pertains to random variables. Suppose we have two random variables $X_1$ and $X_2$. Then we say the two events are independent if

$$
P(X_1 = x_1 \cap X_2 = x_2) = P(X_1 = x_1)P(X_2 = x_2)
(\#eq:descreteind)
$$
In other words, if two random variables are independent, the probability of $X_1 = x_1$ AND $X_2 = x_2$ is equal to the product of their respective probabilities. We've already seen examples of this. Recall example 1.20, we flipped three coins _independently_. If we mapped   




