# Discrete Random Variables
As behavioral researchers, we are often studying a population of something. We might be interested in the percentage of individuals who like cats or the number of cats owned by the average individual. We might be interested in the average happiness level of a population of individuals and/or what makes them happy. The populations we study are often vaguely defined, e.g., as in the population of people who have received some experimental treatment. Here, we will begin to take these somewhat vague notions and make them concrete. It turns out that, when we are studying a population, we are really studying what statisticians call a **random variable**. In this chapter, we will pay particular attention to the **discrete random variable**.

## What is a discrete random variable?

When we are in the process of conducting quantitative research (and statistics), we measure things in numbers. You may have noticed that our previous discussions of discrete sample spaces used non-numeric descriptions - e.g., $S = \{H, T\}$ in the case of a coin flip. To make these sample spaces easier to work with, we will map our discrete outcomes to numbers. For example, if we were interested in a coin flip, we wouldn't work directly with the values of $H$ and $T$. We would map each outcome to a value, e.g., heads to the value of 1 and tails to the value of 0. On any given coin flip, the value of our outcome would therefore be a number. A variable like this - one that assumes a _number_ as a result of some random process (e.g., a flip, a random experiment, drawing out of a hat, rolling, randomly sampling, etc.) - is called a **discrete random variable**. In this section, we'll provide some key definitions and notation for discrete random variables. This will make things more concrete and clear for you and make the interesting stuff more interesting. 

The set of possible numbers a random variable can take is called the **space** of the random variable, and like the discrete sample space, these outcomes shall be either finite or countable. In general we refer to a random variable with a capital letter, typically $X$, and a potential _outcome_ of this random variable is referred to symbolically by a lower case letter, e.g., $x$. For convenience, events in the space of a random variable are often referenced with a simplified set notation. Suppose we are describing the set of outcomes where our random variable $X$ attains a particular value. We will write $X = x$. For example, $X = 1$ describes the outcomes in the space of $X$ where our random variable attains a value of 1 - $\{1\}$. We may also use an inequality - e.g., $X \leq x$ - to reference a range of outcomes.   

Because $X$ attains a value from a random process, we'll need a concrete way of describing it's randomness. Recall in the last chapter, we discussed the probability set operator - $P()$. This operator assigns the probability to an outcome - $x$ - on the basis of a set of rules. The set of rules for the assignment of probability is called a probability distribution, and in the case of a discrete random variable this distribution is called a **probability mass function** (pmf) - $p(x)$. 

In practice, the set of rules (a.k.a, a probability mass function) is frequently described like this:

$$
P(X=x) = p(x) = 
\begin{cases}
\frac{1}{2} & x = 1 \\
\frac{1}{2} & x = 0
\end{cases}
$$

Note that $P(X = x)$ and $p(x)$ mean the same thing - hence the equal sign. After the large curly-brace, you will see a row for each outcome in the space of $X$. In that row you will find the probability for that outcome. For example, $P(X = 1) = p(1) = 1/2$. The pmf of $X$ has two important properties. Specifically:

1. All the outcomes $x$ in the space of $X$ have a probability $0 \leq p(x) \leq 1$. That is, all the outcomes in the space of $X$ must have a probability from 0 to 1. The set of outcomes for which $p(x) > 0$ is called the **support** of $X$.

2. Given the space $S$ of a random variable, $P(S)$ = 1. In other words, if we are conducting a random experiment by drawing a value from the space of $X$ (the set of all numeric outcomes for $X$), the probability that it lands in $S$ is 1. This hearkens back to \@ref(eq:setprop) in the previous section.

In plain language these two properties mean that we can define the pmf of a random variable by taking a total of one unit of probability and spreading it across a set of outcomes (the support of $X$). For example, you might give .25 probability to a value of 1, .5 to a value of 2, and .25 to a value of 3. In this case, each outcome would have a probability from 0 to 1, and these values would sum to 1. 

In this chapter we will dig into discrete random variables in three steps:

1. We shall start by discussing the independence of random variables - with the goal of framing an intuition for the concept.

2. We will then turn our attention to things researchers might want to know about discrete random variables (and, as you will learn, random variables more generally).

3. And finally, we will discuss some random variables that are frequently found in behavioral research.

## Independence of discrete random variables
The concept of independence is found frequently within the field of statistics. As you read on, you will find that it is essential to the derivation, understanding, and validity of some of the most fundamental things we shall learn in statistics. Therefore, it is nice to have a deeper understanding of it. The goal of this section is to frame an intuition of the independence of random variables. A deeper mathematical understanding shall be saved for your inevitable foray into mathematical statistics (no doubt after being inspired by this book to master the topic).

In the last chapter, we discussed the concept of the independence of _events_ in a random experiment. We shall now discuss independence as it pertains to random variables. And, as you will see, it is very similar to the concept of independence for events. Suppose we have two random variables $X_1$ and $X_2$. Then we say the two variables are independent if

$$
P(X_1 = x_1 \cap X_2 = x_2) = P(X_1 = x_1)P(X_2 = x_2)
(\#eq:descreteind)
$$
In other words, if two random variables are independent, the probability of $X_1 = x_1$ AND $X_2 = x_2$ is equal to the product of their respective probabilities. We've already seen examples of this. Recall in example \@ref(exm:coins), we flipped three coins _independently_. If we mapped the outcomes of each coin to 1 and 0 for heads and tails, respectively, then the three resulting random variables are independent in the sense of \@ref(eq:descreteind).

With some algebraic manipulation of \@ref(eq:descreteind), we can see that the definition of independence for random variables is very similar to the definition of independence for events

$$
\frac{P(X_1 = x_1 \cap X_2 = x_2)}{P(X_2 = x_2)}=P(X_1 = x_1)
(\#eq:descreteind2)
$$
And, perhaps not surprisingly (although it's okay if you are surprised), if the two random variables are not independent then

$$
\frac{P(X_1 = x_1 \cap X_2 = x_2)}{P(X_2 = x_2)} = P(X_1 = x_1|X_2 = x_2) \neq P(X_1 = x_1)
$$
Put into the logic and hopefully more understandable intuition of last chapter, when two random variables are dependent, an event in the space of one variable _changes_ the probability of an event in the space of another variable. This may seem a bit confusing at first, but you have already seen an example of the dependence of random variables. Recall example \@ref(exm:dice), when we looked at the sum of two dice. In this case, let's call the sum of our two dice the random variable $Z$ and represent the random variable of the first die by $X$. 

On the basis of \@ref(eq:descreteind), we shall be looking to determine if

$$
\frac{P(Z = z \cap X = x)}{P(X = x)} \neq P(Z = z)
$$

The first thing we will need is $P(Z = z)$. The space of $Z$ are all the possible sums of our two dice - $\{2,3,4,5,6,7,8\}$. Using the table we created in example \@ref(exm:dice), and the law of addition, we can determine a probability distribution for $Z$ (try it yourself and see if you get the same thing as below)

$$
P(Z=z) = p(z) = 
\begin{cases}
1/16 & z = 2 \\
2/16 & z = 3 \\
3/16 & z = 4 \\
4/16 & z = 5 \\
3/16 & z = 6 \\
2/16 & z = 7 \\
1/16 & z = 8
\end{cases}
$$

After our computation, we can see that the most probable value is 5. Because we haven't had much experience with random variables, it is also important to point out that this distribution has all our previously discussed properties of a pmf. The probabilities of all the outcomes in the space of $Z$ sum to 1, and they are constrained between 0 and 1. 

Now that we have determined the probability mass function for $Z$, we shall calculate $(P(Z = z|X = x))$. Hopefully the idea of independence/dependence of random variables will come into focus. Consider the case where $X = 1$ (a 1 was rolled on the first die). What does the distribution of $P(Z = z|X = 1)$ look like? To determine this we shall calculate the conditional probability of each Z - _given_ our first dice roll is a 1. We did this back in example \@ref(exm:dice) for the specific value of $Z = 3$, and now we shall do it here for every value (try it yourself)

$$
P(Z=z|X=1) = p(z|x=1) = 
\begin{cases}
1/4 & z = 2 \\
1/4 & z = 3 \\
1/4 & z = 4 \\
1/4 & z = 5 \\
0 & z = 6 \\
0 & z = 7 \\
0 & z = 8
\end{cases}
$$

Do you see what happens with the event $X = 1$? The probability of $Z$ _changes_ with a roll of a 1 on the first die. For example, getting a $Z = 2$ now has a whopping 1/4 chance, compared to a paltry 1/16 before the first dice was rolled. Moreover, the probability is now spread across only four values - $\{2,3,4,5\}$. That is, the support of $Z$ - given $X = 1$ - is now 2, 3, 4, and 5. Thus, an intuitive definition of independence pivots on changes in the pmf of one random variable given the outcome of another variable. 

This idea of independence shall be sufficient for our purposes, especially as we become more sophisticated in statistics and statistical analysis. As you will see, some of the most fundamental statistical concepts require an _assumption_ of independence. In many cases, we shall assume that our random variables are independent to take advantage of some convenient mathematical properties associated with independence. We'll see one of these convenient properties as soon as this chapter.

## What do we want to know about discrete random variables?
A new statistics student will have made it to this point and reasonably wondered: what the heck is the point of all this back story? This skepticism is understandable, welcomed, and hopefully motivating. In this section, we shall discuss some things - as statisticians and researchers - that we might want to know about random variables. In doing so, hopefully the bigger picture comes into view. This, in my humble opinion, is where things really start to get interesting. I'm hoping you feel the same. 

But first, let's do a very very short review.

### A very quick review: The summation operator
As statisticians we will need to sum a lot of things. It will therefore be useful to have an easy way to describe the operation of summation. To that end, suppose we are summing a set of numbers

$$
x_1 + x_2 + x_3 + x_4 + x_5 \,+\:... + x_n
$$

The **summation operator** helps us write this operation more efficiently and with less ambiguity:

$$
\sum_{i=1}^{n} x_i
(\#eq:summation)
$$
The $i$ at the bottom of the $\sum{}^{}$ is referred to as the _index_. The index is how we refer to a single element in our set of $x$'s. For example, $i=5$ refers to $x_5$. The starting index is specified at the bottom of the summation operator to the right of the equal sign. In the equation above we are starting at the 1st element of our set of $x$'s. The value at the top of the summation operator is the index of the last element to be summed. Here we have written $n$ which is the last element of our set of n $x$'s. In other words

$$
\sum_{i=1}^{n} x_i = x_1 + x_2 + x_3 + x_4 + x_5 + ... + x_n
$$
in terms of our example above. Additionally, it is important to note that n and i are not always used to indicate the last element and the index, respectively. Here we shall try to be consistent. Occasionally, to save a little time we shall write the summation operator like

$$
\sum_{x}^{}
$$

This means - given all the $x's$ in a set - sum all of them. Alright, now it's time for the interesting stuff.

### The expectation of a discrete random variable
The first thing that we shall want to know about a discrete random variable is its expectation. Given a random variable $X$, we refer to its expectation as $E(X)$. We shall also sometimes refer to it symbolically as $\mu$. In other words

$$
E(X) = \mu
$$

The expectation is commonly referred to as the **mean** of a random variable. The expectation of a discrete random variable $X$ with a given pmf - $p(x)$ is calculated 

$$
E(X) = \sum_{i = 1}^{n} x_ip(x_i)
(\#eq:discreteexpectation)
$$

In words: For each $x$ in the space of $X$, multiply by its corresponding probability and sum the resulting values. An example, will help us understand the expectation a bit better.

::: {.example #ogexpectation} 
Suppose we have a random variable $X$ with five outcomes in it's space - $\{1,2,3,4,5\}$. Further assume that the pmf of $X$ assigns $\frac{1}{5}$ probability to each outcome. 

$$
P(X=x) = p(x) = 
\begin{cases}
1/5 & x_1 = 1 \\
1/5 & x_2 = 2 \\
1/5 & x_3 = 3 \\
1/5 & x_4 = 4 \\
1/5 & x_5 = 5 
\end{cases}
$$
This is an example of a **discrete uniform distribution**. Given n outcomes, the discrete uniform distribution assigns $1/n$ probability to each outcome on the space of the random variable.

Now let's calculate the expectation of this random variable.

$$
\begin{align}
E(X) = \sum_{i=1}^{5} x_ip(x_i) &= x_1p(x_1)+x_2p(x_2)+x_3p(x_3)+x_4p(x_4)+5p(x_5) \\
&= 1p(1)+2p(2)+3p(3)+4p(4)+5p(5) \\
&= (1)\frac{1}{5}+(2)\frac{1}{5}+(3)\frac{1}{5}+(4)\frac{1}{5}+(5)\frac{1}{5} \\
&= \frac{15}{5} = 3
\end{align}
$$
We've multiplied each value in the space of our random variable $X$ by its probability and summed the values. The result of our computation is 3 - $E(x) = 3$ - but what does that mean exactly? Well let's explore the idea of expectation further by plotting the pmf of $X$. In the figure below, the probability of each outcome is plotted. You can see the outcomes of the random variable labelled on the $x$ axis, and probability of each is represented by the bar height. In this case, each outcome is equally likely, so the bars are of equal height.  

```{r, echo = FALSE}
a <- data.frame(x = c(1,2,3,4,5), 
                y = rep(1/5, times = 5))
                

ggplot2::ggplot(a, ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_col(fill = 'gray', color = 'black') +
  ggplot2::scale_y_continuous(limits = c(0,1)) +
  ggplot2::theme(
                 panel.grid.minor = ggplot2::element_blank()) +
  ggplot2::labs(x = 'Outcome', y = 'Probability Mass') +
  ggplot2::geom_vline(xintercept = 3)


```

Now suppose each of those bars has _mass_ (a.k.a. probability mass), and further suppose we'll place our wedge under the $x$ axis such that the mass of the plot balances perfectly. Just by looking at the plot, if we put a wedge directly under the value of 3, there would be equivalent probability mass on either side, and the plot would balance perfectly. Notice that the value of 3 was the result of our expectation computation. $E(X)$ can therefore be thought as the balancing point of the plot - the place that would perfectly balance the mass of probability for our random variable.
:::

That said, we shall need to learn a little bit more about the expectation to proceed on our statistical adventure. First let's observe that it is possible to take the expectation of some _function_ of a random variable. In other words, we may apply some function to the outcomes in the space of $X$ and then calculate an expectation. For example, we might have

$$
f(X) = 2X
$$
This function takes the outcomes in the space of $X$ and multiplies each by 2. We shall calculate the expectation of this function of our random variable by 

$$
E(f(X)) = \sum_{i = 1}^{n} f(x_i)p(x_i)
(\#eq:discreteexpectationfunction)
$$

Let's try it out in a couple examples.

::: {.example #exp1}
We will again use the distribution from example \@ref(exm:ogexpectation). Here we will keep it simple (and also demonstrate a property of multiplication you may recall from algebra). Suppose our function $f(x)$ takes every value in the space of $X$ and returns the value one. In other words,

$$
f(X) = 1
$$
Let's apply \@ref(eq:discreteexpectationfunction) to find the expectation of this function. First, let's remind ourselves of the distribution of $X$ and calculate $f(x)$ for each outcome in it's space

$$
p(x) =
\begin{cases}
1/5 & x_1 = 1 & f(1) = 1 \\
1/5 & x_2 = 2 & f(2) = 1 \\
1/5 & x_3 = 3 & f(3) = 1 \\
1/5 & x_4 = 4 & f(4) = 1 \\
1/5 & x_5 = 5 & f(5) = 1 
\end{cases}
$$
Now let's calculate the expectation of $f(X)$. Watch the underlined values for a little bit of algebra review.

$$
\begin{align}
E(X) = \sum_{i=1}^{5} f(x_i)p(x_i) &= \underline{1}\:p(x_1)+\underline{1}\:p(x_2)+\underline{1}\:p(x_3)+\underline{1}\:p(x_4)+\underline{1}\:p(x_5) \\
&= \underline{1}\:[p(x_1) + p(x_2) + p(x_3) + p(x_4) + p(x_5)] \\
&= \underline{1}\:[\frac{1}{5} + \frac{1}{5} + \frac{1}{5} + \frac{1}{5} + \frac{1}{5}] \\
&= \underline{1}(1) = 1
\end{align}
$$
The second line of this calculation was made possible by the distributive property of multiplication. That made the computation easier because we note that the sum of probabilities over the space of $X$ is equal to 1 (see the third line; and see the properties of a discrete random variable at the start of this chapter).
:::

::: {.example #exp2}
Suppose our random variable $X$ is again defined as in example \@ref(exm:ogexpectation). Now let's use the function $f(x)= 2X$. Then 
$$
p(x) = 
\begin{cases}
1/5 & x_1 = 1 & f(1) = 2(1) = 2 \\
1/5 & x_2 = 2 & f(2) = 2(2) = 4 \\
1/5 & x_3 = 3 & f(3) = 2(3) = 6 \\
1/5 & x_4 = 4 & f(4) = 2(4) = 8 \\
1/5 & x_5 = 5 & f(5) = 2(5) = 10 
\end{cases}
$$

$$
\begin{align}
E(2X) = \sum_{i=1}^{5} f(x_i)p(x_i) &= 2p(x_1)+4p(x_2)+6p(x_3)+8p(x_4)+10p(x_5) \\
&= (2)\frac{1}{5} + (4)\frac{1}{5} + (6)\frac{1}{5} + (8)\frac{1}{5} + (10)\frac{1}{5} \\
&= \frac{30}{5} = 6
\end{align}
$$
:::

The astute observer might notice an interesting trend in the last two examples. In example \@ref(exm:exp1), we saw that our function which converts the whole space of $X$ into 1 had the expectation of 1. Indeed, looking back at the computation, we could perform that computation for any constant, e.g., if $a$ is a constant, and $f(X) = a$, then

$$
\begin{align}
E(a) = \sum_{i=1}^{5} ap(x_i) &= \underline{a}\:p(x_1)+\underline{a}\:p(x_2)+\underline{a}\:p(x_3)+\underline{a}\:p(x_4)+\underline{a}\:p(x_5) \\
&= \underline{a}\:[p(x_1) + p(x_2) + p(x_3) + p(x_4) + p(x_5)] \\
&= \underline{a}\:[\frac{1}{5} + \frac{1}{5} + \frac{1}{5} + \frac{1}{5} + \frac{1}{5}] \\
&= \underline{a}(1) = a
\end{align}
$$

In other words, we would get the value of our constant every time. Similarly, in \@ref(exm:exp2), recall that the original expectation of $X$ was 3. Our function $f(X) = 2X$ had the expectation of $2\times3 = 6$. That is, we multiplied $X$ by 2 and got back 2 times its expectation.

Our last two observations highlight a couple of _properties_ of the expectation. We'll list a few below. The first two (or three) you might have deduced yourself. The fourth is new.

::: {.theorem #expprop name="Some Properties of the Expectation"}
If $a$ and $b$ are constants, then

1. $E(b) = b$

2. $E(aX) = aE(X)$

3. More generally, $E(aX + b) = aE(X) + E(b)$

If $X_1$ and $X_2$ are random variables, then 

4. $E(aX_1 + bX_2)$ = $aE(X_1) + bE(X_2)$
:::

These are called properties because they apply to any random variable - assuming that random variable has an expectation (a more advanced topic that we won't discuss in much depth in this book). They allow us to perform computations with expectations without knowing much about the actual distribution of the random variable. And, they make computations easier. As you will see later, they will help us draw certain conclusions about the populations we study. For now, let's see how they work in practice and look at a more thorough example for the fourth.

::: {.example}
Suppose we are given a random variable - $Z$ - and $E(Z) = 7$. What is the expectation of

1. $4Z$

2. $Z + 10$

3. $1$

4. $4Z+10$

We will approach each of these in turn.

1. Property 2 tells us that $E(aX) = aE(X)$, and in this case $a = 4$

$$
E(4Z) = 4E(Z) = 4(7) = 28
$$

Note that property 3 also applies here with $b = 0$.

2. Property 3 tells us that $E(aX + b) = aE(X) + b$, so in this case we have $a = 1$ and $b = 10$  

$$
E(Z + 10) = (1)E(Z) + 10 = 7 + 10 = 17
$$

3. The first property tells us that $E(c) = c$, so 

$$
E(1) = 1
$$ 

Note that property 3 also applies here with $a = 0$ and $b = 1$.

4. Similar to number 2, property 3 applies with $a = 4$ and $b = 10$

$$
E(4Z + 10) = E(4Z) + 10 = 4E(Z) + 10 = 28 + 10 = 38
$$
:::

Do you see how that made things a bit easier? We didn't have to go through the computation of the expectation of our function of $Z$. I bet you can tell by now I like to avoid long computations! Now let's turn our attention towards property 4.

::: {.example #expectationsum}
Suppose we flip two coins - where a flip of $H$ yields a value of 1 and a flip of $T$ yields a value of zero. Call these two random variables $X_1$ and $X_2$, respectively, and further assume that they are independent (recall what that means?). What is the expectation of their sum? To demonstrate why property four is so useful, we'll do this the hard way and then the easy way.

First note that the pmf of $X_1$ and $X_2$ is

$$
p(x_1) = 
\begin{cases}
1/2 & x_1 = 1 \\
1/2 & x_1 = 0 \\
\end{cases}
\\
p(x_2) = 
\begin{cases}
1/2 & x_2 = 1 \\
1/2 & x_2 = 0 \\
\end{cases}
$$
Without the use of property 4, these pmfs won't suffice to calculate the expectation of $X_1 + X_2$. We'll have to figure out the pmf of the sum of the two random variables, a.k.a., $Z = X_1 + X_2$. Note that there are four possible outcomes in the sample space of our two coin flips = ${HH, HT, TH, TT}$. One outcome in this sample space that would lead to a sum of two - ${HH}$. In this case, both values of $X_1$ and $X_2$ would be 1 so 

$$
X_1 + X_2 = 1 + 1 = 2
$$
Because our random variables are independent, we can find the probability of this outcome using the law of multiplication for independent events

$$
P(X_1 = 1 \cap X_2 = 1) = P(X_1 = 1)P(X_2 = 1) = (1/2)(1/2) = 1/4
$$
We can do this calculation for each of the other outcomes in the sample space of our coin flips. The table below breaks down each. It is most easily read from left to right. Find the value of the first flip in the first column. For example, the big top row has a first flip of 1 (Heads). Next scan to the right to see outcome of the second flip, which can either be a 1 or a 0. Scan further to the right to see the sum and probability of the outcome. For example, if we flipped a 1 on the first coin, and then a zero on the second coin, the sum of the two would be 1, and the probability would be 1/4.

```{r, echo = FALSE}
x <- rep(c(1,0), each = 2)
y <- rep(c(1,0), times = 2)
q <- x + y
z <- rep('1/4', times = 4)

sample_space <- data.frame(x,y,q,z)
names(sample_space) <- c('1st Flip', '2nd Flip','Sum (Z)', 'Probability')

x <- flextable::flextable(sample_space)
x <- flextable::merge_v(x, j = 1)
x <- flextable::hline(x, i = c(2,4))
x <- flextable::align(x, align = 'center', part = 'body')
x <- flextable::align(x, align = 'center', part = 'header')
x
```

Looking over this table, we see that our new random variable $Z$ has the space of ${0,1,2}$. There is only 1 way to get a 0 or 2, respectively, and each of those outcomes has a probability of $1/4$. There are two ways to get a 1, so by the law of addition, this outcome in the space of $Z$ has a probability of $1/4 + 1/4 = 1/2$. In other words

$$
p(z) = 
\begin{cases}
1/4 & z = 0 \\
1/2 & z = 1 \\
1/4 & z = 2
\end{cases}
$$
Now, after that slog (you can imagine it being much worse), are in a position to calculate the expectation of $Z = X_1 + X_2$

$$
\begin{align}
E(a) = \sum_{z}^{} zp(z) &= 0\:p(x_1)+1\:p(x_2)+2\:p(x_3) \\
&= 0\frac{1}{4} + 1\frac{1}{2} + 2\frac{1}{4} = \frac{2}{4} + \frac{2}{4} = 1 \\
\end{align}
$$

The usefulness of the fourth property now comes into view. Observe that $E(X_1) = E(X_2) = 1/2$ (calculate this on your own; you can do it!). So, by property 4

$$
E(X_1 + X_2) = E(X_1) + E(X_2) = 1/2 + 1/2 = 1
$$
Easier!

:::

We shall end this section with a word of caution. Some beginning statistics students may understandably read property 4 and think it applies to other arithmetic operations. For example they may think that $E(XY) = E(X)E(Y)$. This is true in some circumstances but it is not true in general. The properties of the expectation apply _specifically_ to the circumstances outlined by each property. You may get into trouble to go beyond that.

### The variance of a discrete random variable
We have just dedicated a bit of time to the expectation of a random variable. Another important thing we will want to know as statisticians is the **variance** of a random variable. The variance of a random variable $X$ is referred to as $Var(X)$ or symbolically as $\sigma^2$. The square root of the variance - $\sigma$ - is called the **standard deviation**. In terms of discrete random variables, the variance is calculated

$$
Var(X) = E((X-\mu)^2) = \sum_{x}^{} (x - \mu)^2p(x)
(\#eq:discretevariance)
$$

It can be thought of as a measure of how "spread" out the probability mass function is for the random $X$ (a.k.a. the _dispersion_). We'll look at some distributions later to make this a bit more concrete. There are indeed many ways you can measure the dispersion of a random variable. New students, for example, often wonder why we don't just calculate $E(X - \mu)$ instead of squaring the difference as in \@ref(eq:discretevariance). Let's do a quick calculation to convince ourselves why this isn't a good idea.

::: {.example}
In particular, we shall calculate $E(X-\mu)$. This will have the added bonus of helping us review properties of the expectation. First notice that $\mu$ is a constant, so

$$
E(X-\mu) = E(X) - E(\mu) = E(X) - \mu
$$

Now observe that $E(X) = \mu$, so

$$
E(X) - \mu = \mu - \mu = 0
$$
$E(X - \mu)$ is zero for every random variable (that has an expectation) which is not a very good measure of dispersion!
:::

::: {.example #firstvariance}
With that minor point out of the way, let's give the variance equation some run. Suppose our random variable $X$ has the pmf

$$
p(x) = 
\begin{cases}
1/8 & x_1 = 0 \\
3/4 & x_2 = 1 \\
1/8 & x_3 = 2
\end{cases}
$$

Reviewing the equation for the variance, it is apparent we shall first need to calculate $\mu$ for $X$ before we can do any calculation of the variance. Knowing what we know about the expected value and the balancing point of probability mass, I think we would be pretty safe to guess that the expectation of $X$ is 1. Let's calculate it really quick

$$
\begin{align}
E(X) = \sum_{i=1}^{3} x_ip(x_i) &= 0\frac{1}{8}+ 1\frac{3}{4} + 2\frac{1}{8} \\
&= \frac{6}{8} + \frac{2}{8} = 1
\end{align}
$$

Our intuition was indeed correct - $E(X) = 1$. We shall now turn our attention to calculating the variance of $X$

$$
\begin{align}
Var(X) = \sum_{i=1}^{3} (x_1-\mu)^2p(x_i) &= (x_1-\mu)^2p(x_1) + (x_2-\mu)^2p(x_2) + (x_3-\mu)^2p(x_3) \\
&= (0 - 1)^2\frac{1}{8} + (1-1)^2\frac{1}{8} + (2-1)^2\frac{1}{8} \\
&= (-1)^2\frac{1}{8} + 0 + 1^2\frac{1}{8} = \frac{1}{4}
\end{align}
$$
Thus, $Var(X) = 1/4$. 

We should stop here and take a breath. In the last two sections, you've calculated your very first mean and variance of a random variable. Congratulations! These concepts may seem a little disconnected from our work as behavioral scientists, but they will make some wonderful things clear later on.
:::

Now let's get back to work. Just like the expectation, we can also calculate the variance of some function of $X$. First calculate $E(f(X))$, referred to here as $\mu_f$, then 

$$
Var(X) = E((f(X)-\mu_f)^2) = \sum_{x}^{} (f(x) - \mu_f)^2p(x)
(\#eq:discretevariancef)
$$

To demonstrate how this equation works we'll use the same pmf as example \@ref(exm:firstvariance).

::: .{example}
Suppose we want to find the variance of $f(X) = 2X$. Let's first multiply all the outcomes in the space of $X$ by 2

$$
p(x) = 
\begin{cases}
1/8 & x_1 = 0 & f(x_1) = (2)0 = 0 \\
3/4 & x_2 = 1 & f(x_2) = (2)1 = 2\\
1/8 & x_3 = 2 & f(x_3) = (2)2 = 4
\end{cases}
$$
Next, in order to calculate $Var(2X)$, we will first need $E(2X)$. I'll just go ahead and give you the fact that $E(2X) = 2$, but you should calculate this for yourself as well (you have all the tools, skill, and excitement to do it!). Next we'll apply our new equation

$$
\begin{align}
Var(2X) = \sum_{i=1}^{3} (f(x_i)-\mu_f)^2p(x_i) &= (f(x_1)-\mu_f)^2p(x_1) + (f(x_2)-\mu_f)^2p(x_2) + (f(x_3)-\mu_f)^2p(x_3) \\
&= (0 - 2)^2\frac{1}{8} + (2-2)^2\frac{1}{8} + (4-2)^2\frac{1}{8} \\
&= (-2)^2\frac{1}{8} + 0 + 2^2\frac{1}{8} = 1
\end{align}
$$
Observe that the variance of our random variable $2X$ is larger than $X$. Let's take a look at a graphical depiction of the pmfs of $2X$ and $X$. This will help to bring the idea of variance home. 

The top plot below is the pmf of $X$, with the vertical line representing $E(X)$. The height of each bar represents the probability mass assigned to a particular value on the $x$-axis - in the space of $X$. For example, $1/8$ mass sits on top of the value 0. The bottom plot is the pmf of $2X$ with a vertical line representing $E(2X)$. Note how the mass for $2X$ now sits on the values of 0, 2, and 4. And, it's pmf is spread out further. It is _dispersed_ in a wider range than the probability mass of $X$. Indeed, this is an intuitive (although maybe not perfectly exact) way of thinking about variance: A random variable with a larger variance will tend to have it's probability mass spread out more. Therefore, it is not surprising that the $Var(X) < Var(2X)$. In fact, this is precisely the sort of behavior we would want out of a measure of variance!

```{r, echo = FALSE}
a <- data.frame(x = c(0,1,2,3,4), 
                y = c(1/8,3/4,1/8,0,0))
                

ggplot2::ggplot(a, ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_col(fill = 'gray', color = 'black') +
  ggplot2::scale_y_continuous(limits = c(0,1)) +
  ggplot2::theme(
                 panel.grid.minor = ggplot2::element_blank()) +
  ggplot2::labs(x = 'Outcome', y = 'Probability Mass (X)') +
  ggplot2::geom_vline(xintercept = 1)


```
```{r, echo = FALSE}
a <- data.frame(x = c(0,1,2,3,4), 
                y = c(1/8,0,3/4,0,1/8))
                

ggplot2::ggplot(a, ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_col(fill = 'gray', color = 'black') +
  ggplot2::scale_y_continuous(limits = c(0,1)) +
  ggplot2::theme(
                 panel.grid.minor = ggplot2::element_blank()) +
  ggplot2::labs(x = 'Outcome', y = 'Probability Mass (2X)') +
  ggplot2::geom_vline(xintercept = 2)


```
:::

We shall observe one more thing about our last two examples. In the first example we found $Var(X) = 1/4$ and in the second example, we found $Var(2X) = 1$. Indeed, you may not have noticed this quickly (I didn't when I first learned this stuff), but

$$
\begin{align}
Var(2X) &= 1 \\
&= 4\frac{1}{4} \\
&= 2^2\frac{1}{4} \\
&= 2^2Var(X)
\end{align}
$$
I might have lost you there. In words, it looks like the variance of $2X$ is simply $2^2$ times the variance of $X$. Indeed this is true in general (for random variables that have a variance). Like the expectation, the variance also has some useful properties. They are listed below:

::: {.theorem #expprop name="Some Properties of the Variance"}
If $a$ and $b$ are constants, then

1. $Var(b) = 0$

2. $Var(aX) = a^2Var(X)$

3. More generally, $Var(aX + b) = a^2Var(X) + Var(b) = a^2Var(X) + 0$

If $X_1$ and $X_2$ are random variables, **and they are independent**, then 

4. $Var(aX_1 + bX_2)$ = $a^2Var(X_1) + b^2Var(X_2)$
:::

Let's do some examples to get a better grasp on these.

::: {.example}
Suppose we have a random variable $X$ and we are seeking to determine the the variance of $f(X) = b$ - e.g., $Var(b)$. This function takes any value in the space of $X$ and returns a constant $b$. First take a moment to recall that $E(f(x)) = E(b) = b$ (Property 1 of the expectation). So then our variance equation boils down to:

$$
Var(X) = E((f(X)-\mu_f)^2) = \sum_{x}^{} (b - b)^2p(x) = 0
$$
Indeed, we could have just used property 1 of the variance and concluded the same thing - $Var(b) = 0$. Within the field of statistics, random variables with zero variance are called *degenerate*. These variables return a constant every time because all their probability sits on exactly one value.
:::

The next example will elucidate a bit of property 3. Specifically, we unpack why adding a constant value to a random variable doesn't change it's variance. That is to say, if we have $a = 1$ and $b=10$, then

$$
Var(aX+b) = (1)Var(X) + 0 = Var(X) 
$$

::: {.example}
Suppose we have a random variable $X$ with $E(X) = 1/2$, $Var(X) = 1/4$, and pmf

$$
p(x) = 
\begin{cases}
1/2 & x = 0 \\
1/2 & x = 1 
\end{cases}
$$
Take a moment to calculate the expectation and variance of this random variable yourself. Now let's say we would like to calculate $Var(X + 5)$. Looking at property 3 of the expectation, we have $a = 1$ and $b = 5$. Therefore

$$
Var(X + 5) = Var(X) + 0 = 1/4
$$
Why didn't adding a constant to $X$ change it's variance?  It's also easier to see why by visualizing the pmfs. In the plot below, you will see two pmfs. The first (blueish) is the pmf of $X$, and the second (redish) is the pmf of $X+5$ - as if we created a new random variable and pmf by adding 5 to $X$. The vertical lines represent $E(X)$ and $E(X+5)$ respectively. Do you see how the plot of $X+5$ is simply the pmf of $X$ shifted to the right? We can see the bars of $X+%$ are still equivalently "spread" out but at a new location on the number line. Indeed, the values in the $X+5$ pmf are just as close to their expectation compared to $X$, so the variance calculation would not turn out differently (I invite you to convince yourself of this by using equation \@ref(eq:discretevariancef).

```{r, echo = FALSE}
a <- data.frame(x = c(0,1,5,6), 
                y = c(1/2,1/2,1/2,1/2),
                grp = c('X','X','X+5','X+5'))
                

ggplot2::ggplot(a, ggplot2::aes(x = x, y = y, fill = grp)) +
  ggplot2::geom_bar(position = 'dodge', stat = 'identity', color = 'black') +
  ggplot2::scale_y_continuous(limits = c(0,1)) +
  ggplot2::scale_x_continuous(limits = c(-1,10)) +
  ggplot2::theme(
                 panel.grid.minor = ggplot2::element_blank()) +
  ggplot2::labs(x = 'Outcome', y = 'Probability Mass') +
  ggplot2::geom_vline(xintercept = 1/2, color = '#F8766D', linewidth = .75) +
    ggplot2::geom_vline(xintercept = 5.5, color = '#619CFF', linewidth = .75) 
    


```
:::

Let's do a few more examples using properties 1 through 3, and then we'll move on to the fourth.

::: {.example}
Given a random variable $X$ with $Var(X) = 10$ what is

1. $Var(5X)$

2. $Var(3)$

3. $Var(2X + 7)$

Let's unpack each in turn.

1. Property 2 says that $Var(aX) = a^2Var(X)$, so

$$
Var(5X) = 25Var(X) = 250
$$

2. Property 1 says that $Var(b) = 0$, so this one is $0$

3. Property 3 says that $Var(aX + b) = a^2Var(X) + 0$, so

$$
Var(2X + 7) = 4Var(X) + 0 = 
$$
:::

The last three examples should give you a good sense of how to use properties 1 through 3 of the variance. We now turn our attention to the fourth property. Notice how this property is very similar to the fourth property of the expectation - except there is an additional caveat. Our random variables must be _independent_.

::: {.example}
We will again work with the random variables in example \@ref(exm:expectationsum). Recall that $Z$ was the sum of two random variables $X_1$ and $X_2$ - each having an expectation of 1/2. From property 4 of the expectation we determined that $E(Z) = E(X_1 + X_2) = 1/2 + 1/2 = 1$. Now let's determine the variance of $Z$.

Recall that the variables X_1 and X_2 are independent. This means that property 4 applies, and we shall use it. First' let's calculate the variance of $X_1$ (which will also be the variance of $X_2$)

$$
\begin{align}
Var(X_1) = \sum_{x}^{} &= (0-\frac{1}{2})^2p(0) + (1-\frac{1}{2})^2p(x_2) \\
&= 0 + \frac{1}{4} \\ &= \frac{1}{4}
\end{align}
$$
And, therefore, by property 4, the variance of $Z$ is

$$
Var(Z) = Var(X_1 + X_2) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}
$$
Now consider the circumstance where we multiply $X_1$ by four before adding it to $X_2$. Then the variance of the sum of $4X_1$ and $X_2$ would be

$$
Var(4X_1 + X_2) = 16Var(X_1) + Var(X_2) = (16) \frac{1}{4} + 1/4 = \frac{17}{4}
$$
And now you see the usefulness of the fourth property of the variance. We only needed the expectations and variances of our simpler variables $X_1$ and $X_2$ to do more complicated computations. In this case, we didn't need to know the distribution or expectation of $4X_1 + X_2$ (although I bet you could figure it out by following the procedure of example \@ref(exm:expectationsum)).
:::
