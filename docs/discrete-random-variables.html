<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Discrete Random Variables | Behavioral Statistics: Theory and Application</title>
  <meta name="description" content="<p>This is a behavioral statistics textbook with additional mathematical depth
that I wish I had had prior to enrolling in my first graduate level behavior
statistics course.|</p>" />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Discrete Random Variables | Behavioral Statistics: Theory and Application" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a behavioral statistics textbook with additional mathematical depth
that I wish I had had prior to enrolling in my first graduate level behavior
statistics course.|</p>" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Discrete Random Variables | Behavioral Statistics: Theory and Application" />
  
  <meta name="twitter:description" content="<p>This is a behavioral statistics textbook with additional mathematical depth
that I wish I had had prior to enrolling in my first graduate level behavior
statistics course.|</p>" />
  

<meta name="author" content="Paul Meinz, Ph.D" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="set-theory-and-probability.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="libs/tabwid-1.1.3/tabwid.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Behavioral Statistics: Theory and Application</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#you-can-learn-statistics"><i class="fa fa-check"></i><b>0.1</b> You can learn statistics</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#why-i-wrote-this-book"><i class="fa fa-check"></i><b>0.2</b> Why I wrote this book</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#what-content-to-expect-in-this-book"><i class="fa fa-check"></i><b>0.3</b> What content to expect in this book</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#what-youll-need-to-know-to-get-the-most-out-of-this-book"><i class="fa fa-check"></i><b>0.4</b> What you’ll need to know to get the most out of this book</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#future-editions-of-this-book"><i class="fa fa-check"></i><b>0.5</b> Future editions of this book</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="set-theory-and-probability.html"><a href="set-theory-and-probability.html"><i class="fa fa-check"></i><b>1</b> Set Theory and Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="set-theory-and-probability.html"><a href="set-theory-and-probability.html#sets-and-sample-spaces"><i class="fa fa-check"></i><b>1.1</b> Sets and Sample Spaces</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="set-theory-and-probability.html"><a href="set-theory-and-probability.html#repeated-samples"><i class="fa fa-check"></i><b>1.1.1</b> Repeated Samples</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="set-theory-and-probability.html"><a href="set-theory-and-probability.html#what-is-probability"><i class="fa fa-check"></i><b>1.2</b> What is probability?</a></li>
<li class="chapter" data-level="1.3" data-path="set-theory-and-probability.html"><a href="set-theory-and-probability.html#conditional-probability-and-independence"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and Independence</a></li>
<li class="chapter" data-level="1.4" data-path="set-theory-and-probability.html"><a href="set-theory-and-probability.html#extended-examples-of-probability"><i class="fa fa-check"></i><b>1.4</b> Extended Examples of Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#what-is-a-discrete-random-variable"><i class="fa fa-check"></i><b>2.1</b> What is a discrete random variable?</a></li>
<li class="chapter" data-level="2.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#independence-of-discrete-random-variables"><i class="fa fa-check"></i><b>2.2</b> Independence of discrete random variables</a></li>
<li class="chapter" data-level="2.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#what-do-we-want-to-know-about-discrete-random-variables"><i class="fa fa-check"></i><b>2.3</b> What do we want to know about discrete random variables?</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#a-very-quick-review-the-summation-operator"><i class="fa fa-check"></i><b>2.3.1</b> A very quick review: The summation operator</a></li>
<li class="chapter" data-level="2.3.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#the-expectation-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>2.3.2</b> The expectation of a discrete random variable</a></li>
<li class="chapter" data-level="2.3.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#the-variance-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>2.3.3</b> The variance of a discrete random variable</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#some-common-discrete-distributions"><i class="fa fa-check"></i><b>2.4</b> Some common discrete distributions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#the-bernoulli-distribution"><i class="fa fa-check"></i><b>2.4.1</b> The Bernoulli distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#the-binomial-distribution"><i class="fa fa-check"></i><b>2.4.2</b> The binomial distribution</a></li>
<li class="chapter" data-level="2.4.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#the-poisson-distribution"><i class="fa fa-check"></i><b>2.4.3</b> The poisson distribution</a></li>
<li class="chapter" data-level="2.4.4" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#practice-problems"><i class="fa fa-check"></i><b>2.4.4</b> Practice Problems</a></li>
<li class="chapter" data-level="2.4.5" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#practice-problem-answers"><i class="fa fa-check"></i><b>2.4.5</b> Practice Problem Answers</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Behavioral Statistics: Theory and Application</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="discrete-random-variables" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Discrete Random Variables<a href="discrete-random-variables.html#discrete-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>As behavioral researchers, we are often studying a population of something. We might be interested in the percentage of individuals who like cats or the number of cats owned by the average individual. We might be interested in the average happiness level of a population of individuals and/or what makes them happy. The populations we study are often vaguely defined, e.g., as in the population of people who have received some experimental treatment. Here, we will begin to take these somewhat vague notions and make them concrete. It turns out that, when we are studying a population, we are really studying what statisticians call a <strong>random variable</strong>. In this chapter, we will pay particular attention to the <strong>discrete random variable</strong>.</p>
<div id="what-is-a-discrete-random-variable" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> What is a discrete random variable?<a href="discrete-random-variables.html#what-is-a-discrete-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we are in the process of conducting quantitative research (and statistics), we measure things in numbers. You may have noticed that our previous discussions of discrete sample spaces used non-numeric descriptions - e.g., <span class="math inline">\(S = \{H, T\}\)</span> in the case of a coin flip. To make these sample spaces easier to work with, we will map our discrete outcomes to numbers. For example, if we were interested in a coin flip, we wouldn’t work directly with the values of <span class="math inline">\(H\)</span> and <span class="math inline">\(T\)</span>. We would map each outcome to a value, e.g., heads to the value of 1 and tails to the value of 0. On any given coin flip, the value of our outcome would therefore be a number. A variable like this - one that assumes a <em>number</em> as a result of some random process (e.g., a flip, a random experiment, drawing out of a hat, rolling, randomly sampling, etc.) - is called a <strong>discrete random variable</strong>. In this section, we’ll provide some key definitions and notation for discrete random variables. This will make things more concrete and clear for you and make the interesting stuff more interesting.</p>
<p>The set of possible numbers a random variable can take is called the <strong>space</strong> of the random variable, and like the discrete sample space, these outcomes shall be either finite or countable. In general we refer to a random variable with a capital letter, typically <span class="math inline">\(X\)</span>, and a potential <em>outcome</em> of this random variable is referred to symbolically by a lower case letter, e.g., <span class="math inline">\(x\)</span>. For convenience, events in the space of a random variable are often referenced with a simplified set notation. Suppose we are describing the set of outcomes where our random variable <span class="math inline">\(X\)</span> attains a particular value. We will write <span class="math inline">\(X = x\)</span>. For example, <span class="math inline">\(X = 1\)</span> describes the outcomes in the space of <span class="math inline">\(X\)</span> where our random variable attains a value of 1 - <span class="math inline">\(\{1\}\)</span>. We may also use an inequality - e.g., <span class="math inline">\(X \leq x\)</span> - to reference a range of outcomes.</p>
<p>Because <span class="math inline">\(X\)</span> attains a value from a random process, we’ll need a concrete way of describing it’s randomness. Recall in the last chapter, we discussed the probability set operator - <span class="math inline">\(P()\)</span>. This operator assigns the probability to an outcome - <span class="math inline">\(x\)</span> - on the basis of a set of rules. The set of rules for the assignment of probability is called a probability distribution, and in the case of a discrete random variable this distribution is called a <strong>probability mass function</strong> (pmf) - <span class="math inline">\(p(x)\)</span>.</p>
<p>In practice, the set of rules (a.k.a, a probability mass function) is frequently described like this:</p>
<p><span class="math display">\[
P(X=x) = p(x) =
\begin{cases}
\frac{1}{2} &amp; x = 1 \\
\frac{1}{2} &amp; x = 0
\end{cases}
\]</span></p>
<p>Note that <span class="math inline">\(P(X = x)\)</span> and <span class="math inline">\(p(x)\)</span> mean the same thing - hence the equal sign. After the large curly-brace, you will see a row for each outcome in the space of <span class="math inline">\(X\)</span>. In that row you will find the probability for that outcome. For example, <span class="math inline">\(P(X = 1) = p(1) = 1/2\)</span>. The pmf of <span class="math inline">\(X\)</span> has two important properties. Specifically:</p>
<ol style="list-style-type: decimal">
<li><p>All the outcomes <span class="math inline">\(x\)</span> in the space of <span class="math inline">\(X\)</span> have a probability <span class="math inline">\(0 \leq p(x) \leq 1\)</span>. That is, all the outcomes in the space of <span class="math inline">\(X\)</span> must have a probability from 0 to 1. The set of outcomes for which <span class="math inline">\(p(x) &gt; 0\)</span> is called the <strong>support</strong> of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Given the space <span class="math inline">\(S\)</span> of a random variable, <span class="math inline">\(P(S)\)</span> = 1. In other words, if we are conducting a random experiment by drawing a value from the space of <span class="math inline">\(X\)</span> (the set of all numeric outcomes for <span class="math inline">\(X\)</span>), the probability that it lands in <span class="math inline">\(S\)</span> is 1. This hearkens back to <a href="set-theory-and-probability.html#eq:setprop">(1.10)</a> in the previous section.</p></li>
</ol>
<p>In plain language these two properties mean that we can define the pmf of a random variable by taking a total of one unit of probability and spreading it across a set of outcomes (the support of <span class="math inline">\(X\)</span>). For example, you might give .25 probability to a value of 1, .5 to a value of 2, and .25 to a value of 3. In this case, each outcome would have a probability from 0 to 1, and these values would sum to 1.</p>
<p>In this chapter we will dig into discrete random variables in three steps:</p>
<ol style="list-style-type: decimal">
<li><p>We shall start by discussing the independence of random variables - with the goal of framing an intuition for the concept.</p></li>
<li><p>We will then turn our attention to things researchers might want to know about discrete random variables (and, as you will learn, random variables more generally).</p></li>
<li><p>And finally, we will discuss some random variables that are frequently found in behavioral research.</p></li>
</ol>
</div>
<div id="independence-of-discrete-random-variables" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Independence of discrete random variables<a href="discrete-random-variables.html#independence-of-discrete-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The concept of independence is found frequently within the field of statistics. As you read on, you will find that it is essential to the derivation, understanding, and validity of some of the most fundamental things we shall learn in statistics. Therefore, it is nice to have a deeper understanding of it. The goal of this section is to frame an intuition of the independence of random variables. A deeper mathematical understanding shall be saved for your inevitable foray into mathematical statistics (no doubt after being inspired by this book to master the topic).</p>
<p>In the last chapter, we discussed the concept of the independence of <em>events</em> in a random experiment. We shall now discuss independence as it pertains to random variables. And, as you will see, it is very similar to the concept of independence for events. Suppose we have two random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Then we say the two variables are independent if</p>
<p><span class="math display" id="eq:descreteind">\[
P(X_1 = x_1 \cap X_2 = x_2) = P(X_1 = x_1)P(X_2 = x_2)
\tag{2.1}
\]</span>
In other words, if two random variables are independent, the probability of <span class="math inline">\(X_1 = x_1\)</span> AND <span class="math inline">\(X_2 = x_2\)</span> is equal to the product of their respective probabilities. We’ve already seen examples of this. Recall in example <a href="set-theory-and-probability.html#exm:coins">1.20</a>, we flipped three coins <em>independently</em>. If we mapped the outcomes of each coin to 1 and 0 for heads and tails, respectively, then the three resulting random variables are independent in the sense of <a href="discrete-random-variables.html#eq:descreteind">(2.1)</a>.</p>
<p>With some algebraic manipulation of <a href="discrete-random-variables.html#eq:descreteind">(2.1)</a>, we can see that the definition of independence for random variables is very similar to the definition of independence for events</p>
<p><span class="math display" id="eq:descreteind2">\[
\frac{P(X_1 = x_1 \cap X_2 = x_2)}{P(X_2 = x_2)}=P(X_1 = x_1)
\tag{2.2}
\]</span>
And, perhaps not surprisingly (although it’s okay if you are surprised), if the two random variables are not independent then</p>
<p><span class="math display">\[
\frac{P(X_1 = x_1 \cap X_2 = x_2)}{P(X_2 = x_2)} = P(X_1 = x_1|X_2 = x_2) \neq P(X_1 = x_1)
\]</span>
Put into the logic and hopefully more understandable intuition of last chapter, when two random variables are dependent, an event in the space of one variable <em>changes</em> the probability of an event in the space of another variable. This may seem a bit confusing at first, but you have already seen an example of the dependence of random variables. Recall example <a href="set-theory-and-probability.html#exm:dice">1.22</a>, when we looked at the sum of two dice. In this case, let’s call the sum of our two dice the random variable <span class="math inline">\(Z\)</span> and represent the random variable of the first die by <span class="math inline">\(X\)</span>.</p>
<p>On the basis of <a href="discrete-random-variables.html#eq:descreteind">(2.1)</a>, we shall be looking to determine if</p>
<p><span class="math display">\[
\frac{P(Z = z \cap X = x)}{P(X = x)} \neq P(Z = z)
\]</span></p>
<p>The first thing we will need is <span class="math inline">\(P(Z = z)\)</span>. The space of <span class="math inline">\(Z\)</span> are all the possible sums of our two dice - <span class="math inline">\(\{2,3,4,5,6,7,8\}\)</span>. Using the table we created in example <a href="set-theory-and-probability.html#exm:dice">1.22</a>, and the law of addition, we can determine a probability distribution for <span class="math inline">\(Z\)</span> (try it yourself and see if you get the same thing as below)</p>
<p><span class="math display">\[
P(Z=z) = p(z) =
\begin{cases}
1/16 &amp; z = 2 \\
2/16 &amp; z = 3 \\
3/16 &amp; z = 4 \\
4/16 &amp; z = 5 \\
3/16 &amp; z = 6 \\
2/16 &amp; z = 7 \\
1/16 &amp; z = 8
\end{cases}
\]</span></p>
<p>After our computation, we can see that the most probable value is 5. Because we haven’t had much experience with random variables, it is also important to point out that this distribution has all our previously discussed properties of a pmf. The probabilities of all the outcomes in the space of <span class="math inline">\(Z\)</span> sum to 1, and they are constrained between 0 and 1.</p>
<p>Now that we have determined the probability mass function for <span class="math inline">\(Z\)</span>, we shall calculate <span class="math inline">\((P(Z = z|X = x))\)</span>. Hopefully the idea of independence/dependence of random variables will come into focus. Consider the case where <span class="math inline">\(X = 1\)</span> (a 1 was rolled on the first die). What does the distribution of <span class="math inline">\(P(Z = z|X = 1)\)</span> look like? To determine this we shall calculate the conditional probability of each Z - <em>given</em> our first dice roll is a 1. We did this back in example <a href="set-theory-and-probability.html#exm:dice">1.22</a> for the specific value of <span class="math inline">\(Z = 3\)</span>, and now we shall do it here for every value (try it yourself)</p>
<p><span class="math display">\[
P(Z=z|X=1) = p(z|x=1) =
\begin{cases}
1/4 &amp; z = 2 \\
1/4 &amp; z = 3 \\
1/4 &amp; z = 4 \\
1/4 &amp; z = 5 \\
0 &amp; z = 6 \\
0 &amp; z = 7 \\
0 &amp; z = 8
\end{cases}
\]</span></p>
<p>Do you see what happens with the event <span class="math inline">\(X = 1\)</span>? The probability of <span class="math inline">\(Z\)</span> <em>changes</em> with a roll of a 1 on the first die. For example, getting a <span class="math inline">\(Z = 2\)</span> now has a whopping 1/4 chance, compared to a paltry 1/16 before the first dice was rolled. Moreover, the probability is now spread across only four values - <span class="math inline">\(\{2,3,4,5\}\)</span>. That is, the support of <span class="math inline">\(Z\)</span> - given <span class="math inline">\(X = 1\)</span> - is now 2, 3, 4, and 5. Thus, an intuitive definition of independence pivots on changes in the pmf of one random variable given the outcome of another variable.</p>
<p>This idea of independence shall be sufficient for our purposes, especially as we become more sophisticated in statistics and statistical analysis. As you will see, some of the most fundamental statistical concepts require an <em>assumption</em> of independence. In many cases, we shall assume that our random variables are independent to take advantage of some convenient mathematical properties associated with independence. We’ll see one of these convenient properties as soon as this chapter.</p>
</div>
<div id="what-do-we-want-to-know-about-discrete-random-variables" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> What do we want to know about discrete random variables?<a href="discrete-random-variables.html#what-do-we-want-to-know-about-discrete-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A new statistics student will have made it to this point and reasonably wondered: what the heck is the point of all this back story? This skepticism is understandable, welcomed, and hopefully motivating. In this section, we shall discuss some things - as statisticians and researchers - that we might want to know about random variables. In doing so, hopefully the bigger picture comes into view. This, in my humble opinion, is where things really start to get interesting. I’m hoping you feel the same.</p>
<p>But first, let’s do a very very short review.</p>
<div id="a-very-quick-review-the-summation-operator" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> A very quick review: The summation operator<a href="discrete-random-variables.html#a-very-quick-review-the-summation-operator" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As statisticians we will need to sum a lot of things. It will therefore be useful to have an easy way to describe the operation of summation. To that end, suppose we are summing a set of numbers</p>
<p><span class="math display">\[
x_1 + x_2 + x_3 + x_4 + x_5 \,+\:... + x_n
\]</span></p>
<p>The <strong>summation operator</strong> helps us write this operation more efficiently and with less ambiguity:</p>
<p><span class="math display" id="eq:summation">\[
\sum_{i=1}^{n} x_i
\tag{2.3}
\]</span>
The <span class="math inline">\(i\)</span> at the bottom of the <span class="math inline">\(\sum{}^{}\)</span> is referred to as the <em>index</em>. The index is how we refer to a single element in our set of <span class="math inline">\(x\)</span>’s. For example, <span class="math inline">\(i=5\)</span> refers to <span class="math inline">\(x_5\)</span>. The starting index is specified at the bottom of the summation operator to the right of the equal sign. In the equation above we are starting at the 1st element of our set of <span class="math inline">\(x\)</span>’s. The value at the top of the summation operator is the index of the last element to be summed. Here we have written <span class="math inline">\(n\)</span> which is the last element of our set of n <span class="math inline">\(x\)</span>’s. In other words</p>
<p><span class="math display">\[
\sum_{i=1}^{n} x_i = x_1 + x_2 + x_3 + x_4 + x_5 + ... + x_n
\]</span>
in terms of our example above. Additionally, it is important to note that n and i are not always used to indicate the last element and the index, respectively. Here we shall try to be consistent. Occasionally, to save a little time we shall write the summation operator like</p>
<p><span class="math display">\[
\sum_{x}^{}
\]</span></p>
<p>This means - given all the <span class="math inline">\(x&#39;s\)</span> in a set - sum all of them. Alright, now it’s time for the interesting stuff.</p>
</div>
<div id="the-expectation-of-a-discrete-random-variable" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> The expectation of a discrete random variable<a href="discrete-random-variables.html#the-expectation-of-a-discrete-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first thing that we shall want to know about a discrete random variable is its expectation. Given a random variable <span class="math inline">\(X\)</span>, we refer to its expectation as <span class="math inline">\(E(X)\)</span>. We shall also sometimes refer to it symbolically as <span class="math inline">\(\mu\)</span>. In other words</p>
<p><span class="math display">\[
E(X) = \mu
\]</span></p>
<p>The expectation is commonly referred to as the <strong>mean</strong> of a random variable. The expectation of a discrete random variable <span class="math inline">\(X\)</span> with a given pmf - <span class="math inline">\(p(x)\)</span> is calculated</p>
<p><span class="math display" id="eq:discreteexpectation">\[
E(X) = \sum_{i = 1}^{n} x_ip(x_i)
\tag{2.4}
\]</span></p>
<p>In words: For each <span class="math inline">\(x\)</span> in the space of <span class="math inline">\(X\)</span>, multiply by its corresponding probability and sum the resulting values. An example, will help us understand the expectation a bit better.</p>
<div class="example">
<p><span id="exm:ogexpectation" class="example"><strong>Example 2.1  </strong></span>Suppose we have a random variable <span class="math inline">\(X\)</span> with five outcomes in it’s space - <span class="math inline">\(\{1,2,3,4,5\}\)</span>. Further assume that the pmf of <span class="math inline">\(X\)</span> assigns <span class="math inline">\(\frac{1}{5}\)</span> probability to each outcome.</p>
<p><span class="math display">\[
P(X=x) = p(x) =
\begin{cases}
1/5 &amp; x_1 = 1 \\
1/5 &amp; x_2 = 2 \\
1/5 &amp; x_3 = 3 \\
1/5 &amp; x_4 = 4 \\
1/5 &amp; x_5 = 5
\end{cases}
\]</span>
This is an example of a <strong>discrete uniform distribution</strong>. Given n outcomes, the discrete uniform distribution assigns <span class="math inline">\(1/n\)</span> probability to each outcome on the space of the random variable.</p>
<p>Now let’s calculate the expectation of this random variable.</p>
<p><span class="math display">\[
\begin{align}
E(X) = \sum_{i=1}^{5} x_ip(x_i) &amp;= x_1p(x_1)+x_2p(x_2)+x_3p(x_3)+x_4p(x_4)+5p(x_5) \\
&amp;= 1p(1)+2p(2)+3p(3)+4p(4)+5p(5) \\
&amp;= (1)\frac{1}{5}+(2)\frac{1}{5}+(3)\frac{1}{5}+(4)\frac{1}{5}+(5)\frac{1}{5} \\
&amp;= \frac{15}{5} = 3
\end{align}
\]</span>
We’ve multiplied each value in the space of our random variable <span class="math inline">\(X\)</span> by its probability and summed the values. The result of our computation is 3 - <span class="math inline">\(E(x) = 3\)</span> - but what does that mean exactly? Well let’s explore the idea of expectation further by plotting the pmf of <span class="math inline">\(X\)</span>. In the figure below, the probability of each outcome is plotted. You can see the outcomes of the random variable labelled on the <span class="math inline">\(x\)</span> axis, and probability of each is represented by the bar height. In this case, each outcome is equally likely, so the bars are of equal height.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Now suppose each of those bars has <em>mass</em> (a.k.a. probability mass), and further suppose we’ll place our wedge under the <span class="math inline">\(x\)</span> axis such that the mass of the plot balances perfectly. Just by looking at the plot, if we put a wedge directly under the value of 3, there would be equivalent probability mass on either side, and the plot would balance perfectly. Notice that the value of 3 was the result of our expectation computation. <span class="math inline">\(E(X)\)</span> can therefore be thought as the balancing point of the plot - the place that would perfectly balance the mass of probability for our random variable.</p>
</div>
<p>That said, we shall need to learn a little bit more about the expectation to proceed on our statistical adventure. First let’s observe that it is possible to take the expectation of some <em>function</em> of a random variable. In other words, we may apply some function to the outcomes in the space of <span class="math inline">\(X\)</span> and then calculate an expectation. For example, we might have</p>
<p><span class="math display">\[
f(X) = 2X
\]</span>
This function takes the outcomes in the space of <span class="math inline">\(X\)</span> and multiplies each by 2. We shall calculate the expectation of this function of our random variable by</p>
<p><span class="math display" id="eq:discreteexpectationfunction">\[
E(f(X)) = \sum_{i = 1}^{n} f(x_i)p(x_i)
\tag{2.5}
\]</span></p>
<p>Let’s try it out in a couple examples.</p>
<div class="example">
<p><span id="exm:exp1" class="example"><strong>Example 2.2  </strong></span>We will again use the distribution from example <a href="discrete-random-variables.html#exm:ogexpectation">2.1</a>. Here we will keep it simple (and also demonstrate a property of multiplication you may recall from algebra). Suppose our function <span class="math inline">\(f(x)\)</span> takes every value in the space of <span class="math inline">\(X\)</span> and returns the value one. In other words,</p>
<p><span class="math display">\[
f(X) = 1
\]</span>
Let’s apply <a href="discrete-random-variables.html#eq:discreteexpectationfunction">(2.5)</a> to find the expectation of this function. First, let’s remind ourselves of the distribution of <span class="math inline">\(X\)</span> and calculate <span class="math inline">\(f(x)\)</span> for each outcome in it’s space</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
1/5 &amp; x_1 = 1 &amp; f(1) = 1 \\
1/5 &amp; x_2 = 2 &amp; f(2) = 1 \\
1/5 &amp; x_3 = 3 &amp; f(3) = 1 \\
1/5 &amp; x_4 = 4 &amp; f(4) = 1 \\
1/5 &amp; x_5 = 5 &amp; f(5) = 1
\end{cases}
\]</span>
Now let’s calculate the expectation of <span class="math inline">\(f(X)\)</span>. Watch the underlined values for a little bit of algebra review.</p>
<p><span class="math display">\[
\begin{align}
E(X) = \sum_{i=1}^{5} f(x_i)p(x_i) &amp;= \underline{1}\:p(x_1)+\underline{1}\:p(x_2)+\underline{1}\:p(x_3)+\underline{1}\:p(x_4)+\underline{1}\:p(x_5) \\
&amp;= \underline{1}\:[p(x_1) + p(x_2) + p(x_3) + p(x_4) + p(x_5)] \\
&amp;= \underline{1}\:[\frac{1}{5} + \frac{1}{5} + \frac{1}{5} + \frac{1}{5} + \frac{1}{5}] \\
&amp;= \underline{1}(1) = 1
\end{align}
\]</span>
The second line of this calculation was made possible by the distributive property of multiplication. That made the computation easier because we note that the sum of probabilities over the space of <span class="math inline">\(X\)</span> is equal to 1 (see the third line; and see the properties of a discrete random variable at the start of this chapter).</p>
</div>
<div class="example">
<p><span id="exm:exp2" class="example"><strong>Example 2.3  </strong></span>Suppose our random variable <span class="math inline">\(X\)</span> is again defined as in example <a href="discrete-random-variables.html#exm:ogexpectation">2.1</a>. Now let’s use the function <span class="math inline">\(f(x)= 2X\)</span>. Then
<span class="math display">\[
p(x) =
\begin{cases}
1/5 &amp; x_1 = 1 &amp; f(1) = 2(1) = 2 \\
1/5 &amp; x_2 = 2 &amp; f(2) = 2(2) = 4 \\
1/5 &amp; x_3 = 3 &amp; f(3) = 2(3) = 6 \\
1/5 &amp; x_4 = 4 &amp; f(4) = 2(4) = 8 \\
1/5 &amp; x_5 = 5 &amp; f(5) = 2(5) = 10
\end{cases}
\]</span></p>
<p><span class="math display">\[
\begin{align}
E(2X) = \sum_{i=1}^{5} f(x_i)p(x_i) &amp;= 2p(x_1)+4p(x_2)+6p(x_3)+8p(x_4)+10p(x_5) \\
&amp;= (2)\frac{1}{5} + (4)\frac{1}{5} + (6)\frac{1}{5} + (8)\frac{1}{5} + (10)\frac{1}{5} \\
&amp;= \frac{30}{5} = 6
\end{align}
\]</span></p>
</div>
<p>The astute observer might notice an interesting trend in the last two examples. In example <a href="discrete-random-variables.html#exm:exp1">2.2</a>, we saw that our function which converts the whole space of <span class="math inline">\(X\)</span> into 1 had the expectation of 1. Indeed, looking back at the computation, we could perform that computation for any constant, e.g., if <span class="math inline">\(a\)</span> is a constant, and <span class="math inline">\(f(X) = a\)</span>, then</p>
<p><span class="math display">\[
\begin{align}
E(a) = \sum_{i=1}^{5} ap(x_i) &amp;= \underline{a}\:p(x_1)+\underline{a}\:p(x_2)+\underline{a}\:p(x_3)+\underline{a}\:p(x_4)+\underline{a}\:p(x_5) \\
&amp;= \underline{a}\:[p(x_1) + p(x_2) + p(x_3) + p(x_4) + p(x_5)] \\
&amp;= \underline{a}\:[\frac{1}{5} + \frac{1}{5} + \frac{1}{5} + \frac{1}{5} + \frac{1}{5}] \\
&amp;= \underline{a}(1) = a
\end{align}
\]</span></p>
<p>In other words, we would get the value of our constant every time. Similarly, in <a href="discrete-random-variables.html#exm:exp2">2.3</a>, recall that the original expectation of <span class="math inline">\(X\)</span> was 3. Our function <span class="math inline">\(f(X) = 2X\)</span> had the expectation of <span class="math inline">\(2\times3 = 6\)</span>. That is, we multiplied <span class="math inline">\(X\)</span> by 2 and got back 2 times its expectation.</p>
<p>Our last two observations highlight a couple of <em>properties</em> of the expectation. We’ll list a few below. The first two (or three) you might have deduced yourself. The fourth is new.</p>
<div class="theorem">
<p><span id="thm:expprop" class="theorem"><strong>Theorem 2.1  (Some Properties of the Expectation) </strong></span>If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, then</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(E(b) = b\)</span></p></li>
<li><p><span class="math inline">\(E(aX) = aE(X)\)</span></p></li>
<li><p>More generally, <span class="math inline">\(E(aX + b) = aE(X) + E(b)\)</span></p></li>
</ol>
<p>If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are random variables, then</p>
<ol start="4" style="list-style-type: decimal">
<li><span class="math inline">\(E(aX_1 + bX_2)\)</span> = <span class="math inline">\(aE(X_1) + bE(X_2)\)</span></li>
</ol>
</div>
<p>These are called properties because they apply to any random variable - assuming that random variable has an expectation (a more advanced topic that we won’t discuss in much depth in this book). They allow us to perform computations with expectations without knowing much about the actual distribution of the random variable. And, they make computations easier. As you will see later, they will help us draw certain conclusions about the populations we study. For now, let’s see how they work in practice and look at a more thorough example for the fourth.</p>
<div class="example">
<p><span id="exm:unlabeled-div-22" class="example"><strong>Example 2.4  </strong></span>Suppose we are given a random variable - <span class="math inline">\(Z\)</span> - and <span class="math inline">\(E(Z) = 7\)</span>. What is the expectation of</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(4Z\)</span></p></li>
<li><p><span class="math inline">\(Z + 10\)</span></p></li>
<li><p><span class="math inline">\(1\)</span></p></li>
<li><p><span class="math inline">\(4Z+10\)</span></p></li>
</ol>
<p>We will approach each of these in turn.</p>
<ol style="list-style-type: decimal">
<li>Property 2 tells us that <span class="math inline">\(E(aX) = aE(X)\)</span>, and in this case <span class="math inline">\(a = 4\)</span></li>
</ol>
<p><span class="math display">\[
E(4Z) = 4E(Z) = 4(7) = 28
\]</span></p>
<p>Note that property 3 also applies here with <span class="math inline">\(b = 0\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Property 3 tells us that <span class="math inline">\(E(aX + b) = aE(X) + b\)</span>, so in this case we have <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = 10\)</span></li>
</ol>
<p><span class="math display">\[
E(Z + 10) = (1)E(Z) + 10 = 7 + 10 = 17
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>The first property tells us that <span class="math inline">\(E(c) = c\)</span>, so</li>
</ol>
<p><span class="math display">\[
E(1) = 1
\]</span></p>
<p>Note that property 3 also applies here with <span class="math inline">\(a = 0\)</span> and <span class="math inline">\(b = 1\)</span>.</p>
<ol start="4" style="list-style-type: decimal">
<li>Similar to number 2, property 3 applies with <span class="math inline">\(a = 4\)</span> and <span class="math inline">\(b = 10\)</span></li>
</ol>
<p><span class="math display">\[
E(4Z + 10) = E(4Z) + 10 = 4E(Z) + 10 = 28 + 10 = 38
\]</span></p>
</div>
<p>Do you see how that made things a bit easier? We didn’t have to go through the computation of the expectation of our function of <span class="math inline">\(Z\)</span>. I bet you can tell by now I like to avoid long computations! Now let’s turn our attention towards property 4.</p>
<div class="example">
<p><span id="exm:expectationsum" class="example"><strong>Example 2.5  </strong></span>Suppose we flip two coins - where a flip of <span class="math inline">\(H\)</span> yields a value of 1 and a flip of <span class="math inline">\(T\)</span> yields a value of zero. Call these two random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, respectively, and further assume that they are independent (recall what that means?). What is the expectation of their sum? To demonstrate why property four is so useful, we’ll do this the hard way and then the easy way.</p>
<p>First note that the pmf of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is</p>
<p><span class="math display">\[
p(x_1) =
\begin{cases}
1/2 &amp; x_1 = 1 \\
1/2 &amp; x_1 = 0 \\
\end{cases}
\\
p(x_2) =
\begin{cases}
1/2 &amp; x_2 = 1 \\
1/2 &amp; x_2 = 0 \\
\end{cases}
\]</span>
Without the use of property 4, these pmfs won’t suffice to calculate the expectation of <span class="math inline">\(X_1 + X_2\)</span>. We’ll have to figure out the pmf of the sum of the two random variables, a.k.a., <span class="math inline">\(Z = X_1 + X_2\)</span>. Note that there are four possible outcomes in the sample space of our two coin flips = <span class="math inline">\({HH, HT, TH, TT}\)</span>. One outcome in this sample space that would lead to a sum of two - <span class="math inline">\({HH}\)</span>. In this case, both values of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> would be 1 so</p>
<p><span class="math display">\[
X_1 + X_2 = 1 + 1 = 2
\]</span>
Because our random variables are independent, we can find the probability of this outcome using the law of multiplication for independent events</p>
<p><span class="math display">\[
P(X_1 = 1 \cap X_2 = 1) = P(X_1 = 1)P(X_2 = 1) = (1/2)(1/2) = 1/4
\]</span>
We can do this calculation for each of the other outcomes in the sample space of our coin flips. The table below breaks down each. It is most easily read from left to right. Find the value of the first flip in the first column. For example, the big top row has a first flip of 1 (Heads). Next scan to the right to see outcome of the second flip, which can either be a 1 or a 0. Scan further to the right to see the sum and probability of the outcome. For example, if we flipped a 1 on the first coin, and then a zero on the second coin, the sum of the two would be 1, and the probability would be 1/4.</p>
<div class="tabwid"><style>.cl-44276058{}.cl-44220a86{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-44241d9e{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4424327a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-44243284{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-44243285{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-44243286{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-44243287{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-44276058'><thead><tr style="overflow-wrap:break-word;"><th class="cl-4424327a"><p class="cl-44241d9e"><span class="cl-44220a86">1st Flip</span></p></th><th class="cl-4424327a"><p class="cl-44241d9e"><span class="cl-44220a86">2nd Flip</span></p></th><th class="cl-4424327a"><p class="cl-44241d9e"><span class="cl-44220a86">Sum (Z)</span></p></th><th class="cl-4424327a"><p class="cl-44241d9e"><span class="cl-44220a86">Probability</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td  rowspan="2"class="cl-44243284"><p class="cl-44241d9e"><span class="cl-44220a86">1</span></p></td><td class="cl-44243285"><p class="cl-44241d9e"><span class="cl-44220a86">1</span></p></td><td class="cl-44243285"><p class="cl-44241d9e"><span class="cl-44220a86">2</span></p></td><td class="cl-44243285"><p class="cl-44241d9e"><span class="cl-44220a86">1/4</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-44243284"><p class="cl-44241d9e"><span class="cl-44220a86">0</span></p></td><td class="cl-44243284"><p class="cl-44241d9e"><span class="cl-44220a86">1</span></p></td><td class="cl-44243284"><p class="cl-44241d9e"><span class="cl-44220a86">1/4</span></p></td></tr><tr style="overflow-wrap:break-word;"><td  rowspan="2"class="cl-44243286"><p class="cl-44241d9e"><span class="cl-44220a86">0</span></p></td><td class="cl-44243287"><p class="cl-44241d9e"><span class="cl-44220a86">1</span></p></td><td class="cl-44243287"><p class="cl-44241d9e"><span class="cl-44220a86">1</span></p></td><td class="cl-44243287"><p class="cl-44241d9e"><span class="cl-44220a86">1/4</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-44243284"><p class="cl-44241d9e"><span class="cl-44220a86">0</span></p></td><td class="cl-44243284"><p class="cl-44241d9e"><span class="cl-44220a86">0</span></p></td><td class="cl-44243284"><p class="cl-44241d9e"><span class="cl-44220a86">1/4</span></p></td></tr></tbody></table></div>
<p>Looking over this table, we see that our new random variable <span class="math inline">\(Z\)</span> has the space of <span class="math inline">\({0,1,2}\)</span>. There is only 1 way to get a 0 or 2, respectively, and each of those outcomes has a probability of <span class="math inline">\(1/4\)</span>. There are two ways to get a 1, so by the law of addition, this outcome in the space of <span class="math inline">\(Z\)</span> has a probability of <span class="math inline">\(1/4 + 1/4 = 1/2\)</span>. In other words</p>
<p><span class="math display">\[
p(z) =
\begin{cases}
1/4 &amp; z = 0 \\
1/2 &amp; z = 1 \\
1/4 &amp; z = 2
\end{cases}
\]</span>
Now, after that slog (you can imagine it being much worse), are in a position to calculate the expectation of <span class="math inline">\(Z = X_1 + X_2\)</span></p>
<p><span class="math display">\[
\begin{align}
E(a) = \sum_{z}^{} zp(z) &amp;= 0\:p(x_1)+1\:p(x_2)+2\:p(x_3) \\
&amp;= 0\frac{1}{4} + 1\frac{1}{2} + 2\frac{1}{4} = \frac{2}{4} + \frac{2}{4} = 1 \\
\end{align}
\]</span></p>
<p>The usefulness of the fourth property now comes into view. Observe that <span class="math inline">\(E(X_1) = E(X_2) = 1/2\)</span> (calculate this on your own; you can do it!). So, by property 4</p>
<p><span class="math display">\[
E(X_1 + X_2) = E(X_1) + E(X_2) = 1/2 + 1/2 = 1
\]</span>
Easier!</p>
</div>
<p>We shall end this section with a word of caution. Some beginning statistics students may understandably read property 4 and think it applies to other arithmetic operations. For example they may think that <span class="math inline">\(E(XY) = E(X)E(Y)\)</span>. This is true in some circumstances but it is not true in general. The properties of the expectation apply <em>specifically</em> to the circumstances outlined by each property. You may get into trouble to go beyond that.</p>
</div>
<div id="the-variance-of-a-discrete-random-variable" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> The variance of a discrete random variable<a href="discrete-random-variables.html#the-variance-of-a-discrete-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have just dedicated a bit of time to the expectation of a random variable. Another important thing we will want to know as statisticians is the <strong>variance</strong> of a random variable. The variance of a random variable <span class="math inline">\(X\)</span> is referred to as <span class="math inline">\(Var(X)\)</span> or symbolically as <span class="math inline">\(\sigma^2\)</span>. The square root of the variance - <span class="math inline">\(\sigma\)</span> - is called the <strong>standard deviation</strong>. In terms of discrete random variables, the variance is calculated</p>
<p><span class="math display" id="eq:discretevariance">\[
Var(X) = E((X-\mu)^2) = \sum_{x}^{} (x - \mu)^2p(x)
\tag{2.6}
\]</span></p>
<p>It can be thought of as a measure of how “spread” out the probability mass function is for a given random variable (a.k.a. the <em>dispersion</em>). That is, it will tend to be higher for variables that have probability mass spread over larger ranges of values. In the case of the variance calculation described here, we are measuring dispersion by how far values are spread out from the expectation or mean of the random variable. There are indeed many ways you can measure the dispersion of a random variable. New students, for example, often wonder why we don’t just calculate <span class="math inline">\(E(X - \mu)\)</span> instead of squaring the difference as in <a href="discrete-random-variables.html#eq:discretevariance">(2.6)</a>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-23" class="example"><strong>Example 2.6  </strong></span>Let’s do a quick calculation to convince ourselves why this isn’t a good idea. Such a calculation will have the added bonus of helping us review properties of the expectation. First notice that <span class="math inline">\(\mu\)</span> is a constant, so</p>
<p><span class="math display">\[
E(X-\mu) = E(X) - E(\mu) = E(X) - \mu
\]</span></p>
<p>Now observe that <span class="math inline">\(E(X) = \mu\)</span>, so</p>
<p><span class="math display">\[
E(X) - \mu = \mu - \mu = 0
\]</span>
<span class="math inline">\(E(X - \mu)\)</span> is zero for every random variable (that has an expectation) which is not a very good measure of dispersion!</p>
</div>
<div class="example">
<p><span id="exm:firstvariance" class="example"><strong>Example 2.7  </strong></span>With that minor point out of the way, let’s give the variance equation some run. Suppose our random variable <span class="math inline">\(X\)</span> has the pmf</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
1/8 &amp; x_1 = 0 \\
3/4 &amp; x_2 = 1 \\
1/8 &amp; x_3 = 2
\end{cases}
\]</span></p>
<p>Reviewing the equation for the variance, it is apparent we shall first need to calculate <span class="math inline">\(\mu\)</span> for <span class="math inline">\(X\)</span> before we can do any calculation of the variance. Knowing what we know about the expected value and the balancing point of probability mass, I think we would be pretty safe to guess that the expectation of <span class="math inline">\(X\)</span> is 1. Nevertheless, let’s calculate it really quick</p>
<p><span class="math display">\[
\begin{align}
E(X) = \sum_{i=1}^{3} x_ip(x_i) &amp;= 0\frac{1}{8}+ 1\frac{3}{4} + 2\frac{1}{8} \\
&amp;= \frac{6}{8} + \frac{2}{8} = 1
\end{align}
\]</span></p>
<p>Our intuition was indeed correct - <span class="math inline">\(E(X) = 1\)</span>. We shall now turn our attention to calculating the variance of <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
\begin{align}
Var(X) = \sum_{i=1}^{3} (x_1-\mu)^2p(x_i) &amp;= (x_1-\mu)^2p(x_1) + (x_2-\mu)^2p(x_2) + (x_3-\mu)^2p(x_3) \\
&amp;= (0 - 1)^2\frac{1}{8} + (1-1)^2\frac{1}{8} + (2-1)^2\frac{1}{8} \\
&amp;= (-1)^2\frac{1}{8} + 0 + 1^2\frac{1}{8} = \frac{1}{4}
\end{align}
\]</span>
Thus, <span class="math inline">\(Var(X) = 1/4\)</span>.</p>
<p>We should stop here and take a breath. In the last two sections, you’ve calculated your very first mean and variance of a random variable. Congratulations! These concepts may seem a little disconnected from our work as behavioral scientists, but they will make some wonderful things clear later on. I promise.</p>
</div>
<p>Now let’s get back to work. Just like the expectation, we can also calculate the variance of some function of <span class="math inline">\(X\)</span>. First calculate <span class="math inline">\(E(f(X))\)</span>, referred to here as <span class="math inline">\(\mu_f\)</span>, then</p>
<p><span class="math display" id="eq:discretevariancef">\[
Var(X) = E((f(X)-\mu_f)^2) = \sum_{x}^{} (f(x) - \mu_f)^2p(x)
\tag{2.7}
\]</span></p>
<p>To demonstrate how this equation works we’ll use the same pmf as example <a href="discrete-random-variables.html#exm:firstvariance">2.7</a>.</p>
<div class=".{example}">
<p>Suppose we want to find the variance of <span class="math inline">\(f(X) = 2X\)</span>. As with calculation of the expecation in the last section, let’s first multiply all the outcomes in the space of <span class="math inline">\(X\)</span> by 2</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
1/8 &amp; x_1 = 0 &amp; f(x_1) = (2)0 = 0 \\
3/4 &amp; x_2 = 1 &amp; f(x_2) = (2)1 = 2\\
1/8 &amp; x_3 = 2 &amp; f(x_3) = (2)2 = 4
\end{cases}
\]</span>
Next, in order to calculate <span class="math inline">\(Var(2X)\)</span>, we will first need <span class="math inline">\(E(2X)\)</span>. I’ll just go ahead and give you the fact that <span class="math inline">\(E(2X) = 2\)</span>, but you should calculate this for yourself as well (you have all the tools, skill, and excitement to do it!). Next we’ll apply our new equation</p>
<p><span class="math display">\[
\begin{align}
Var(2X) = \sum_{i=1}^{3} (f(x_i)-\mu_f)^2p(x_i) &amp;= (f(x_1)-\mu_f)^2p(x_1) + (f(x_2)-\mu_f)^2p(x_2) + (f(x_3)-\mu_f)^2p(x_3) \\
&amp;= (0 - 2)^2\frac{1}{8} + (2-2)^2\frac{1}{8} + (4-2)^2\frac{1}{8} \\
&amp;= (-2)^2\frac{1}{8} + 0 + 2^2\frac{1}{8} = 1
\end{align}
\]</span>
Observe that the variance of our random variable <span class="math inline">\(2X\)</span> is larger than <span class="math inline">\(X\)</span>. To understand why, let’s take a look at the probability mass functions of <span class="math inline">\(X\)</span> and <span class="math inline">\(2X\)</span>.</p>
<p>The top plot below is the pmf of <span class="math inline">\(X\)</span>, with the vertical line representing <span class="math inline">\(E(X)\)</span>. The height of each bar represents the probability mass assigned to a particular value on the <span class="math inline">\(x\)</span>-axis - in the space of <span class="math inline">\(X\)</span>. For example, <span class="math inline">\(1/8\)</span> mass sits on top of the value 0. The bottom plot is the pmf of <span class="math inline">\(2X\)</span> - as if we created a new random variable as <span class="math inline">\(x\)</span> but with each of the outcomes multiplied by 2. That is, for example, instead of <span class="math inline">\(1/8\)</span> probability sitting on top of 2, it now sits on top of 4. In this case, the vertical line represents <span class="math inline">\(E(2X)\)</span>. Note how the mass for <span class="math inline">\(2X\)</span> now sits on the values of 0, 2, and 4 (the outcomes after we have applied <span class="math inline">\(f(x)\)</span>). And, it’s pmf is spread out further around its expectation. It is <em>dispersed</em> more widely than the probability mass of <span class="math inline">\(X\)</span>. Indeed, this is an intuitive (although maybe not perfectly exact) way of thinking about variance: A random variable with a larger variance will tend to have it’s probability mass spread out more. Therefore, it is not surprising that the <span class="math inline">\(Var(X) &lt; Var(2X)\)</span>. In fact, this is precisely the sort of behavior we would want out of a measure of variance!</p>
<p><img src="_main_files/figure-html/unnamed-chunk-8-1.png" width="672" />
<img src="_main_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<p>We shall observe one more thing about our last two examples. In the first example we found <span class="math inline">\(Var(X) = 1/4\)</span> and in the second example, we found <span class="math inline">\(Var(2X) = 1\)</span>. Indeed, you may not have noticed this quickly (I didn’t when I first learned this stuff), but</p>
<p><span class="math display">\[
\begin{align}
Var(2X) &amp;= 1 \\
&amp;= 4\frac{1}{4} \\
&amp;= 2^2\frac{1}{4} \\
&amp;= 2^2Var(X)
\end{align}
\]</span>
Because <span class="math inline">\(Var(X) = 1/4\)</span>. I might have lost you there. In words, it looks like the variance of <span class="math inline">\(2X\)</span> is simply <span class="math inline">\(2^2\)</span> times the variance of <span class="math inline">\(X\)</span>. If we did this a few more times, you might begin to think that, given some value <span class="math inline">\(c\)</span>, the variance of <span class="math inline">\(cX\)</span> is just <span class="math inline">\(c^2Var(X)\)</span>. Indeed this is true in general (for random variables that have a variance). Like the expectation, the variance also has some useful properties. They are listed below:</p>
<div class="theorem">
<p><span id="thm:expprop" class="theorem"><strong>Theorem 2.1  (Some Properties of the Variance) </strong></span>If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, then</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(Var(b) = 0\)</span></p></li>
<li><p><span class="math inline">\(Var(aX) = a^2Var(X)\)</span></p></li>
<li><p>More generally, <span class="math inline">\(Var(aX + b) = a^2Var(X) + Var(b) = a^2Var(X) + 0\)</span></p></li>
</ol>
<p>If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are random variables, <strong>and they are independent</strong>, then</p>
<ol start="4" style="list-style-type: decimal">
<li><span class="math inline">\(Var(aX_1 + bX_2)\)</span> = <span class="math inline">\(a^2Var(X_1) + b^2Var(X_2)\)</span></li>
</ol>
</div>
<p>Let’s do some examples to get a better grasp on these.</p>
<div class="example">
<p><span id="exm:unlabeled-div-24" class="example"><strong>Example 2.8  </strong></span>Suppose we have a random variable <span class="math inline">\(X\)</span> and we are seeking to determine the the variance of <span class="math inline">\(f(X) = b\)</span> - e.g., <span class="math inline">\(Var(b)\)</span>. This function takes any value in the space of <span class="math inline">\(X\)</span> and returns a constant <span class="math inline">\(b\)</span>. First take a moment to recall that <span class="math inline">\(E(f(X)) = E(b) = b\)</span> (Property 1 of the expectation). So then our variance equation boils down to:</p>
<p><span class="math display">\[
Var(X) = E((f(X)-\mu_f)^2) = \sum_{x}^{} (b - b)^2p(x) = 0
\]</span>
Indeed, we could have just used property 1 of the variance and concluded the same thing - <span class="math inline">\(Var(b) = 0\)</span>. As an aside: within the field of statistics, random variables with zero variance are called <em>degenerate</em>. These variables return a constant every time because all their probability sits on exactly one value.</p>
</div>
<p>The next example will elucidate a bit of property 3. Specifically, we unpack why adding a constant value to a random variable doesn’t change it’s variance. That is to say, if we have <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b=10\)</span>, then</p>
<p><span class="math display">\[
Var(aX+b) = (1)Var(X) + 0 = Var(X)
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-25" class="example"><strong>Example 2.9  </strong></span>Suppose we have a random variable <span class="math inline">\(X\)</span> with <span class="math inline">\(E(X) = 1/2\)</span>, <span class="math inline">\(Var(X) = 1/4\)</span>, and pmf</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
1/2 &amp; x = 0 \\
1/2 &amp; x = 1
\end{cases}
\]</span>
Take a moment to calculate the expectation and variance of this random variable yourself. Now let’s say we would like to calculate <span class="math inline">\(Var(X + 5)\)</span>. Looking at property 3 of the expectation, we have <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = 5\)</span>. Therefore</p>
<p><span class="math display">\[
Var(X + 5) = Var(X) + 0 = 1/4
\]</span>
Why didn’t adding a constant to <span class="math inline">\(X\)</span> change it’s variance? As is often the case, it’s easier to see why by visualizing some pmfs. In the plot below, you will see two pmfs. The first (blueish) is the pmf of <span class="math inline">\(X\)</span>, and the second (redish) is the pmf of <span class="math inline">\(X+5\)</span> - as if we created a new random variable and pmf by adding 5 to each outcome of <span class="math inline">\(X\)</span>. The vertical lines represent <span class="math inline">\(E(X)\)</span> and <span class="math inline">\(E(X+5)\)</span> respectively. Do you see how the plot of <span class="math inline">\(X+5\)</span> is simply the pmf of <span class="math inline">\(X\)</span> shifted to the right? We can see the bars of <span class="math inline">\(X+%\)</span> are still equivalently “spread” out but at a new location on the number line. Indeed, the values in the <span class="math inline">\(X+5\)</span> pmf are just as close to their expectation compared to <span class="math inline">\(X\)</span>, so the variance calculation would not turn out differently (I invite you to convince yourself of this by using equation <a href="discrete-random-variables.html#eq:discretevariancef">(2.7)</a>.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<p>Let’s do a few more examples using properties 1 through 3, and then we’ll move on to the fourth.</p>
<div class="example">
<p><span id="exm:unlabeled-div-26" class="example"><strong>Example 2.10  </strong></span>Given a random variable <span class="math inline">\(X\)</span> with <span class="math inline">\(Var(X) = 10\)</span> what is</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(Var(5X)\)</span></p></li>
<li><p><span class="math inline">\(Var(3)\)</span></p></li>
<li><p><span class="math inline">\(Var(2X + 7)\)</span></p></li>
</ol>
<p>Let’s unpack each in turn.</p>
<ol style="list-style-type: decimal">
<li>Property 2 says that <span class="math inline">\(Var(aX) = a^2Var(X)\)</span>, so</li>
</ol>
<p><span class="math display">\[
Var(5X) = 25Var(X) = 250
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Property 1 says that <span class="math inline">\(Var(b) = 0\)</span>, so this one is <span class="math inline">\(0\)</span></p></li>
<li><p>Property 3 says that <span class="math inline">\(Var(aX + b) = a^2Var(X) + 0\)</span>, so</p></li>
</ol>
<p><span class="math display">\[
Var(2X + 7) = 4Var(X) + 0 =
\]</span></p>
</div>
<p>The last three examples should give you a good sense of how to use properties 1 through 3 of the variance. They can each be useful and save us the rigmarole of the variance calculation. We now turn our attention to the fourth property. Notice how this property is very similar to the fourth property of the expectation - except there is an additional caveat. Our random variables must be <em>independent</em>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-27" class="example"><strong>Example 2.11  </strong></span>We will again work with the random variables in example <a href="discrete-random-variables.html#exm:expectationsum">2.5</a>. Recall that <span class="math inline">\(Z\)</span> was the sum of two random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> - each having an expectation of 1/2. In this example, we determined the distribution and expectation of <span class="math inline">\(Z\)</span>. From property 4 of the expectation we found that <span class="math inline">\(E(Z) = E(X_1 + X_2) = 1/2 + 1/2 = 1\)</span>. Now let’s determine the variance of this random variable.</p>
<p>Recall that the variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent. This means that property 4 applies, and we shall use it. First let’s calculate the variance of <span class="math inline">\(X_1\)</span> (which will also be the variance of <span class="math inline">\(X_2\)</span>)</p>
<p><span class="math display">\[
\begin{align}
Var(X_1) = \sum_{x}^{} &amp;= (0-\frac{1}{2})^2p(0) + (1-\frac{1}{2})^2p(x_2) \\
&amp;= 0 + \frac{1}{4} \\ &amp;= \frac{1}{4}
\end{align}
\]</span>
And, therefore, by property 4, the variance of <span class="math inline">\(Z\)</span> is</p>
<p><span class="math display">\[
Var(Z) = Var(X_1 + X_2) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}
\]</span>
Now consider the circumstance where we multiply <span class="math inline">\(X_1\)</span> by four before adding it to <span class="math inline">\(X_2\)</span>. Then the variance of the sum of <span class="math inline">\(4X_1\)</span> and <span class="math inline">\(X_2\)</span> would be</p>
<p><span class="math display">\[
Var(4X_1 + X_2) = 16Var(X_1) + Var(X_2) = (16) \frac{1}{4} + 1/4 = \frac{17}{4}
\]</span>
by property 4. And now you see the usefulness of the fourth property of the variance. We only needed the expectations and variances of our simpler variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> to do more complicated computations. In this case, we didn’t need to know the distribution or expectation of <span class="math inline">\(4X_1 + X_2\)</span> (although I bet you could figure it out by following the procedure of example <a href="discrete-random-variables.html#exm:expectationsum">2.5</a>). We only need to know that they are independent. As you will see, many of the statistical procedures we will learn will <em>assume</em> independence to take advantage of this particular property of the variance.</p>
</div>
<p>We close out this section with a final word of caution and a reference to the bigger picture. First the word of caution: The properties of the variance must be applied exactly how they are described. For example, without more information (which we will not persue here) it would be difficult to tell what <span class="math inline">\(Var(X_1X_2)\)</span> would be.</p>
<p>Now the bigger picture: We have spent the last two sections of this chapter talking about the expectation and variance of random variables. In our research practice, we typically <em>do not know</em> the expectation and variance of the population (random variable) we are trying to study. Statistics is the mathematical science of figuring these things out. Over the years some very smart people have invented some wonderful ways to study random variables. Understanding the expectation and variance shall help us develop a deeper understanding how to study random variables. We’re on the journey to learn the beginnings of this wonderful field.</p>
</div>
</div>
<div id="some-common-discrete-distributions" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Some common discrete distributions<a href="discrete-random-variables.html#some-common-discrete-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we have seen from the previous sections, there are an infinitude of probability mass functions/discrete distributions. Nevertheless, as behavioral researchers we tend to encounter certain types of random variables and discrete distributions more frequently. In this section, we shall discuss three commonly occurring distributions of discrete random variables:</p>
<ol style="list-style-type: decimal">
<li><p>The Bernoulli distribution,</p></li>
<li><p>The binomial distribution,</p></li>
<li><p>and the Poisson distribution.</p></li>
</ol>
<p>As you will see, these distributions are closely related.</p>
<div id="the-bernoulli-distribution" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> The Bernoulli distribution<a href="discrete-random-variables.html#the-bernoulli-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable - <span class="math inline">\(X\)</span> - with a Bernoulli distribution (here referred to as a <em>Bernoulli random variable</em>) can take two outcomes - 1 or 0 - where 1 is typically mapped to a “success” or outcome of interest. We’ve already seen an example of a Bernoulli random variable - a coin flip with pmf</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
1/2 &amp; x = 1 \\
1/2 &amp; x = 0
\end{cases}
\]</span></p>
<p>In general, however, the probability of a success does not have to be 1/2. We can choose any probability <span class="math inline">\(p\)</span> with <span class="math inline">\(0 \leq p \leq 1\)</span>, such that the pmf of our random variable <span class="math inline">\(X\)</span> is</p>
<p><span class="math display" id="eq:bernoulli">\[
p(x) =
\begin{cases}
p &amp; x = 1 \\
1-p &amp; x = 0
\end{cases}
\tag{2.8}
\]</span></p>
<p>That is to say, we assign <span class="math inline">\(p\)</span> as the probability of success and then assign the remaining probability <span class="math inline">\(1-p\)</span> to a non-success. Notice how the probabilities add to 1, e.g.,</p>
<p><span class="math display">\[
p + (1-p) = 1
\]</span></p>
<p>Here we refer to <span class="math inline">\(p\)</span> as a <strong>parameter</strong> of the pmf of <span class="math inline">\(X\)</span>. A parameter is a value that is necessary to fully specify a distribution and determine a probability from it. In this case, we need the value of <span class="math inline">\(p\)</span> in order to determine the probability of a success or non-success, respectively. For example, if <span class="math inline">\(p = 1/4\)</span>, then the pmf of the random variable <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
1/4 &amp; x = 1 \\
1-1/4 = 3/4 &amp; x = 0
\end{cases}
\]</span>
As we have seen from the previous two sections, it helps to have quick ways of calculating expectations and variances. For each of the common distributions, we will therefore present or derive equations for each. As you know, in order to get the variance, we first need to expectation. With regards to a Bernoulli random variable (<span class="math inline">\(X\)</span>) with some parameter <span class="math inline">\(p\)</span>, the expectation is</p>
<p><span class="math display" id="eq:bernexpectation">\[
E(X) = \sum_{x} xp(x) = 1(p) + 0(1-p) = p
\tag{2.9}
\]</span>
In words, the expectation of a Bernoulli random variable is the probability of a success. As you recall from our previous discussion, now that we have the expectation of our random variable, we have enough information to calculate the variance. This is a little more complicated, although certainly within our reach. I’ll try my best to breakdown the steps. Recall the definition of the Bernoulli distribution:</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
p &amp; x = 1 \\
1-p &amp; x = 0
\end{cases}
\]</span></p>
<p>Now let’s calculate the variance with this as reference.</p>
<p><span class="math display">\[
\begin{align}
Var(X) = \sum_{x} (x - \mu)^2p(x) &amp;= (x-p)^2p(x) \\
&amp;= (1-p)^2p + (0-p)^2(1-p) \\
&amp;= (1-p)^2p + p^2(1-p) \\
&amp;= p(1-p)^2 + p^2(1-p) \\
&amp;= p(1-p)(1-p) + p(p)(1-p)
\end{align}
\]</span>
Note that we just rearranged terms from the 3rd to 4th line. On the 5th line we wrote out the squares as multiplications. Now, we’ll stop here and point something out. This equation has two terms - one before the + and one after. Each of these terms has a common factor of <span class="math inline">\(p(1-p)\)</span>. We’re going to use the magic of the distributive property. Watch the underlines below.</p>
<p><span class="math display" id="eq:bernvariance">\[
\begin{align}
&amp;= \underline{p(1-p)}(1-p) + p\underline{(p)(1-p)} \\
&amp;= \underline{p(1-p)}[(1-p) + p] \\
&amp;= p(1-p)[1-p+p] \\
&amp;= p(1-p)[1] \\
&amp;= p(1-p)
\end{align}
\tag{2.10}
\]</span>
The variance of a Bernoulli random variable is therefore <span class="math inline">\(p(1-p)\)</span>. These equations help us quickly calculate important aspects of Bernoulli distributions, but they will also help us derive the expectation and variance of our next important distribution. An example will kick us off.</p>
<div class="example">
<p><span id="exm:unlabeled-div-28" class="example"><strong>Example 2.12  </strong></span>Suppose an individual who really likes lottery tickets has a .01 chance of winning any money from their favorite scratcher. They purchase 100 lottery tickets. What is the average number of winning lottery tickets we would expect from such a large purchase?</p>
<p>First, observe that we are talking about a Bernoulli random variable (100 of them in fact) with <span class="math inline">\(p = 0.01\)</span>. Our pmf is therefore</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
.01 &amp; x = 1 \\
.99 &amp; x = 0
\end{cases}
\]</span>
From our derivations above, we also know that the expectation of our random variable is .01. We also know that (for the sake of completeness) its variance is</p>
<p><span class="math display">\[
p(1-p) = .01(1-.01) = 0.0099
\]</span>
We are, however, asked to determine the expectation of 100 scratchers. You can think about each lottery ticket as a single random variable. That is we have a set of 100 random variables</p>
<p><span class="math display">\[
\{X_1, X_2, X_3,...,X_{100}\}
\]</span>
The key observation here is that their <em>sum</em> represents the number of successful lottery tickets. That is, if we added them all up, e.g.,</p>
<p><span class="math display">\[
\sum_{i-1}^{100} X_i
\]</span>
then the sum of their outcomes would be the number of winning lotto tickets. For example, if we had 3 winners, we would get three ones and 97 zeros. The total sum would be 3. Indeed, we know how to calculate the expectation of a sum of random variables thanks to the properties of the expectation.</p>
<p><span class="math display">\[
\begin{align}
E(\sum_{i=1}^{100} X_i) &amp;= E(X_1) + E(X_2) + E(X_3) + ... + E(X_{100}) \\
&amp;= p + p + p + ... + p \\
&amp;= 100p \\
&amp;= 100(.01) = 1 \\
\end{align}
\]</span>
The lotto ticket enthusiast shall expect to have 1 winning ticket on average. That’s not a stupendously lucrative hobby, but who says hobbies have to be profitable.</p>
<p>To close out this example, let’s think about <span class="math inline">\(\sum_{i=1}^{100} X_i\)</span>. The sum of these random variables is itself a random variable. Call it <span class="math inline">\(Z\)</span>. Note that we have already looked at similar variables as in examples <a href="discrete-random-variables.html#exm:expectationsum">2.5</a> and <a href="set-theory-and-probability.html#exm:dice">1.22</a>. Although, we were able to determine the expectation of this variable <span class="math inline">\(Z\)</span> using our knowledge of the expectation and Bernoulli random variables, we do not know its pmf.</p>
<p>Well, you’re in for a treat! We shall next discuss the pmf of this random variable (and others like it). Fair warning: This is one of my favorite pmfs of all time.</p>
</div>
</div>
<div id="the-binomial-distribution" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> The binomial distribution<a href="discrete-random-variables.html#the-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The binomial distribution is the pmf of the sum of <span class="math inline">\(n\)</span> Bernoulli random variables. In our last example, we saw an example of a binomial distribution with 100 Bernoulli random variables, and we were already equipped to calculate its expectation (and variance if independence is assumed). Instead of throwing the whole distribution at you at once, we’ll work up from a single example.</p>
<div class="example">
<p><span id="exm:binom" class="example"><strong>Example 2.13  </strong></span>Suppose we have three independent Bernoulli random variables - <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span> - with with <span class="math inline">\(p = 1/3\)</span>, and <span class="math inline">\(1-p = 1-1/3 = 2/3\)</span>. We would like to know the probability that they sum to the value of 1. In order to get a value of 1 from our sum, we will need exactly 1 success and 2 non-successes at the resolution of our random process (e.g. flipping three coins and hoping for heads). There are three such combinations of successes and non-successes that will yield this result. I’ve listed each in a set below. Note that the order of the digits in each outcome corresponds to variables 1, 2, and 3, respectively.</p>
<p><span class="math display">\[
\{1\,0\,0, 0\,1\,0, 0\,0\,1\}
\]</span>
Each of these outcomes in our three-variable sample space would result in a value of 1. For example, with regards to the first element, we have <span class="math inline">\(1 + 0 + 0 = 1\)</span>. The event <span class="math inline">\(Y = \{1\,0\,0, 0\,1\,0, 0\,0\,1\}\)</span> therefore fully specifies the scenario where our Bernoulli random variables sum to 1. We have learned all the tools to calculate the probability of this event. We will first need the probability of each of the outcomes in the set <span class="math inline">\(Y\)</span>. Our random variables are independent, so we can multiply <span class="math inline">\(P(X_1 = x_1)\)</span>, <span class="math inline">\(P(X_2 = x_2)\)</span>, and <span class="math inline">\(P(X_3 = x_3)\)</span> to determine the probability of each of the outcomes in <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[
P(\{1\,0\,0\})=(P(X_1 = 1)P(X_2 = 0)P(X_3 = 0) = 1/3 \times 2/3 \times 2/3 = 4/27 \\
P(\{0\,1\,0\})=(P(X_1 = 0)P(X_2 = 1)P(X_3 = 0) = 2/3 \times 1/3 \times 2/3 = 4/27 \\
P(\{0\,0\,1\})=(P(X_1 = 0)P(X_2 = 0)P(X_3 = 1) = 2/3 \times 2/3 \times 1/3 = 4/27
\]</span>
Therefore each has the exact same probability, and by the law of addition, <span class="math inline">\(P(Y) = 4/27 + 4/27 + 4/27 = 12/27 = 4/9\)</span>. Importantly, note how each of the combinations has exactly the same probability. This is because each has the same number of successes and the same number of non-successes. A little algebra helps to see this. Let’s take a closer look at the combination - <span class="math inline">\(\{0\,1\,0\}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
P(\{1\,0\,0\})=(P(X_1 = 0)P(X_2 = 1)P(X_3 = 0) &amp;= (1-p)p(1-p) \\
&amp;= p(1-p)(1-p) \\
&amp;= p^1(1-p)^2 \\
&amp;= (1/3)^1(1-1/3)^2
\end{align}
\]</span>
Note that the second line from the top is just a rearrangement of factors thanks to the commutative property of multiplication. Looking carefully, we see that we have <span class="math inline">\(p^1\)</span> because we have 1 success, and we have <span class="math inline">\((1-p)^2\)</span> because we have two non-successes. You could rearrange the factors of the other combinations and get the same result. That is to say, each outcome has the same probability because they have the same number of successes and non-successes.</p>
</div>
<p>There are a couple of important things to note from the last example. If we are looking to determine the pmf of a random variable <span class="math inline">\(Z\)</span> - where <span class="math inline">\(Z\)</span> is the sum of <span class="math inline">\(n\)</span> Bernoulli random variables (e.g., <span class="math inline">\(Z = X_1 + X_2 +...+X_n\)</span>) - we will need two pieces of information. The first thing we will need is the number of ways we can get a given value of <span class="math inline">\(Z\)</span>. In the example above, we found that there were three different ways to get a value of 1. We enumerated them by hand, but you can see that this process would be incredibly cumbersome for large <span class="math inline">\(n\)</span>’s. The second thing we shall need is the probability of single outcome where <span class="math inline">\(Z=z\)</span>. We only need the probability of 1, because each possible combination will have the same number of successes and non-success, and therefore, the same probability. In our last example, we had three possible combinations - each with a probability of 4/27, and observe</p>
<p><span class="math display">\[
\frac{4}{27} + \frac{4}{27} + \frac{4}{27} = 3\times\frac{4}{27}
\]</span></p>
<p>as we calculated before. We shall discuss each of these - the number and the probability - in turn.</p>
<p>First we will discuss the number of combinations, and to do this we will need a little bit of review. Specifically, we will need to know how to calculate a <em>factorial</em>. The factorial looks a little intimidating, but it’s not too hard to unpack. Given some integer <span class="math inline">\(n\)</span>, a factorial is defined as</p>
<p><span class="math display">\[
n! = n(n-1)(n-2)...(1)
\]</span>
so for example, if <span class="math inline">\(n = 3\)</span> then,</p>
<p><span class="math display">\[
3! = 3\cdot2\cdot1 = 6
\]</span>
We multiply <span class="math inline">\(3\)</span> by all the integers less than <span class="math inline">\(3\)</span> - all the way down to 1. Now you may recall (or be wondering) what we do with <span class="math inline">\(0!\)</span>. A zero factorial is defined to be 1, so</p>
<p><span class="math display">\[
0! = 1
\]</span></p>
<p>With that brief review out of the way, we now return to calculating the number of combinations. Supposing we have <span class="math inline">\(n\)</span> Bernoulli random variables, and <span class="math inline">\(z\)</span> successes, then the number of combinations resulting in the value <span class="math inline">\(z\)</span> is calculated</p>
<p><span class="math display">\[
[Number\: of \: Combinations]=\frac{n!}{z!(n-z)!}
\]</span></p>
<p>This equation is often symbolically referenced as <span class="math inline">\({n \choose z}\)</span>. That is</p>
<p><span class="math display" id="eq:nchoosez">\[
{n \choose z} = \frac{n!}{z!(n-z)!}
\tag{2.11}
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-29" class="example"><strong>Example 2.14  </strong></span>Returning to our previous example with <span class="math inline">\(n = 3\)</span>, we can now quickly calculate the number of ways to get a sum of 1.</p>
<p><span class="math display">\[
\begin{align}
{3 \choose 1} &amp;= \frac{3!}{1!(3-1)!} \\
&amp;= \frac{3 \cdot 2 \cdot 1}{1 \cdot 2 \cdot 1}
\end{align}
\]</span></p>
<p>Then if we cancel out common multiples in the numerator and denominator, we get</p>
<p><span class="math display">\[
\frac{3 \cdot 2 \cdot 1}{1 \cdot 2 \cdot 1} = \frac{3}{1} = 3
\]</span></p>
<p>which is exactly what we were looking for.</p>
</div>
<p>Now that we know how to calculate the number of combinations necessary for <span class="math inline">\(z\)</span> successes, we shall turn our attention to the probability of a single combination. Looking back at example <a href="discrete-random-variables.html#exm:binom">2.13</a>, a single outcome had one success and two non-successes. The probability of a single was therefore</p>
<p><span class="math display">\[
p(1-p)(1-p) = p(1-p)^2
\]</span></p>
<p>Generally speaking, we can calculate the probability of a <em>single</em> combination of <span class="math inline">\(z\)</span> successes and <span class="math inline">\(n-z\)</span> non-successes by</p>
<p><span class="math display" id="eq:singleprob">\[
p^z(1-p)^{n-z}
\tag{2.12}
\]</span></p>
<p>In our example, we had <span class="math inline">\(z = 1\)</span> and <span class="math inline">\(n-z = 3-1 = 2\)</span>, so we had <span class="math inline">\(p(1-p)^2\)</span> for each of the outcomes. Indeed, this agrees with our algebraic manipulation at the end of example <a href="discrete-random-variables.html#exm:binom">2.13</a>.</p>
<p>Now we are ready to put it all together. The pmf of the sum of <span class="math inline">\(n\)</span> Bernoulli random variables, each with probability of success <span class="math inline">\(p\)</span>, is</p>
<p><span class="math display" id="eq:binom">\[
p(x) =
\begin{cases}
{n \choose z} p^z(1-p)^{n-z} &amp; z = 0,1,...,n \\
0 &amp; Everywhere\: else
\end{cases}
\tag{2.13}
\]</span>
The <span class="math inline">\({n \choose z}\)</span> gives us the number of ways to get a value of z, and the <span class="math inline">\(p^z(1-p)^{n-z}\)</span> gives us the probability for a single way. If we plugged our first example into this equation, then we get</p>
<p><span class="math display">\[
3 \times \frac{4}{27} = \frac{4}{9}
\]</span>
as we observed before when we calculated the probability by hand. The binomial distribution is often referenced as <span class="math inline">\(B(n,p)\)</span> - where <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> are parameters. To indicate the distribution of <span class="math inline">\(X\)</span> we would write <span class="math inline">\(X \sim B(n,p)\)</span>, meaning <span class="math inline">\(X\)</span> is a random variable with a binomial distribution, a.k.a., a binomial random variable. Not all distributions have a special symbol, but when they do we will also sometimes use the shorthand above.</p>
<p>We now turn our attention to the expectation and variance of a binomial random variable. Our random variable - call it <span class="math inline">\(Z\)</span> - is just the sum of <span class="math inline">\(n\)</span> indepndent Bernoulli random variables with parameter <span class="math inline">\(p\)</span>, so the expectation is calculated as</p>
<p><span class="math display">\[
E(Z) = E(\sum_{i=1}^{n}X_i)
\]</span>
And each of the <span class="math inline">\(X_i\)</span>’s has <span class="math inline">\(E(X_i) = p\)</span>, so we would sum <span class="math inline">\(p\)</span> a total of <span class="math inline">\(n\)</span> times. That is,</p>
<p><span class="math display" id="eq:binomexpectation">\[
E(Z) = E(\sum_{i=1}^{n}X_i) = \sum_{i=1}^{n}p = np
\tag{2.14}
\]</span></p>
<p>where <span class="math inline">\(\sum_{i=1}^{n}p\)</span> means that for each <span class="math inline">\(X_i\)</span> we add another <span class="math inline">\(p\)</span>, and as such, <span class="math inline">\(E(X) = np\)</span>. The logic behind the variance is very similar. Our Bernoulli random variables are independent, so we can apply property 4 of the variance. Recall that the variance of a Bernoulli random variable is <span class="math inline">\(p(1-p)\)</span>, so</p>
<p><span class="math display" id="eq:binomvariance">\[
Var(Z) = Var(\sum_{i=1}^{n}X_i) = \sum_{i=1}^{n}p(1-p) = np(1-p)
\tag{2.15}
\]</span></p>
<p>For each random variable, we add a <span class="math inline">\(p(1-p)\)</span>, so <span class="math inline">\(Var(Z) = np(1-p)\)</span>. Let’s close this section out with a few examples of the binomial distribution.</p>
<div class="example">
<p><span id="exm:unlabeled-div-30" class="example"><strong>Example 2.15  </strong></span>Suppose we are flipping 4 fair coins - each with <span class="math inline">\(p=1/2\)</span> probability of flipping heads. Further suppose that we assign <span class="math inline">\(H\)</span> the value of 1 and <span class="math inline">\(T\)</span> the value of zero, and sum the flipped outcomes. Our new random variable has a space of <span class="math inline">\(\{0,1,2,3,4\}\)</span>. Let’s call this new (binomial) random variable <span class="math inline">\(Z\)</span>. In this case, what is the probability of flipping zero heads - <span class="math inline">\(P(Z = 0)\)</span>? What is the probability of flipping <em>at least</em> 1 head - <span class="math inline">\(P(Z \geq 1)\)</span>? What are the expectation and variance of this new random variable <span class="math inline">\(Z\)</span>.</p>
<p>For the first part of the question, we have <span class="math inline">\(n = 4\)</span>, <span class="math inline">\(p = 1/2\)</span>, and <span class="math inline">\(z = 0\)</span>, so let’s plug these into the pmf for a binomial distribution.</p>
<p><span class="math display">\[
\begin{align}
P(Z = 0) &amp;= {4 \choose 0} 1/2^0(1-1/2)^{4-0} \\
&amp;= \frac{4!}{0!(4-0)!}(1/2)^0(1-1/2)^{4-0} \\
&amp;= \frac{4 \cdot 3 \cdot 2 \cdot 1}{1 \cdot 4 \cdot 3 \cdot 2 \cdot 1}(1/2)^0(1-1/2)^{4-0}
\end{align}
\]</span></p>
<p>Notice how the factors are the same for the top and bottom of <span class="math inline">\(\frac{4 \cdot 3 \cdot 2 \cdot 1}{1 \cdot 4 \cdot 3 \cdot 2 \cdot 1}\)</span>, so we can cancel out all the common factors and get 1. Then</p>
<p><span class="math display">\[
\begin{align}
&amp;= (1)(1/2)^0(1-1/2)^{4-0} \\
&amp;= (1)(1/2)^0(1/2)^{4} \\
&amp;= (1/2)^{4} = 1/16
\end{align}
\]</span>
where the last step follows because a number raised to the 0th power is 1. Therefore, <span class="math inline">\(P(Z = 0) = 1/16\)</span>. The second part follows from this first part of the question. Let’s call the space of our random variable <span class="math inline">\(S = \{0,1,2,3,4\}\)</span>. Note that the set <span class="math inline">\(\{Z \geq 1\} = \{1,2,3,4\}\)</span> is the complement of <span class="math inline">\(\{Z = 0\}\)</span>. Therefore, we can use equation <a href="set-theory-and-probability.html#eq:pcomplement">(1.11)</a>.</p>
<p><span class="math display">\[
P(Z \geq 1) = 1 - P(Z = 0) = 1 - 1/16 = 15/16
\]</span>
Finally the expectation and variance of this random variable is</p>
<p><span class="math display">\[
E(Z) = np = 4(1/2) = 2
Var(Z) = np(1-p) = 4(1/2)(1-1/2) = 1
\]</span>
respectively.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-31" class="example"><strong>Example 2.16  </strong></span>Suppose there are 30 students in a particularly difficult course at College of Learning (a preeminent college). Over many many years, researchers at the college determined that the probability of passing this very difficult course is about .25. What is the probability of 0 students passing the course, and what is the probability of at least 1 student passing the course? What is the expectation of the number of students passing?</p>
<p>Here we have <span class="math inline">\(p = .25\)</span> and <span class="math inline">\(n=30\)</span>. Our binomial distribution therefore looks like this:</p>
<p><span class="math display" id="eq:binom">\[
p(x) =
\begin{cases}
{30 \choose z} .25^z(1-.25)^{30-z} &amp; z = 0,1,...,30 \\
0 &amp; Everywhere\: else
\end{cases}
\tag{2.13}
\]</span></p>
<p>With regards to the first part of the question, we have <span class="math inline">\(z = 0\)</span>, and the probability is calculated</p>
<p><span class="math display">\[
\begin{align}
P(Z = 0) &amp;= {30 \choose 0} .25^0(1-.25)^{30-0} \\
&amp;=\frac{30!}{0!(30-0)!}(.75)^{30} \\
&amp;=\frac{30!}{30!}(.75)^{30} = .00019
\end{align}
\]</span>
<span class="math inline">\(P(Z=0) = 0.00019\)</span> (after some rounding), and we can proceed with the second part as we did in the last example.</p>
<p><span class="math display">\[
P(Z \geq 1) = 1 - P(Z=0) = 1 - 0.00019 = 0.99981
\]</span>
Finally, the expectation of our random variable is <span class="math inline">\(E(Z) = .25(30) = 7.5\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-32" class="example"><strong>Example 2.17  </strong></span>We’ll close out the section with one final example that is a little less straight forward (but very interesting). Suppose a casino has a lottery with a <span class="math inline">\(p = 1/100000\)</span> chance to win. Multiple winners are possible, such that one person winning does not affect the odds of another person winning. There are 10000 gamblers who play this lottery every time. If these gamblers return to the casino every week for 10 years (520 weeks), what is the probability that at least one gambler wins this lottery twice during the period?</p>
<p>We will first need the probability that a single gambler wins the lottery twice. That is, if we call the number of wins <span class="math inline">\(Z\)</span> then we will need <span class="math inline">\(P(Z \geq 2)\)</span>. To do this, we will calculate <span class="math inline">\(P(Z=0)\)</span> and <span class="math inline">\(P(Z=1)\)</span>. Here we have <span class="math inline">\(n = 520\)</span> - corresponding to 520 weekly attempts over 10 years - and <span class="math inline">\(p = 1/100000\)</span>, so</p>
<p><span class="math display">\[
P(Z=0) = p(0) = {520 \choose 0} \left(\frac{1}{100000}\right)^0\left(1-\frac{1}{100000}\right)^{520-0} \approx 0.99481\\
P(Z=1) = p(1) = {520 \choose 1} \left(\frac{1}{100000}\right)^1\left(1-\frac{1}{100000}\right)^{520-1} \approx 0.00517
\]</span>
Note that I’ve used a computer for these calculations, and if you wish to work through the problem yourself, I suggest you use one as well. In order to calculate <span class="math inline">\(P(Z \geq 2)\)</span>, we proceed as in the previous examples.</p>
<p><span class="math display">\[
P(Z \geq 2) = 1 - (P(Z = 0) + P(Z=1)) = 1 - .99998 = 0.00002
\]</span>
Now many people might stop there, and conclude that the probability we seek is <span class="math inline">\(0.00002\)</span>. However, we aren’t looking for the probability that a <em>single</em> person wins the casino lottery, we are looking for the probability that one person out of the 10000 weekly gamblers wins twice. Now we have another binomial distribution, this time each gambler is a Bernoulli random variable - each with <span class="math inline">\(p = 0.00002\)</span> of winning the lottery twice.</p>
<p>First let’s calculate the probability that no gamblers win twice. With <span class="math inline">\(p = 0.00002\)</span> and <span class="math inline">\(n = 10000\)</span>,</p>
<p><span class="math display">\[
p(0) = {10000 \choose 0} (0.00002)^0(1-0.00002)^{10000-0} \approx 0.81873
\]</span>
Therefore, the probability that 1 or more gamblers wins this lottery twice is</p>
<p><span class="math display">\[
1 - 0.81873 = 0.18127
\]</span>
Indeed, that is a low probability, but it is much much higher than the probability for a single person. Sometimes during the chaotic news cycle, we catch the story of an individual who won some lottery twice. Our minds jump directly to the probability of an individual winning the lottery. It’s a perfectly human thing to do, but the probability is indeed much higher when considering the entire population of lottery players. Isn’t that interesting!? And we’ve learned it all thanks to the binomial distribution.</p>
</div>
</div>
<div id="the-poisson-distribution" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> The poisson distribution<a href="discrete-random-variables.html#the-poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The poisson distribution is the pmf of a random variable representing the count of something in a fixed time period - e.g. the number of people entering a building in an hour, the number of requests received in a week, etc. Here we shall refer to a random variable with a poisson distribution as a poisson random variable. As you will see, the poisson distribution is strongly related to the binomial distribution. In fact, it’s derived from it. We will unpack the intuition here.</p>
<p>Suppose we were interested in counting the number of cars that pass through an intersection during the same 1 hour period every Monday. Let’s further suppose that we aren’t particularly good at counting, so instead we divide the time period into 10 equal intervals. If a car appears during an interval we assign a 1 to the interval and wait for the next interval to start. It doesn’t matter how many cars appear in a given interval. We simply assign the interval a 1 if <em>any number</em> of cars appear. Otherwise, we assign a zero. At the end we sum up all our 1’s and 0’s across the intervals to get a count of cars.</p>
<p>If the probability (call it <span class="math inline">\(p_1\)</span>) of a car appearing in an interval is constant across the intervals, and the occurrence of a car doesn’t change the likelihood another car will appear later (the car appearance events are independent), then you can think of our counting method as a binomial distribution. In this case, we would have a random variable with a <span class="math inline">\(B(10,p)\)</span> distribution because we would be summing 10 Bernoulli random variables. However, if you are tracking to this point, you will probably have noticed that <em>this is not a very good way to count</em>. You would end up counting a maximum of 10 cars, and if more than one car appeared during an interval 10 could certainly be an underestimate.</p>
<p>We could make our counting a little more accurate if we divided the time period into more intervals - say to 20. In this case the intervals would be shortened, so the probability that a car appeared in a given interval would get smaller. That is, we now have a random variable with a <span class="math inline">\(B(20, p_2)\)</span> where <span class="math inline">\(p_2 &lt; p_1\)</span>. This is <em>still not a very good way to count</em>, but it is a little bit better. Because our interval is smaller, the chance that two cars appear during the same interval is reduced, and furthermore, we can now get a maximum of 20 cars if a car appears in each interval.</p>
<p>Suppose we continue with this logic of splitting the time period into smaller and smaller intervals - e.g. <span class="math inline">\(B(30, p_3)\)</span>, <span class="math inline">\(B(100, p_4)\)</span>, <span class="math inline">\(B(1000, p_5)\)</span> with <span class="math inline">\(p_5 &lt; p_4 &lt; p_3 &lt; p_2 &lt; p_1\)</span>, and so on. Eventually, we would make the intervals so small that the occurrence of two cars in the same interval would be virtually impossible. Our method would then be equivalent to a count of cars over the time period.</p>
<p>This is the fundamental logic behind the poisson distribution. We divide up a fixed amount of time into an infinite (or extraordinarily large) number of intervals and we treat each interval - with very small probability <span class="math inline">\(p\)</span> that an event will occur - as a Bernoulli random variable. The sum of these Bernoulli random variables would reflect a count because the intervals are so small such that the chance of two events occurring in the same interval is virtually zero.</p>
<p>With the above in mind, suppose we have a random variable <span class="math inline">\(X\)</span> that represents the count of something. The pmf of this random variable is</p>
<p><span class="math display" id="eq:poisson">\[
P(X = x) = p(x) =
\begin{cases}
\frac{\lambda^x}{x!} e^{-\lambda} &amp; x = 0,1,2,3,4,... \\
0 &amp; x &lt; 0
\end{cases}
\tag{2.16}
\]</span>
Where the parameter <span class="math inline">\(\lambda\)</span> is the average rate of appearance over the fixed time period in question - e.g. the average number of cars that appear over the time period, the average number of people entering a building over a time period, etc. At an abstract level, if we think about the binomial distributions with more and more intervals, <span class="math inline">\(\lambda\)</span> is the expectation of a binomial distribution with a very very large number of intervals and a very small <span class="math inline">\(p\)</span>. Moreover, recall that <span class="math inline">\(e\)</span> represents a special number that can be found on most calculators. It is roughly equal to 2.71828. You should not memorize this number. You should just use your calculator.</p>
<p>Note one further thing about our poisson random variable. Specifically, the support of random variable with a poisson distribution is any integer greater than, or equal to, zero. As such, we have finally arrived at an example of a discrete random variable with a space that is discrete and countable, but infinite. We will have to get clever in calculating certain probabilities with the poisson pmf, and the examples we work through later will elucidate this.</p>
<p>Indeed, I’ll admit that the poisson pmf doesn’t look much like a binomial distribution. You can rest assured that it is derived from the binomial, however. There are a couple of “rigorous” (I try to avoid this word for our purposes but sometimes it’s unavoidable) derivations of the binomial distribution that you will encounter if you journey in mathematics beyond this book. They are a wonderful demonstration of how mathematics can be made arbitrarily difficult. The first could be grasped by Calculus 1 student, and the second would require some background in differential equations and a lot of patience. Both are very interesting.</p>
<p>The expectation and variance of random variable with a poisson distribution are simple to remember. Their derivation are just slightly beyond our grasp, so I’ll write them here. If <span class="math inline">\(X\)</span> is a poisson random variable then</p>
<p><span class="math display">\[
E(X) = Var(X) = \lambda
\]</span>
In words, the variance and expectation of <span class="math inline">\(X\)</span> are equal to our lower-case lambda. Having specified the poisson pmf, variance, and expectation, let’s work out some examples.</p>
<div class="example">
<p><span id="exm:unlabeled-div-33" class="example"><strong>Example 2.18  </strong></span>Suppose the average number of cars passing through an intersection between 5pm and 6pm is 10. What is the probability of exactly 10 cars passing through the intersection? What is the probability of <em>at least one</em> car passing through the intersection? And, finally, what is the mean and variance of the random variable representing car count? We shall deal with each of these in turn.</p>
<p>We are calculating probabilities for the count of an event (cars passing) that has a constant rate of occurrence over a given time period. The poisson distribution will do the trick! Call our random variable <span class="math inline">\(X\)</span>. Here we have an average rate - <span class="math inline">\(\lambda = 10\)</span>, and the first question asks what is <span class="math inline">\(P(X = 10)\)</span>. So</p>
<p><span class="math display">\[
\begin{align}
p(10) = \frac{\lambda^{10}}{10!} e^{-\lambda} &amp;= \frac{10^{10}}{10!} e^{-10} \\
&amp;= \frac{10\cdot10^{9}}{10\cdot9\cdot8\cdot7\cdot6\cdot5\cdot4\cdot3\cdot2\cdot1} e^{-10} \\
&amp;= \frac{10^{9}}{9\cdot8\cdot7\cdot6\cdot5\cdot4\cdot3\cdot2\cdot1} e^{-10} \\
&amp;= \frac{10^{9}}{9!} e^{-10} = 0.12511
\end{align}
\]</span>
Thus, <span class="math inline">\(P(X = 10) = 0.12511\)</span>. The second part of the question is a bit tricky, but we shall implement a strategy we saw demonstrated in the examples for the binomial distribution. Here we are seeking to calculate <span class="math inline">\(P(X\geq1)\)</span>. Our random variable is countably infinite, so we can’t just calculate probabilities for each value greater than zero and sum them. However, recall that the poisson distribution is a pmf, and one of the properties of a pmf is that - given the space of a random variable (<span class="math inline">\(S\)</span>) - then <span class="math inline">\(P(S) = 1\)</span>. Therefore,</p>
<p><span class="math display">\[
\begin{align}
P(X\geq1) &amp;= 1 - P(X = 0) \\
&amp;= 1 - \frac{10^{0}}{0!} e^{-10} \\
&amp;= 1 - \frac{1}{1} e^{-10} = 0.9999546
\end{align}
\]</span>
In other words, there’s a pretty strong chance you’ll see at least one car between 5pm and 6pm at the intersection.</p>
<p>Finally, we shall make quick work of the last part of the question. Now that we know we are working with a poisson random variable, we know that the mean and variance are each equal to <span class="math inline">\(\lambda\)</span>. Thus, <span class="math inline">\(E(X) = Var(X) = 10\)</span>.</p>
</div>
<p>As we noted before, the parameter <span class="math inline">\(\lambda\)</span> may be thought of as the expectation of a binomial random variable with a very large <span class="math inline">\(n\)</span> and a small <span class="math inline">\(p\)</span>. Therefore, may not be surprised to learn that we can sometimes approximate particularly difficult binomial calculations with a poisson pmf.</p>
<div class="example">
<p><span id="exm:unlabeled-div-34" class="example"><strong>Example 2.19  </strong></span>Suppose we want to calculate <span class="math inline">\({1000 \choose 1} .001^{1}(1-.001)^{1000-1}\)</span>. You’ll hopefully recognize this by now as the calculation of <span class="math inline">\(P(X=1)\)</span> for <span class="math inline">\(X \sim B(1000,.001)\)</span>. In this case, <span class="math inline">\(n\)</span> is large and <span class="math inline">\(p\)</span> is small, so we can use a poisson distribution to approximate the computation. For the exact calculation we have</p>
<p><span class="math display">\[
{1000 \choose 1} .001^{1}(1-.001)^{1000-1} = 0.3680635
\]</span>
First, we can calculate the <span class="math inline">\(\lambda\)</span> we need by calculating the expectation of our binomial distribution. That is <span class="math inline">\(E(X) = 1000(.001) = 1\)</span>. In other words, if our binomial random variable represented an fixed amount of time divided into 1000 intervals - each with <span class="math inline">\(p = .001\)</span> of an event occurring, then the average number of events would be 1. We will therefore use <span class="math inline">\(\lambda = 1\)</span> in our approximation. So</p>
<p><span class="math display">\[
\frac{1^{1}}{1!} e^{-1} = 0.3678794
\]</span>
which is indeed very close to the actual value of our binomial computation.</p>
</div>
</div>
<div id="practice-problems" class="section level3 hasAnchor" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Practice Problems<a href="discrete-random-variables.html#practice-problems" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>What is the expectation of a random variable with space = <span class="math inline">\(\{0,1\}\)</span> and <span class="math inline">\(p = .15\)</span> of the random process resulting in one?</p></li>
<li><p>Suppose we have three independent random variables as in problem 1 above. What is the expectation of their sum, and what is the variance of their sum?</p></li>
<li><p>Given five coins, how many different ways can you flip exactly 2 heads? If the probability of flipping heads on each coin is .5, what is the probability of one of the aforementioned ways? What if the probability of getting heads were <span class="math inline">\(p = .25\)</span> on each coin?</p></li>
<li><p>How many different ways can you draw 5 cards from a deck of 52 cards? (Hint: This is a bit outside the scope of the current discussion. Think about a novel way of “drawing” cards where you throw the deck up in the air and count the number of cards that land face up.)</p></li>
<li><p>Suppose the probability of winning a bingo game is .10. A particularly exciting person plays 20 games of bingo. What is the probability they they win zero games? What is the probability that they win <em>at least</em> one game?</p></li>
<li><p>Suppose June the cat goes outside 3 times a day - seven days a week. When she goes outside, she has a probability of .20, that she’ll catch a moth. What is the probability that she catches two or more moths in a day? What is the expected value and variance of the number of moths June will catch in a day?</p></li>
<li><p>From the last problem, determine the probability that June has at least 1 day in a week where she catches two moths (a.k.a. a very good day for an apex predator)?</p></li>
<li><p>A researcher goes into the quad at the same time every day and counts students who pass left-to-right over a specific point. Over many observational periods, She has determined that, on average, 20 students pass over the point. What is the probability that 10 students pass over the point in a given observational period? What is the expectation and variance of this random variable?</p></li>
<li><p>In the problem above, suppose that a second researcher counts students who pass right-to-left during the same observational period. He finds that 5 students on average pass right to left during the period. Given this information, what is the expected value of the count of students passing over the point - regardless of direction?</p></li>
<li><p>Estimate the probability of a sum of 4 from random variable with <span class="math inline">\(X \sim B(10000,.0001)\)</span>.</p></li>
</ol>
</div>
<div id="practice-problem-answers" class="section level3 hasAnchor" number="2.4.5">
<h3><span class="header-section-number">2.4.5</span> Practice Problem Answers<a href="discrete-random-variables.html#practice-problem-answers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(0.15\)</span></p></li>
<li><p><span class="math inline">\(0.45\)</span>, <span class="math inline">\(0.3825\)</span></p></li>
<li><p><span class="math inline">\(10\)</span>, <span class="math inline">\(.5^5\)</span>, <span class="math inline">\(.25^2\cdot.75^3\)</span></p></li>
<li><p><span class="math inline">\(2598960\)</span></p></li>
<li><p><span class="math inline">\(0.12158\)</span>, <span class="math inline">\(0.87842\)</span></p></li>
<li><p><span class="math inline">\(0.104\)</span>, <span class="math inline">\(0.6\)</span>, <span class="math inline">\(0.48\)</span></p></li>
<li><p><span class="math inline">\(0.53637\)</span></p></li>
<li><p><span class="math inline">\(0.00582\)</span>, <span class="math inline">\(20\)</span>, <span class="math inline">\(20\)</span></p></li>
<li><p><span class="math inline">\(E(X_{left}) + E(X_{right}) = 20 + 5 = 25\)</span></p></li>
<li><p><span class="math inline">\(0.01533\)</span></p></li>
</ol>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="set-theory-and-probability.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-discreterandomvariables.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.pdf", "_main.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
