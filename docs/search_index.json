[["index.html", "Behavioral Statistics: Theory and Application Preface 0.1 You can learn statistics 0.2 Why I wrote this book 0.3 What content to expect in this book 0.4 What you’ll need to know to get the most out of this book 0.5 Future editions of this book", " Behavioral Statistics: Theory and Application Paul Meinz, Ph.D Preface 0.1 You can learn statistics Many textbooks in mathematics begin with a useful high level overview of the content of the book. We’ll get to that in a moment. I want to start this one off a little differently: You can learn statistics. I believe that you can learn statistics and I’m happy you have opened this book to do it. Statistics is a wonderful field, and I am excited for you to learn it. I believe in you. 0.2 Why I wrote this book First and foremost, I wrote this book because I’m excited about statistics (really excited; who writes a textbook for fun?), and I want other people to be excited about statistics too. The second and most important reason I wrote this textbook is because I would have appreciated a deeper mathematical understanding when I was learning statistics. Indeed, as a graduate student in Psychology, I became very comfortable with the applied end of statistics. I nevertheless found myself approaching statistics like a recipe book, e.g., if you have this type of data, it should be analyzed in this way. I did eventually pick up semi-mathematical “explanations” for why certain analyses were appropriate, and although these explanations were accurate (in a sense) they were sometimes quite amorphous. I have grown to appreciate the concrete and exact nature of mathematics - particularly the field of mathematical statistics. This book is therefore meant to teach the a deeper mathematical theory in addition to the application of statistics. My hope is that some curious student (like myself long ago) can have their curiosity assuaged. 0.3 What content to expect in this book To that end, this book will review many concepts learned in an undergraduate behavioral statistics course, but with more mathematical emphasis. It will proceed in three parts: Chapters 1, 2, and 3 give a background on probability and probability distributions. Within behavioral statistics courses, we often discuss a vaguely defined “population” of study. These three chapters define more concretely what we mean when we refer to a population or populations. Hint: Populations can be thought of as random variables and their corresponding probability distributions. We’ll dig into this later. Chapter 4 is our first foray into statistical inference. While Chapters 1, 2, and 3 tell us about populations, this chapter will begin our journey of inferring some basic things about these probability distributions. We will learn about the Law of Large Numbers and the amazing Central Limit Theorem. Chapter 5 (and the chapters thereafter) will introduce us to hypothesis testing - the cornerstone of statistical inference in the behavioral sciences. We will cover several statistical techniques such as z-tests, t-tests, correlation, chi-square tests, and ANOVA. Thank you for reading! 0.4 What you’ll need to know to get the most out of this book The most important “thing” you need to read this book is excitement. I would not describe myself personally as a mathematical prodigy in any way. I’m just excited about the subject, and that provides me with the extra boost to understand the difficult stuff. The second thing you’ll need is an okay grasp of algebra. If you are rusty, I suspect enough will come back to you to understand everything in this book. I’ll try to give gentle reviews where I can throughout. There is lots of content online (like this book) that would offer an awesome review of algebra, so if you are confused and I don’t provide a review, you could almost certainly find something to get you up to speed. 0.5 Future editions of this book This book is a working document. It was created based on some hand written notes I developed whilst teaching my colleagues. I am still teaching my colleagues and I shall try to add chapters in pace with that endeavor. I also plan to work in practice problems and tutorials on R sometime in the future. Stay tuned. "],["set-theory-and-probability.html", "Chapter 1 Set Theory and Probability 1.1 Sets and Sample Spaces 1.2 What is probability? 1.3 Conditional Probability and Independence 1.4 Extended Examples of Probability", " Chapter 1 Set Theory and Probability In the course of research, behavioral scientists will often take samples of human behavior, e.g., a survey of a group of people, measurements of reaction time, etc. These samples will take some outcome, and most of us understand and have observed the inherent randomness of this act. That is to say, you might take the exact same sample in a carefully controlled circumstance and get different results every time. Statisticians often refer to an analogous circumstance when talking about probability. Suppose we are able to describe all the possible outcomes of our sampling procedure. We take samples over and over again in the same circumstances, and each time the outcome may change. This is referred to as a random experiment. In order to familiarize you with some introductory concepts around probability, we’re going to start in this context of a random experiment. First, we’ll learn about sets and set theory in order to describe our sample space. This will also be useful in describing the form that outcomes of a random experiment may take. Then we’ll more concretely define the randomness of our experiment by talking about probability - the quantification of randomness. 1.1 Sets and Sample Spaces Before we get into it, we aren’t going to learn set theory in it’s entirety. That would be overly (and hilariously) ambitious. What we are going to learn will make understanding later sections easier, get your brain warmed up, and make it a bit easier to understand set notation in other circumstances. The first step in conducting a random experiment is clearly defining all the possible outcomes of your sampling procedure. This is referred to as the sample space. Suppose we have five different possible outcomes in our sample space: A, B, C, D, and E. Let’s call this sample space \\(S\\). Then we would describe it like this: \\[ S = \\{A, B, C, D, E\\} \\tag{1.1} \\] That is to say we’ll describe the set \\(S\\) with curly braces, and on the inside of the curly braces, there shall be a list of distinct elements of our set. The sample space above is an example of a discrete sample space - meaning it either contains a finite number of elements or it is countable. In this case, the sample space has a finite number of elements (five to be exact), so we can call it discrete. Describing and identifying a discrete sample space that is countable but infinite (has an infinite number of elements) is much more difficult. My hope is you can gain a small sense for it from my explanation here. We’ll see examples of it later, so it’s worth a brief consideration. Specifically, a discrete sample space can also contain an infinite number of elements - so long as those elements can be lined up one-to-one with the natural numbers (e.g., 1, 2, 3, 4, …). That is, the elements can be “counted”, albeit infinitely and forever. This is probably not the most intuitive definition for people who don’t think about mathematics every day (it certainly wasn’t for me). In a more reachable sense, a discrete sample space has elements with nothing in between them. In \\(S\\) above, there is nothing in between A and B, for example. If your discrete set had an infinite number of elements, you could order them by some criteria and line them up with the natural numbers. After this alignment, you could find nothing between the first and second elements (or any given sequential pair for that matter). You could just keep counting without worrying that you missed something. In addition to the discrete sample spaces above, we may also find ourselves sampling from a continuous sample space, e.g., all the values between and including 0 and 1 (e.g \\(0 \\leq x \\leq 1\\)). Since you can’t enumerate all the values of a continuous sample space as we did with \\(S\\) (people have “tried”; we’ll talk about one such example later), we use a different form of notation. Suppose we have a continuous sample space \\(R\\), then we would specify it as \\[ R = \\{x\\, | \\, 0 \\leq x \\leq 1 \\} \\tag{1.2} \\] The inclusion criteria for the set is written after the \\(|\\) in the curly braces. The \\(|\\) can be read loosely as meaning “where”, so reading the whole thing in the curly braces left to right you get x where \\(0 \\leq x \\leq 1\\). Therefore, our set is all the \\(x\\)’s from 0 to 1. Note that I will occasionally drop the \\(x\\) and \\(|\\) when it is clear what variable I am referencing. As an aside, continuous sample spaces can be found everywhere in the field of behavioral statistics. For reasons that will become apparent later, we shall nonetheless approach these spaces with slightly less mathematical depth. That said, it is time to turn our attention towards the outcome of our random experiment. Suppose we are randomly drawing a single item from our sample space \\(S = \\{A, B, C, D, E\\}\\). The outcome of our experiment is a single item from our sample space, and we will talk about outcomes in terms of events. Events are described by a subset of distinct elements from the sample space. That is, they may be one or more outcomes combined. For example, we might say the event is \\(C = \\{A, B, C\\}\\), and if our outcome lands in the that subset (as either A, B, or C), then we shall say the event “occurred”. For the sake of brevity, and positivity, I will referred to the occurrence of an event as a “success” (congratulations all around!). You can indicate a set is a subset by: \\[ C \\subset S \\tag{1.3} \\] In this case, we are saying \\(C\\) is a subset of \\(S\\). We might also, for example say that our event is a single element/outcome \\(C_1 = \\{A\\}\\) or the entire sample space \\(C_2 = \\{A,B,C,D,E\\}\\). We can be as creative as we would like to be, so long as the event is in our sample space. Indeed, in practice we might want to get a little more creative in terms of the outcomes of our random experiment. Let’s talk about a slightly more complicated scenario. Example 1.1 Suppose we have two events \\(C_1 = \\{A,B,C\\}\\) and \\(C_2 = \\{B,C,D\\}\\), and we are interested in whether or not our outcome lands in \\(C_1\\) or \\(C_2\\). It’s helpful here to consider the circumstances where we this event would occur. Namely, If the instantiation of our outcome was an A, B, C, or D, then we would be successful (success!). That is to say, we’ve taken all the unique outcomes of either set and combined them into a new set. If our outcome lands in that new subset, we can be confident that \\(C_1\\) or \\(C_2\\) occurred. There’s a handy symbol that indicates this operation: \\[ C_1 \\cup C_2 = \\{A,B,C\\}\\:\\cup\\:\\{B,C,D\\} = \\{A,B,C,D\\} \\tag{1.4} \\] This union operator (\\(\\cup\\)) takes the unique elements of both sets and puts them into a set of their own. In this circumstance it is used synonymously with or. If the event of interest is our outcome landing in \\(C_1\\) OR \\(C_2\\), then we would be successful if the event landed in the union of the two sets, e.g., all the unique elements from both sets. Example 1.2 Here’s another example, suppose we are interested in if our outcome falls into \\(C_1 = \\{A\\}\\) or \\(C_2 = \\{B\\}\\). Then our event would occur if the element was A or B: \\[ \\{A\\}\\:\\cup\\:\\{B\\} = \\{A,B\\} \\] Our random experiment would be successful if our outcome was A OR B. Indeed, the union operator helps us figure out the conditions for success when we are interested in the occurrence of one event OR another. Let us now turn our attention to another equally important operator. Example 1.3 Suppose we have our two previously described events - \\(C_1 = \\{A,B,C\\}\\) and \\(C_2 = \\{B,C,D\\}\\) - each a subset of \\(S\\). Lets further say that we are interested in whether or not both events occur as a result of our random experiment. That is, we shall be successful if our outcome falls in both \\(C_1\\) AND \\(C_2\\). We are therefore interested in the shared elements of our two events. Specifically, just by eyeballing the sets above, B and C are shared by the two events \\(C_1\\) and \\(C_2\\), but if we were unfortunate enough to draw a D or an A, game over. The intersect operator will help us signify this process: \\[ C_1 \\cap C_2 = \\{A,B,C\\} \\cap \\{B,C,D\\} = \\{B,C\\} \\tag{1.5}\\] Unlike \\(\\cup\\), the intersect operator represents and. It tells us the conditions for success if we would like both events to occur (a.k.a., our outcome landing in both events). This operator can create some interesting conundrums, so it’s worthwhile to do another example or two. Example 1.4 This time let’s define our events to be \\(C_1 = \\{A,B\\}\\) and \\(C_2 = \\{C,D\\}\\). Now we’re interested in whether or not our outcome falls in \\(C_1\\) and \\(C_2\\) (What do you think is going to happen here?): \\[ C_1 \\cap C_2 = \\{A, B\\} \\cap \\{C, D\\} = \\{\\} \\] Well, you may have guessed it: Our two sets had no overlap. You couldn’t possibly draw a single element that landed in both the sets, because the sets share no elements. As a result, we get a very special set - a null set - \\(\\{\\}\\) - which is a set with zero elements. In this circumstance, when the intersect of sets results in a null set (a.k.a. they have no overlap), we shall refer to them as being mutually exclusive - because the two sets do not share elements. In terms of probability, mutually exclusive sets can be much easier to work with. We’ll talk about this soon. Now you have some understanding of the two main set operators we’ll need - the union (\\(\\cup\\)) and the intersect (\\(\\cap\\)). We shall need to know a little bit more about the null set, so let’s take a slightly deeper look. Example 1.5 It’s interesting to see how a null set works with other sets that have elements. For example: \\[ \\{A,B,C,D,E\\} \\cap \\{\\} = \\{\\} \\] A set with elements intersected with a null set is a null set. A set with something in it can share nothing with a set that has no elements at all. This is very similar to multiplying an integer by zero. In fact, when you first start writing set notation, you might be inclined to use arithmetic operators with sets, e.g., \\(\\{A,B,C\\} - \\{A,B\\} = \\{C\\}\\) as opposed to \\(\\{A,B,C\\} \\cap \\{A,B\\} = \\{C\\}\\). The null set and intersect gives us a handy way to multiply a set by zero without using \\(\\times\\) or 0. What do you suppose happens when we union a null set with something else? Let’s try it out: \\[ \\{A,B,C,D,E\\} \\cup \\{\\} = \\{A,B,C,D,E\\} \\] A set unioned with a null set is itself. A null set has nothing unique to add to our “something set”. Although the null set is really cool (in my opinion; and I hope your opinion too), working with it is, metaphorically speaking, like talking to the most boring person in the world. You have nothing in common with them (\\(\\cap\\)), and they have nothing to add (\\(\\cup\\)). And, with that, you have learned your first set theory insult. Up to this point, we’ve learned how to describe our sample space, the concept of an outcome and event, how to create new events with union/intersect (or/and) operators, and the fancy null set. That’s about all we need. I would like to finish out this brief section on sets with a discussion of the complement of a subset and a brief treatment of how the previously mentioned operators work in the continuous case. The complement of a subset describes all the elements of a sample space not in that subset. For the sake of consistency, we’ll keep using our sample space \\(S\\) (1.1), and let’s work with \\(Y = \\{A,B,C\\}\\), \\(Y\\subset S\\). The complement of that subset in the sample space would be \\(\\{D, E\\}\\). The operation of taking the complement of a subset is indicated by: \\[ Y^c = \\{A,B,C\\}^c = \\{D,E\\} \\tag{1.6}\\] The complement allows for a wider breadth of operations. Example 1.6 Let’s again consider our sample space \\(S\\). Suppose we were interested in an event that was NOT in the two subsets \\(C_1 = \\{A,B\\}\\) or \\(C_2 =\\{C,D\\}\\). The first thing we would do in this circumstance is maybe consider all the ways our outcome could be unsuccessful (we won’t use “fail” here; too negative). It would be unsuccessful if it landed in \\(C_1\\) or \\(C_2\\). Recall that the \\(\\cup\\) (union) gives us the unique elements in both sets. Let’s take these two sets and union them into a set \\(X\\) together. \\[ C_1 \\cup C_2 = \\{A,B,C,D\\} = X \\] Now that we have our subset of unsuccessful elements, we just take the complement of that: \\[ X^c = \\{E\\} \\] Of course, you might have known the answer to this all along, but it’s nice to see the steps broken down exactly. You could have written it all in one go using parentheses to specify order of operations, e.g.: \\[ (C_1 \\cup C_2)^c = \\{A, B, C, D\\}^c = \\{E\\} \\] Notice how we do the operation in the parentheses before taking the complement. Example 1.7 Let’s try one more example with \\(C_1\\) and \\(C_2\\). Let’s say we defined a successful outcome as all the things NOT in \\(C_1\\) and \\(C_2\\). Notice the key word “and”. Our list of unsuccessful items will be different this time because we are referring to the shared (intersect) of both our subsets. Therefore: \\[ (C_1 \\cap C_2)^c = \\{\\}^c = \\{A,B,C,D,E\\} \\] The two sets have no overlap, so their intersect is a null set, and the complement of the null set is all the elements in our sample space. With a good understanding of some of the basic operators and sets, let’s turn to a brief treatment of the continuous sample space. The continuous case is very similar to the discrete case - although it takes some drawing of number lines if you are rusty with inequalities (no problem). Example 1.8 Suppose we are sampling from a sample space \\(S = \\{x\\,|\\,0 \\leq x \\leq 10 \\}\\) and we are interested in if the outcome lands in \\(C_1 = \\{x\\,|\\,0 \\leq x \\leq 2 \\}\\) OR \\(C_2 = \\{x\\,|\\,2 \\leq x \\leq 5 \\}\\). A successful outcome here would occur if our realized element landed anywhere from zero to five. In other words: \\[ \\{0 \\leq x \\leq 2 \\}\\:\\cup\\:\\{2 \\leq x \\leq 5\\} = \\{0 \\leq x \\leq 5\\} \\] Let’s keep the same continuous sample space and subsets as above, and this time let’s look at the intersect of the two sets: \\[ \\{0 \\leq x \\leq 2 \\}\\:\\cap\\:\\{2 \\leq x \\leq 5\\} = \\{2\\} \\] The two sets only overlap at exactly \\(x = 2\\), so their intersect is just a set with the value 2. Let’s do a couple more so you can get a sense for it. Example 1.9 Suppose we have a sample space \\(Q = \\{x\\,|\\, 5 \\leq x \\leq 10\\}\\) and define \\(C \\subset Q\\) where \\(C = \\{x\\,|\\, 5 \\leq x &lt; 7\\}\\). That is to say, \\(C\\) is a subset of Q, and its interval includes the lower side (5) but it doesn’t include the upper side (7). What is the complement of \\(C\\)? \\[ C^c = \\{5 \\leq x &lt; 7\\}^c = \\{7 \\leq x \\leq 10\\} \\] The complement includes 7 up to 10. In the plot below, you can see a visual depiction of this. The top line represents the subset \\(C\\), and the bottom line represents the complement. The open circle at the end of our subset \\(C\\) indicates that the value 7 is not included in its range. The elements of C are strictly less than 7. This is in contrast to the complement line below - which has a solid black dot indicating 7’s inclusion due to its exclusion from \\(C\\). Thus the complement of \\(C\\) ranges from 7 to 10. Example 1.10 One more comment is worth a mention with regards to continuous sample spaces. Suppose we have two subsets of our space \\(Q\\) that don’t overlap in any way. For example, take \\(C_1 = \\{x\\,|\\, 5 \\leq x &lt; 7\\}\\) and \\(C_2 = \\{x\\,|\\, 9 \\leq x \\leq 10\\}\\). Then you would simply write their union as: \\[ \\{\\, 5 \\leq x &lt; 7\\} \\cup \\{\\, 9 \\leq x \\leq 10\\}\\] There is no simplified way to write this in set notation. Note the intersect of these two sets would be \\(\\{\\}\\) - a null set. 1.1.1 Repeated Samples The previous introduction to sets was dedicated to a random experiment where we only draw a single element from our sample space. In practice, researchers and statisticians are often sampling/conducting a random experiment repeatedly. In this section we’ll talk about how to describe the sample space of an experiment that involves multiple draws. Note, for the purposes of our learning, we shall only be discussing sampling with replacement. That is, we will be talking about circumstances where the 1st, 2nd, 3rd, … , nth outcome gets returned to the sample space prior to the next draw. This is opposed to the circumstance where an outcome of a single sample reduces the size of the sample space each time. For example, suppose we have a sample space, \\(S = \\{A,B,C\\}\\). On our first experiment, we randomly draw an \\(A\\). In sampling without replacement, the sampling space for the next draw would be \\(\\{B,C\\}\\) (or \\(\\{A\\}^c)\\) and in sampling with replacement \\(A\\) would be returned to the space. To understand how to describe the sample spaces mentioned above, we’ll turn to a good-old-fashioned coin toss (the mainstay of every introductory probability book). Suppose we perform a single flip (experiment) from a fair coin. Our sample space would be \\(S = \\{H, T\\}\\) where \\(H\\) is “heads”, and \\(T\\) is “tails”. Now let’s suppose we flip two counts. The sample space would be all the possible permutations of heads and tails from the two flips. It helps to draw draw a tree diagram to get the gist of it. You can follow the branches of the tree above to see each possible permutation of the two flips. For example, suppose the first flip was heads, you would follow the line at the first node (on the left) up the the “H” (heads) node. Then at the “heads” node, the next flip could either be an “H” (heads) or a “T” (tails), corresponding to the top and second from the top nodes on the right. Going down each branch, top-to-bottom, we get all the possible permutations of our two flips - \\(\\{HH,HT,TH,TT\\}\\). Our random experiment is now a selection from this new sample space. Drawing a tree to determine all the permutations of multiple draws, could get quite cumbersome, so it’s convenient to have a formula. In the example above, the first flip could take 2 outcomes. The second flip would then result in two additional branches for each of the outcomes of the first flip. In other words, we have \\(2 \\times 2 = 4\\) possible permutations for the two coins. Had we flipped a third coin, each of nodes for the second flip would then be accompanied by two branches per node, so the calculation would be \\(2 \\times 2 \\times 2 = 8\\) permutations. Indeed, given a sample space that has \\(N\\) total elements/outcomes and \\(k\\) repeated samples, all the possible permutations would be given by: \\[ N^k \\tag{1.7}\\] Note: If we had sampled from several sample spaces, all with a different number of elements, then then we would just multiply the respective element lengths together. I will end this section with an important discussion on notation. As you can see from (1.7), the sample space can grow quite rapidly. For example, four coin flips would have a sample space of 16 elements. Consider the two coin sample space - \\(\\{HH,HT,TH,TT\\}\\). The event of getting a head on the first flip can be described by all the coin flip combinations with heads on the first flip - \\(\\{HH,HT\\}\\). If our random experiment and outcome fell into this subset, we would indeed have flipped a heads on the first coin. Now consider a circumstance where the sample space is much larger (e.g., 10 coins), it would become quickly untenable to write out all the possible permutations. Instead in some circumstances I may use some short hand like or similar to \\(\\{H \\: on \\: 1st \\: flip\\}\\), because I don’t have the rest of my natural born life to list permutations (and you don’t have the rest of your natural born life to read them). Rest assured that these are exactly equivalent, and they shall be associated with the exact same probabilities. Indeed, sometimes the short hand will make it more clear as to what probability we are referring to. 1.2 What is probability? Many of us have an internal sense of this stuff we call probability. In the crisp fall of the California foothills (which is about the time that I’m writing this section), you can ask a local of Placerville how often the apple orchards get overwhelmed with tourists. More often than not, they will give you some percentage of the time, and this time of the year that percentage will undoubtedly be 100%. You might ask a Sacramento resident how confident they are in their local basketball team winning, and indeed, they may also provide you with something close to a percentage of time (my guess is 15%). The two preceding scenarios are examples of the relative frequency approach to probability. In this approach, you can think of probability in the random experiment sense. That is, if we did a random experiment over and over again, we might expect some event to occur a percentage of the time. We shall call this percentage a probability. Within this approach, probabilities can take values anywhere from \\(0\\leq x\\leq1\\). Indeed a “percentage of time” greater than 1 or less than zero would be nonsensical. In the last section we discussed how we might describe sample spaces, outcomes, and events of random experiments. In this section we’ll assign probabilities to them, and we shall adopt the above relative frequency approach interpretation. Suppose we have some event \\(C\\), then the probability set operator - \\(P(C)\\) - takes \\(C\\), and based on a set of rules, returns a probability for that event. The set of rules for the assignment of probability is referred to as a probability distribution. We’ll talk about probability distributions in the next chapter, but for now we’ll just focus on probability and the probability set operator. Calculating probabilities for the continuous sample space can be tricky, so we’ll stick with the discrete case and talk about the continuous later. Example 1.11 Consider for a moment the circumstance at the end of last section: Flipping two fair coins. If you recall, our sample space was \\(S = \\{HH,HT,TH,TT\\}\\). Suppose the probability set operator assigns a probability of 1/4 to each of the outcomes in this sample space. The probability, for example, of the event \\(C = \\{HH\\}\\) would therefore be \\[ P(C) = P(\\{HH\\}) = 1/4 \\] The probability set operator takes the set \\(C\\) and assigns to it a probability. Within the relative frequency approach, the assigned probability of 1/4 means that over many many random experiments, we might see that about 25% of the outcomes are HH. When I have taught this to my colleagues and students, many will point out that they have flipped two fair coins a large number of times and didn’t get exactly 25% of outcomes landing HH (budding statisticians!). That is indeed possible. When I talk about “many” trials, I am referring to an exorbitantly large number of trials (e.g., infinitely large numbers of flips). Indeed, if we were to flip two truly fair coins until the end of time, a total of 1/4 of those flips would be HH. Example 1.12 While you wrap your mind around that, let’s now turn our attention to a more informative example. We’ll stick with fair coins for now. Suppose we wanted to know the probability of getting a heads on the first flip - \\(C = \\{HH,HT\\}\\) - how does the probability set operator go about assigning probabilities to an event with more than one outcome? Well, lucky for us, there is the law of addition for probability. For two events \\(A\\) and \\(B\\) for which \\(A \\cap B = \\{\\}\\) (e.g,. they are mutually exclusive): \\[ P(A \\cup B) = P(A) + P(B) \\tag{1.8}\\] Put in plain language: The probability of our outcome landing in A or B (\\(\\cup\\)) is equal to the probability of A plus the probability of B. This law helps calculate the probability of success if we want our outcome to land in one event OR another event. In this case, how does it apply to our single event \\(C\\) above? Well, recall from our earlier discussion, we said that an event contains a set of distinct elements from the sample space. Because of this, every event can just be written as a union of mutually exclusive events. In our case, \\[ C = \\{HH,HT\\} = \\{HH\\} \\cup \\{HT\\} \\] And with this in mind, we can apply the law of addition: \\[ P(\\{HH\\} \\cup \\{HT\\}) = P(\\{HH\\}) + P(\\{HT\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{2}{4} = \\frac{1}{2} \\] Of course, in practice, you don’t have to write an event as a union of distinct events or outcomes before applying the law of addition. You can simply sum up the respective probability of each outcome in the event. Example 1.13 For example, suppose we were interested in the probability of at least one head. In this case, a successful outcome would be HT, TH, or HH, whereas TT would be unsuccessful. Thus we are considering the probability of the event - \\(C = \\{HT, TH, HH\\}\\), and with the law of addition: \\[ P(\\{HT,TH, HH\\}) = \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} = \\frac{3}{4} \\tag{1.9} \\] The law of addition makes things easier in multiple ways, and it implies another (easier!) way to calculate the probability above. To see this, we shall start with a handy property of the probability set function. Namely, for some sample space \\(S\\): \\[ P(S) = 1 \\tag{1.10}\\] In English, this means that the probability our outcome lands in the sample space is equal to 1. Of course this is true! The sample space is - by definition - the collection of all the possible outcomes of our random experiment. As I said, this property makes some computations easier. Example 1.14 For example, suppose we are interested in the probability of getting at least one head (as above). In this case, we don’t need to go through the rigmarole of enumerating all the combinations with an H. We know there’s exactly one combination with no heads - TT - and the rest of the outcomes (the complement of TT; \\(\\{TT\\}^c\\)) have at least one H. So, let’s say that the event \\(C\\) contains the outcomes with at least one head, and let’s also say that \\(S\\) is our full sample space: Observe that: \\[ C \\cup \\{TT\\} = S \\] Look back at the definition of our two-coin flip space, and you’ll see why I’m right about this. So then we can do this: \\[ \\begin{align} P(C \\cup \\{TT\\}) &amp;= P(S) = 1 \\\\ P(C) + P(\\{TT\\}) &amp;= 1 \\\\ P(C) + P(\\{TT\\}) - P(\\{TT\\}) &amp;= 1 - P(\\{TT\\}) \\\\ P(C) &amp;= 1 - P(\\{TT\\}) \\\\ P(C) &amp;= 1 - \\frac{1}{4} = \\frac{3}{4} \\end{align} \\] Thus, without listing out all the permutations of heads and tails for our two coins, we’ve calculated what we need. That is to say, sometimes it’s much easier to calculate the probability of the complement (in the case above TT) of our event of interest and subtract the probability from 1. More formally, to calculate the probability of some event \\(C\\), we could take its complement \\(C^c\\). Then \\[ P(C) = 1 - P(C^c) \\tag{1.11} \\] Example 1.15 Let’s work in another example of this just to hammer this concept home. Let’s say we have a sample space \\(S = \\{0,1,2,3,4,5,6,7,8,9,10\\}\\), and the probability set operator assigns a probability of 1/11 to each outcome in this sample space. We want to know the probability that our draw will land in the event \\(C = \\{3 \\leq X\\}\\) - an outcome of 3 or more. You might be able to do this in your head. I’m personally too lazy to write out the law of addition for \\(C\\), so we’ll use (1.11). First observe that \\[ C^c = \\{0,1,2\\}\\\\ P(C^c) = \\frac{1}{11} + \\frac{1}{11} + \\frac{1}{11} = \\frac{3}{11} \\] And therefore \\[ \\begin{align} P(C) &amp;= 1 - P(C^c) \\\\ P(C) &amp;= 1 - \\frac{3}{11} \\\\ P(C) &amp;= \\frac{8}{11} \\end{align} \\] Much easier! Okay, I have to stop the narrative now to tell you I have done a great misdeed. For the sake of simplicity I’ve left out an important part of the law of addition (I know: \\(P({Hell}) &gt; 0\\)). Now it’s time for the truth, and I’ll show you what I’ve done and what we need to add. Example 1.16 Let’s define our sample space to be \\(S = \\{1,2,3\\}\\), and suppose each outcome has a probability of 1/3. Define two events \\(C_1 = \\{1,2\\}\\) and \\(C_2 = \\{1,2\\}\\). We would like to know the probability of our outcome landing in \\(C_1\\) or \\(C_2\\). This sounds like a job for the law of addition. However, remember our current version of the law only works for mutually exclusive sets, but \\(C_1 \\cap C_2 = \\{2\\}\\). We could always get around the conundrum describe above by executing the set operation before applying the probability set operator. Then with that new set, we could calculate the probability as in (1.9), e.g., using the first law of addition I described for mutually exclusive events. For example: Observe that \\[ C_1 \\cup C_2 = \\{1,2\\} \\cup \\{2,3\\} = \\{1,2,3\\} \\] And therefore the probability of \\(C_1\\) or \\(C_2\\) is \\[ P(\\{1,2,3\\}) = \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1 \\] You can see that works pretty well. If we want to use the law of addition, we will nevertheless need to make a small addition. Specifically, we shall have to subtract out the intersect (\\(\\cap\\)) between our two sets. That is, given two events \\(A\\) and \\(B\\) \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\tag{1.12} \\] This law says that if we were to simply sum the probability of each event, we would double count the overlap between sets A and B (\\(\\cap\\)) . In the case of our example, we would double count the probability of \\(C_1 \\cap C_2 = {2}\\) twice. If you don’t believe me, look at this. Consider \\(C_1\\) and \\(C_2\\) as defined above \\[ P(C_1) = P(\\{1,2\\}) = P(1) + P(2) \\\\ P(C_2) = P(\\{2,3\\}) = P(2) + P(3) \\\\ \\] So if we applied our first version of the law of addition, then \\[ \\begin{align} P(C_1 \\cup C_2) &amp;= P(C_1) + P(C_2) \\\\ &amp;= P(1) + P(2) + P(2) + P(3) \\\\ &amp;= P(1) + 2P(2) + P(1) \\end{align} \\] We have one too many \\(P(2)\\)s! Indeed, if we counted the probability of 2 twice, we would end up with a nonsensical value of 4/3 (e.g., \\(1/3 + 2 \\times 1/3 + 1/3\\)). So let’s put it all together using the second (more complete) law of addition. Recall that we assigned a probability of 1/3 to each element in our sample space \\[ \\begin{align} P(C_1 \\cup C_2) &amp;= P(C_1) + P(C_2) - P(C_1 \\cap C_2) \\\\ P(C_1 \\cup C_2) &amp;= P(\\{1,2\\}) + P(\\{2,3\\}) - P(\\{2\\}) \\\\ P(C_1 \\cup C_2) &amp;= \\frac{2}{3} + \\frac{2}{3} - \\frac{1}{3} = 1 \\end{align} \\] Fixed! Example 1.17 Using the full version of the law of addition helps to make clear why we can drop the last term for mutually exclusive sets (1.8). Suppose we have two mutually exclusive events \\(A\\) and \\(B\\), each a subset of some sample space \\(S\\). Then, because they are mutually exclusive, \\(A \\cap B = \\{\\}\\). And, indeed, the probability of the null set - \\(P(\\{\\})\\) - is zero. You can’t draw nothing on a random experiment. So in the case of mutually exclusive events \\[P(A \\cup B) = P(A) + P(B) - 0\\] What we just learned will serve as a nice initial step into probability. We shall close this session by further emphasizing that \\(P(A \\cup B)\\) can be thought of as the probability that our outcome lands in A OR B. On the other hand, \\(P(A \\cap B)\\) can be thought of as the probability our outcome is an element shared by A AND B. Indeed, although we can use our set operations and the probability set operator to calculate the ladder, there is another method that we will discuss next. 1.3 Conditional Probability and Independence Having learned a little bit about probability and the law of addition, we will turn to a slightly more complicated topic - conditional probability. Conditional Probability describes the probability of an event given the occurrence of another event. For example, suppose you are on a game show where you select one of four doors. Further suppose that a prize is equally likely to be behind each door. That is, the probability set function assigns a probability of 1/4 to each door. Right before you select a door, the host yells, “stop!”, and then it is revealed that the prize is behind doors 1, 2, or 3. That is some fortuitous news! Common sense says that the probability our prize lands behind door number 4 is now zero, and we should constrain our guess to doors 1, 2, and 3. That is to say, the conditional probability our prize lands behind door 4 is zero, given that the event \\(C = \\{door\\:1,door\\:2,door\\:3\\}\\) has occurred. It is often possible to intuit the conditional probability of an event, but a concrete formula will be essential for more complex scenarios. The conditional probability of an event can be calculated using some familiar terms \\[ \\frac{P(A \\cap B)}{P(B)} = P(A|B) \\tag{1.13} \\] Indeed, the idea of conditional probability can be a sticking point for new learners of probability, so it’s worth doing a few examples. Example 1.18 First, let’s define a sample space - \\(S = \\{1,2,3\\}\\) - and assign a probability of 1/3 to each outcome. Now, let’s define two events - \\(C_1 = \\{1\\}\\) and \\(C_2 = \\{1,2\\}\\). We shall endeavor to calculate \\(P(C_1|C_2)\\). In other words we want to know the probability of a “1” outcome/event in our random experiment, given the event \\(C_2\\) has occurred. Intuitively speaking, if we know that the outcome has landed in \\(C_2 = \\{1,2\\}\\), we may feel as if the probability of getting a 1 is increased. Indeed, the outcome of 3 is now out of the picture. Let’s not lean on our intuition though. Looking at the equation for conditional probability, we shall need two quantities for our calculation - the probability of \\(C_1\\) AND \\(C_2\\) (\\(\\cap\\)) and the probability of \\(C_2\\): The tools in the last section will prove sufficient to calculate these. \\[ P(C_1 \\cap C_2) = P(\\{1,2\\} \\cap \\{1\\}) = P(\\{1\\}) = 1/3 \\\\ P(C_2) = 1/3 + 1/3 = 2/3 \\] Now that we have what we need, it’s just a matter of plugging in the numbers to the conditional probability equation. \\[ \\frac{P(C_1 \\cap C_2)}{P(C_2)} = P(C_1|C_2)\\\\ \\frac{1/3}{2/3} = \\frac{1}{2} \\] The probability of \\(C_1\\) given that \\(C_2\\) has occurred is 1/2. The computation isn’t too hard (although it may be at first), but the interpretation of conditional probability can certainly be tricky. It’s worth unpacking our last example a bit more. One way to envision the last example is to think of a probability pie (stick with me here). In the pie chart below, observe how each of the individual outcomes is assigned an equivalent amount of area. That is, each gets 1/3 of the total probability. Now, we are learn that the event \\(C_2 = \\{1,2\\}\\) has occurred. Put another way, we now know that the outcome from our random experiment has landed somewhere in \\(C_2\\). The wedge in our pie chart corresponding to outcome 3 is removed. You can see this in the pie chart below (it’s now white). Observe that the two remaining outcomes - 1 or 2 - have an equal amount of probability assigned to them. One might intuit that, in the present circumstance, each of the two outcomes would occur about half the time. Indeed, the result of our calculation confirmed this. The conditional probability of getting a 1 - given \\(C_2\\) occurred - is 1/2. Because \\(C_2\\) occurred, the probability of \\(C_1\\) increased from 1/3 to 1/2. Example 1.19 Let’s take a look at an example that will give us another view of things. Suppose we flip two coins and, as in previous examples, our sample space is \\(S = \\{TT,TH,HT,HH\\}\\). Like before, our probability set operator assigns to each a probability of 1/4. In this case, what is the probability of getting a head on the second flip - given we’ve flipped a head on the first? Recall from our previous work, and the note at the end of the section 2.1, that flipping a head on the second coin is described by the event - \\(C_2 = \\{TH,HH\\}\\). Note that here I’ve used \\(C_2\\) to remind you that we are referring to a head on the second flip. Getting a head on the first flip is described by \\(C_1 = \\{HT,HH\\}\\). The probability of each of these events is: \\[ P(C_1) = P(\\{HT,HH\\}) = 1/4 + 1/4 = 1/2 \\\\ P(C_2) = P(\\{TH,HH\\}) = 1/4 + 1/4 = 1/2 \\] In order to calculate the conditional probability - \\(P(C_2|C_1)\\) - we shall also need \\(P(C_1 \\cap C_2)\\) - e.g., the probability that our outcome lands in \\(C_1\\) AND \\(C_2\\). \\[ P(C_1 \\cap C_2) = P(\\{HT,HH\\} \\cap \\{TH,HH\\}) = P(\\{HH\\}) = 1/2 \\] With \\(P(C_1)\\) and \\(P(C_1 \\cap C_2)\\), we can now calculate \\(P(C_2|C_1)\\) \\[ \\frac{P(C_1 \\cap C_2)}{P(C_1)} = \\frac{1/4}{1/2} = \\frac{1}{2} \\] The probability of flipping a second head, given you’ve already flipped one, is 1/2. Indeed, we already observed that the \\(P(C_2)\\) (head on second flip) was 1/2, and, interestingly, \\(P(C_2|C_1)\\) was also equal to 1/2. Put another way, the occurrence of a head on the first flip did not change the likelihood of a head on the second flip. In terms of probability, we say these two events are independent. In a more concrete manner, given two events - A and B - with each assigned the probabilities \\(P(A)\\) and \\(P(B)\\), the two events are independent if \\[ \\frac{P(A \\cap B)}{P(B)} = P(A|B) = P(A) \\tag{1.13} \\] The occurrence of B doesn’t change the probability of A; the probability of A occurring, given B occurred, is unchanged. As I promised at the end of the last section, independence and conditional probability give us a new way of calculating P(A B), and it’s hidden in our equations for conditional probability. Specifically \\[ \\begin{align} \\frac{P(A \\cap B)}{P(B)} &amp;= P(A|B) \\\\ P(B)\\frac{P(A \\cap B)}{P(B)} &amp;= P(A|B)\\,P(B) \\\\ P(A \\cap B) &amp;= P(A|B)\\,P(B) \\tag{1.14} \\end{align} \\] The last line above - \\(P(A \\cap B) = P(A|B)\\,P(B)\\) - is known as the law of multiplication. It gives us another way to calculate the \\(P(A \\cap B)\\). For our purposes, this law finds it’s greatest use in the case of events that are independent. Recall from above that in the case of independent events - \\(P(A|B) = P(A)\\). Therefore the law of multiplication becomes \\[ P(A \\cap B) = P(A)\\,P(B) \\tag{1.15} \\] In other words, in the case of two independent events, we can calculate the probability of A and B (\\(P(A \\cap B)\\)), by multiplying \\(P(A)\\) and \\(P(B)\\) together. Let’s give these equations some run: Example 1.20 Suppose we are flipping three coins independently. That is, assume the event we get a heads or tails on a flip is independent from getting a heads/tails on any other flip. Let’s say \\(C_1\\) represents the first flip event, \\(C_2\\) the second, and \\(C_3\\) the third. Here I’m using a shorthand because writing out the sample space would be tiresome. For each coin, assign the probability of 1/2 to the event that the coin flips heads (and 1/2 that it flips tails). What is the probability of \\(C_1 = \\{H\\}\\), \\(C_2 = \\{H\\}\\), and \\(C_3 = \\{H\\}\\)? We can use the law of multiplication for independent events \\[ P(C_1)\\,P(C_2)\\,P(C_3)= 1/2 \\times 1/2 \\times 1/2 = 1/8 \\] We shall get three heads about 1/8th the time. As an interesting aside, suppose you wrote out the sample space and specifically identified the events described by my shorthand for \\(C_1\\), \\(C_2\\), and \\(C_3\\), you’d find that the intersect of the sets is \\(\\{HHH\\}\\). I didn’t show you the exact method to determine the intersect between three sets, but you can simply do it by comparison. You may have also noticed that I used the law of multiplication for three events - not two as I described earlier. However, if a set of events are mutually independent, you may multiply their probabilities together to get the probability of event 1 AND event 2 AND event 3… We won’t discuss the mathematical definition of mutual independence here. I’ll never provide you with a task in the textbook where it isn’t clear. 1.4 Extended Examples of Probability You’ve learned a lot about probability to this point. Great job! Probability can be a tough subject, and when I have taught people the subject, I find that students can get quite confused (albeit it could be the teacher and not the subject). It’s therefore a good idea to do some more detailed and tricky examples. In this section, we’ll add a little bit of flourish to our examples to make probability a little bit more clear. As we go through each example, I recommend attempting them yourself first. Remember: It is okay to be wrong. In fact, being wrong is very important to fully understanding a concept, so don’t get discouraged if you can’t quite answer each question. Example 1.21 Let’s start on the simple side. Suppose the probability that a person likes hot dogs is - \\(P(\\{likes\\:hot\\:dogs\\}) = .25\\) - and the probability a person likes broccoli is - \\(P(\\{likes\\:broccoli\\}) = .15\\). For the sake of my sanity, let’s refer to these as events A and B, respectively. Further assume that if an individual likes broccoli, then the probability they like hot dogs is \\(.10\\). Are the events \\(A\\) and \\(B\\) independent? What is the probability that someone likes hot dogs AND broccoli? Bonus question: What is the sample space of our random experiment? We shall deal with each in turn. First, we are given that \\(P(A|B) = .10\\). You may have missed this because I used less formal language (dang word problems!). This conditional probability does not equal \\(P(A) = .25\\). Therefore, because \\(P(A|B)\\neq P(A)\\), the two are dependent. Put another way, if someone likes broccoli (event B occurred), it lowers the probability that they like hot dogs. The second question refers to the law of multiplication for dependent events. Here it is again: \\[P(A \\cap B) = P(A|B)\\,P(B)\\] We have \\(P(A|B) = .10\\) and \\(P(B) = .15\\). So, plugging these numbers in we get \\[P(A|B)\\,P(B) = .10 \\times .15 = .015\\] The probability of liking hot dogs AND liking broccoli is just \\(.015\\). Out of a thousand random experiments, you might find 15 people who like broccoli and hot dogs. The actual rate is probably much higher, but we aren’t talking about real data just yet. The third part of this question concerns the sample space. I used short hand to describe the sample space, so it’s worth unpacking things for clarity. Let’s represent “liking hot dogs” with the symbol \\(LH\\), and “disliking hot dogs” with the symbol \\(DH\\). Then let’s represent “likes broccoli” with \\(LB\\) and “dislikes broccoli” with \\(DB\\). Like the outcomes of two coin flips, each event in our sample space shall represent a combination of a hot dog preference and a broccoli preference. \\[S = \\{LHLB, LHDB, DHLB, DHDB\\}\\] And our event “liking hot dogs” would be \\[ A = \\{LHLB, LHDB\\} \\] Our event liking broccoli would be \\[ B = \\{LHLB, DHLB\\} \\] The event “liking broccoli” and “liking hot dogs” would be described as \\[ A \\cap B = \\{LHLB\\} \\] And we needed independence and the law of multiplication to calculate the probability of this event - .015. Eat your veggies! Example 1.22 Let’s turn our attention to another classic probability example: dice! Suppose we are rolling two four-sided die, and assign a probability of 1/4 to each number for each die, respectively. Let’s look at some scenarios with increasing complexity. What is the probability of rolling a 1 followed by a 1? What is the probability of rolling a 1 and a 2? What is the probability of the dice summing to 3? What is the probability of getting a sum of 3 - given you’ve rolled a 1 on the first dice? In the first scenario, we’ll again use a short hand to refer to the outcomes of the sample space. \\(A_1\\) will refer to the first roll and \\(A_2\\) will refer to the second roll. We have the event \\(A_1 = \\{1\\}\\) - which can also be written as the combination of all the outcomes where the first dice is 1 - \\(\\{1\\,1,1\\,2,1\\,3,1\\,4\\}\\). Both are equivalent. The second way of writing it is more exact. We also have \\(A_2 = \\{1\\}\\), and each event has \\(P(A_1) = P(A_2) = 1/4\\). They both have a probability of 1/4. We also know that the two events are independent. So, using the law of multiplication: \\[P(A_1 \\cap A_2) = P(A_1)P(A_2) = 1/4 \\times 1/4 = 1/16\\] Indeed, had we picked any two numbers for the first and second dice the probability would be the same - \\(1/4 \\times 1/4 = 1/16\\). This fact will help us with the second question above, because we now know the probability set operator will assign a probability of 1/16 to each outcome in our two roll sample space. Our two roll sample space has \\(4 \\times 4 = 16\\) possible outcomes - so the probability has been divided equally across each outcome. The second question asks us for the probability of rolling a 1 and a 2. In this case, we could get a 1 followed by a 2 OR a 2 followed by a 1. That is to say, we are talking about the probability of the event \\(A = \\{1\\,2, 2\\,1\\}\\), and using the law of addition \\[P(\\{1\\,2, 2\\,1\\}) = 1/16 + 1/16 = 2/16 = 1/8\\] Our work in the last step has set us up nicely to approach the third part of the question. We shall find it useful to fully specify the sample space. The table below will help us with that endeavor. Each larger row represents the first dice roll, and embedded within each larger row are all the possible rolls of the second dice. For example, let your eyes focus on the first big row - the row corresponding to a 1 on the first die. Next scan to the right and see the row corresponding to a 1 on the second dice. The probability of getting a 1 followed by a 1 (summing to 2) is 1/16. .cl-e0892bbe{}.cl-e069c1d4{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-e07e1b8e{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-e07e4f6e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e07e4f78{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e07e4f82{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e07e4f83{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e07e4f8c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}1st Die2nd DieSumProbability1121/16231/16341/16451/162131/16241/16351/16461/163141/16251/16361/16471/164151/16261/16371/16481/16 Thanks to this table we can find the combinations that lead to a sum of three. Specifically we have \\(S = \\{1\\,2, 2\\,1\\}\\), and each of these outcomes is 1/16, so the probability of getting an outcome that sums to three is 1/8. Now it’s time for the final boss! How shall we determine the probability of the dice summing to 3 given we roll a 1 on the first die? How shall we determine the independence of these two events? Well, let’s look at the equation for conditional probability again \\[ \\frac{P(A \\cap B)}{P(B)} = P(A|B) \\] We shall need the probability of getting a sum of 3 AND rolling a 1 on the first die, as well as, the probability of rolling a 1 on the first die. Furthermore, for the determination of independence we will need the probability of rolling two dice that sum to 3. Let’s calculate each of these in turn. Let’s denote the event of rolling a 1 on the first dice as \\(A = \\{1\\,1, 1\\,2, 1\\,3, 1\\,4\\}\\). Way back in the beginning of this example we said that P(\\(A\\)) = 1/4. Let’s deal with the probability of our two dice summing to 3 first. Denote this event as \\(B = \\{1\\,2,2\\,1\\}\\). We just calculated this probability as 1/8. We can calculate \\(P(A \\cap B)\\) in a couple of ways. First we can lean on our intuition a bit. Looking at the table above, we see that there is only one way to roll a 1 on the first die AND produce a sum of three - \\(\\{1\\,2\\}\\). The probability of this event is 1/16. If we are having difficulty leaning on our intuition, we have learned the tools to calculate \\(P\\{A \\cap B\\}\\). \\[ \\begin{align} P\\{A \\cap B\\} &amp;= P(\\{1\\,1, 1\\,2, 1\\,3, 1\\,4\\} \\cap \\{1\\,2,2\\,1\\}) \\\\ &amp;= P(\\{1\\,2\\}) \\\\ &amp;= 1/16 \\end{align} \\] Now we can calculate the probability of B given A: \\[ \\frac{P(A \\cap B)}{P(B)} = \\frac{1/16}{1/8}= \\frac{1}{4} \\] And now we are prepared to evaluate independence. The probability of our dice summing to three is \\(P(B) = 1/8\\), and \\(P(A|B) = 1/4\\). As such, \\(P(B) \\neq P(A|B)\\). The two events are dependent. Rolling a 1 increases our overall chances of getting two dice that sum to 3. Indeed, reflecting on this a bit more, had we rolled a 4 on the first die it would be impossible to have a sum of 3 (e.g., \\(P(B)\\) = 0). It therefore makes sense that our two events are dependent. Example 1.23 One more example is worth considering here. Suppose we know the probability of liking the San Francisco Giants is .10. Denote this event as \\(A\\). Further suppose that we know the probability of liking meatballs is .40. Denote this event as \\(B\\). What is the probability of A and B - \\((P(A \\cap B)\\)? The answer to this question is that it is unanswerable. We need more information about their independence to go any further. If the two events were independent, we could simply multiply their probabilities together. If they were not independent, we would need \\(P(A|B)\\) or \\(P(B|A)\\) before applying the law of multiplication. "],["discrete-random-variables.html", "Chapter 2 Discrete Random Variables 2.1 What is a discrete random variable? 2.2 Independence of discrete random variables 2.3 What do we want to know about discrete random variables? 2.4 Some common discrete distributions", " Chapter 2 Discrete Random Variables As behavioral researchers, we are often studying a population of something. We might be interested in the percentage of individuals who like cats or the number of cats owned by the average individual. We might be interested in the average happiness level of a population of individuals and/or what makes them happy. The populations we study are often vaguely defined, e.g., as in the population of people who have received some experimental treatment. Here, we will begin to take these somewhat vague notions and make them concrete. It turns out that, when we are studying a population, we are really studying what statisticians call a random variable. In this chapter, we will pay particular attention to the discrete random variable. 2.1 What is a discrete random variable? When we are in the process of conducting quantitative research (and statistics), we measure things in numbers. You may have noticed that our previous discussions of discrete sample spaces used non-numeric descriptions - e.g., \\(S = \\{H, T\\}\\) in the case of a coin flip. To make these sample spaces easier to work with, we will map our discrete outcomes to numbers. For example, if we were interested in a coin flip, we wouldn’t work directly with the values of \\(H\\) and \\(T\\). We would map each outcome to a value, e.g., heads to the value of 1 and tails to the value of 0. On any given coin flip, the value of our outcome would therefore be a number. A variable like this - one that assumes a number as a result of some random process (e.g., a flip, a random experiment, drawing out of a hat, rolling, randomly sampling, etc.) - is called a discrete random variable. In this section, we’ll provide some key definitions and notation for discrete random variables. This will make things more concrete and clear for you and make the interesting stuff more interesting. The set of possible numbers a random variable can take is called the space of the random variable, and like the discrete sample space, these outcomes shall be either finite or countable. In general we refer to a random variable with a capital letter, typically \\(X\\), and a potential outcome of this random variable is referred to symbolically by a lower case letter, e.g., \\(x\\). For convenience, events in the space of a random variable are often referenced with a simplified set notation. Suppose we are describing the set of outcomes where our random variable \\(X\\) attains a particular value. We will write \\(X = x\\). For example, \\(X = 1\\) describes the outcomes in the space of \\(X\\) where our random variable attains a value of 1 - \\(\\{1\\}\\). We may also use an inequality - e.g., \\(X \\leq x\\) - to reference a range of outcomes. Because \\(X\\) attains a value from a random process, we’ll need a concrete way of describing it’s randomness. Recall in the last chapter, we discussed the probability set operator - \\(P()\\). This operator assigns the probability to an outcome - \\(x\\) - on the basis of a set of rules. The set of rules for the assignment of probability is called a probability distribution, and in the case of a discrete random variable this distribution is called a probability mass function (pmf) - \\(p(x)\\). In practice, the set of rules (a.k.a, a probability mass function) is frequently described like this: \\[ P(X=x) = p(x) = \\begin{cases} \\frac{1}{2} &amp; x = 1 \\\\ \\frac{1}{2} &amp; x = 0 \\end{cases} \\] Note that \\(P(X = x)\\) and \\(p(x)\\) mean the same thing - hence the equal sign. After the large curly-brace, you will see a row for each outcome in the space of \\(X\\). In that row you will find the probability for that outcome. For example, \\(P(X = 1) = p(1) = 1/2\\). The pmf of \\(X\\) has two important properties. Specifically: All the outcomes \\(x\\) in the space of \\(X\\) have a probability \\(0 \\leq p(x) \\leq 1\\). That is, all the outcomes in the space of \\(X\\) must have a probability from 0 to 1. The set of outcomes for which \\(p(x) &gt; 0\\) is called the support of \\(X\\). Given the space \\(S\\) of a random variable, \\(P(S)\\) = 1. In other words, if we are conducting a random experiment by drawing a value from the space of \\(X\\) (the set of all numeric outcomes for \\(X\\)), the probability that it lands in \\(S\\) is 1. This hearkens back to (1.10) in the previous section. In plain language these two properties mean that we can define the pmf of a random variable by taking a total of one unit of probability and spreading it across a set of outcomes (the support of \\(X\\)). For example, you might give .25 probability to a value of 1, .5 to a value of 2, and .25 to a value of 3. In this case, each outcome would have a probability from 0 to 1, and these values would sum to 1. In this chapter we will dig into discrete random variables in three steps: We shall start by discussing the independence of random variables - with the goal of framing an intuition for the concept. We will then turn our attention to things researchers might want to know about discrete random variables (and, as you will learn, random variables more generally). And finally, we will discuss some random variables that are frequently found in behavioral research. 2.2 Independence of discrete random variables The concept of independence is found frequently within the field of statistics. As you read on, you will find that it is essential to the derivation, understanding, and validity of some of the most fundamental things we shall learn in statistics. Therefore, it is nice to have a deeper understanding of it. The goal of this section is to frame an intuition of the independence of random variables. A deeper mathematical understanding shall be saved for your inevitable foray into mathematical statistics (no doubt after being inspired by this book to master the topic). In the last chapter, we discussed the concept of the independence of events in a random experiment. We shall now discuss independence as it pertains to random variables. And, as you will see, it is very similar to the concept of independence for events. Suppose we have two random variables \\(X_1\\) and \\(X_2\\). Then we say the two variables are independent if \\[ P(X_1 = x_1 \\cap X_2 = x_2) = P(X_1 = x_1)P(X_2 = x_2) \\tag{2.1} \\] In other words, if two random variables are independent, the probability of \\(X_1 = x_1\\) AND \\(X_2 = x_2\\) is equal to the product of their respective probabilities. We’ve already seen examples of this. Recall in example 1.20, we flipped three coins independently. If we mapped the outcomes of each coin to 1 and 0 for heads and tails, respectively, then the three resulting random variables are independent in the sense of (2.1). With some algebraic manipulation of (2.1), we can see that the definition of independence for random variables is very similar to the definition of independence for events \\[ \\frac{P(X_1 = x_1 \\cap X_2 = x_2)}{P(X_2 = x_2)}=P(X_1 = x_1) \\tag{2.2} \\] And, perhaps not surprisingly (although it’s okay if you are surprised), if the two random variables are not independent then \\[ \\frac{P(X_1 = x_1 \\cap X_2 = x_2)}{P(X_2 = x_2)} = P(X_1 = x_1|X_2 = x_2) \\neq P(X_1 = x_1) \\] Put into the logic and hopefully more understandable intuition of last chapter, when two random variables are dependent, an event in the space of one variable changes the probability of an event in the space of another variable. This may seem a bit confusing at first, but you have already seen an example of the dependence of random variables. Recall example 1.22, when we looked at the sum of two dice. In this case, let’s call the sum of our two dice the random variable \\(Z\\) and represent the random variable of the first die by \\(X\\). On the basis of (2.1), we shall be looking to determine if \\[ \\frac{P(Z = z \\cap X = x)}{P(X = x)} \\neq P(Z = z) \\] The first thing we will need is \\(P(Z = z)\\). The space of \\(Z\\) are all the possible sums of our two dice - \\(\\{2,3,4,5,6,7,8\\}\\). Using the table we created in example 1.22, and the law of addition, we can determine a probability distribution for \\(Z\\) (try it yourself and see if you get the same thing as below) \\[ P(Z=z) = p(z) = \\begin{cases} 1/16 &amp; z = 2 \\\\ 2/16 &amp; z = 3 \\\\ 3/16 &amp; z = 4 \\\\ 4/16 &amp; z = 5 \\\\ 3/16 &amp; z = 6 \\\\ 2/16 &amp; z = 7 \\\\ 1/16 &amp; z = 8 \\end{cases} \\] After our computation, we can see that the most probable value is 5. Because we haven’t had much experience with random variables, it is also important to point out that this distribution has all our previously discussed properties of a pmf. The probabilities of all the outcomes in the space of \\(Z\\) sum to 1, and they are constrained between 0 and 1. Now that we have determined the probability mass function for \\(Z\\), we shall calculate \\((P(Z = z|X = x))\\). Hopefully the idea of independence/dependence of random variables will come into focus. Consider the case where \\(X = 1\\) (a 1 was rolled on the first die). What does the distribution of \\(P(Z = z|X = 1)\\) look like? To determine this we shall calculate the conditional probability of each Z - given our first dice roll is a 1. We did this back in example 1.22 for the specific value of \\(Z = 3\\), and now we shall do it here for every value (try it yourself) \\[ P(Z=z|X=1) = p(z|x=1) = \\begin{cases} 1/4 &amp; z = 2 \\\\ 1/4 &amp; z = 3 \\\\ 1/4 &amp; z = 4 \\\\ 1/4 &amp; z = 5 \\\\ 0 &amp; z = 6 \\\\ 0 &amp; z = 7 \\\\ 0 &amp; z = 8 \\end{cases} \\] Do you see what happens with the event \\(X = 1\\)? The probability of \\(Z\\) changes with a roll of a 1 on the first die. For example, getting a \\(Z = 2\\) now has a whopping 1/4 chance, compared to a paltry 1/16 before the first dice was rolled. Moreover, the probability is now spread across only four values - \\(\\{2,3,4,5\\}\\). That is, the support of \\(Z\\) - given \\(X = 1\\) - is now 2, 3, 4, and 5. Thus, an intuitive definition of independence pivots on changes in the pmf of one random variable given the outcome of another variable. This idea of independence shall be sufficient for our purposes, especially as we become more sophisticated in statistics and statistical analysis. As you will see, some of the most fundamental statistical concepts require an assumption of independence. In many cases, we shall assume that our random variables are independent to take advantage of some convenient mathematical properties associated with independence. We’ll see one of these convenient properties as soon as this chapter. 2.3 What do we want to know about discrete random variables? A new statistics student will have made it to this point and reasonably wondered: what the heck is the point of all this back story? This skepticism is understandable, welcomed, and hopefully motivating. In this section, we shall discuss some things - as statisticians and researchers - that we might want to know about random variables. In doing so, hopefully the bigger picture comes into view. This, in my humble opinion, is where things really start to get interesting. I’m hoping you feel the same. But first, let’s do a very very short review. 2.3.1 A very quick review: The summation operator As statisticians we will need to sum a lot of things. It will therefore be useful to have an easy way to describe the operation of summation. To that end, suppose we are summing a set of numbers \\[ x_1 + x_2 + x_3 + x_4 + x_5 \\,+\\:... + x_n \\] The summation operator helps us write this operation more efficiently and with less ambiguity: \\[ \\sum_{i=1}^{n} x_i \\tag{2.3} \\] The \\(i\\) at the bottom of the \\(\\sum{}^{}\\) is referred to as the index. The index is how we refer to a single element in our set of \\(x\\)’s. For example, \\(i=5\\) refers to \\(x_5\\). The starting index is specified at the bottom of the summation operator to the right of the equal sign. In the equation above we are starting at the 1st element of our set of \\(x\\)’s. The value at the top of the summation operator is the index of the last element to be summed. Here we have written \\(n\\) which is the last element of our set of n \\(x\\)’s. In other words \\[ \\sum_{i=1}^{n} x_i = x_1 + x_2 + x_3 + x_4 + x_5 + ... + x_n \\] in terms of our example above. Additionally, it is important to note that n and i are not always used to indicate the last element and the index, respectively. Here we shall try to be consistent. Occasionally, to save a little time we shall write the summation operator like \\[ \\sum_{x}^{} \\] This means - given all the \\(x&#39;s\\) in a set - sum all of them. Alright, now it’s time for the interesting stuff. 2.3.2 The expectation of a discrete random variable The first thing that we shall want to know about a discrete random variable is its expectation. Given a random variable \\(X\\), we refer to its expectation as \\(E(X)\\). We shall also sometimes refer to it symbolically as \\(\\mu\\). In other words \\[ E(X) = \\mu \\] The expectation is commonly referred to as the mean of a random variable. The expectation of a discrete random variable \\(X\\) with a given pmf - \\(p(x)\\) is calculated \\[ E(X) = \\sum_{i = 1}^{n} x_ip(x_i) \\tag{2.4} \\] In words: For each \\(x\\) in the space of \\(X\\), multiply by its corresponding probability and sum the resulting values. An example, will help us understand the expectation a bit better. Example 2.1 Suppose we have a random variable \\(X\\) with five outcomes in it’s space - \\(\\{1,2,3,4,5\\}\\). Further assume that the pmf of \\(X\\) assigns \\(\\frac{1}{5}\\) probability to each outcome. \\[ P(X=x) = p(x) = \\begin{cases} 1/5 &amp; x_1 = 1 \\\\ 1/5 &amp; x_2 = 2 \\\\ 1/5 &amp; x_3 = 3 \\\\ 1/5 &amp; x_4 = 4 \\\\ 1/5 &amp; x_5 = 5 \\end{cases} \\] This is an example of a discrete uniform distribution. Given n outcomes, the discrete uniform distribution assigns \\(1/n\\) probability to each outcome on the space of the random variable. Now let’s calculate the expectation of this random variable. \\[ \\begin{align} E(X) = \\sum_{i=1}^{5} x_ip(x_i) &amp;= x_1p(x_1)+x_2p(x_2)+x_3p(x_3)+x_4p(x_4)+5p(x_5) \\\\ &amp;= 1p(1)+2p(2)+3p(3)+4p(4)+5p(5) \\\\ &amp;= (1)\\frac{1}{5}+(2)\\frac{1}{5}+(3)\\frac{1}{5}+(4)\\frac{1}{5}+(5)\\frac{1}{5} \\\\ &amp;= \\frac{15}{5} = 3 \\end{align} \\] We’ve multiplied each value in the space of our random variable \\(X\\) by its probability and summed the values. The result of our computation is 3 - \\(E(x) = 3\\) - but what does that mean exactly? Well let’s explore the idea of expectation further by plotting the pmf of \\(X\\). In the figure below, the probability of each outcome is plotted. You can see the outcomes of the random variable labelled on the \\(x\\) axis, and probability of each is represented by the bar height. In this case, each outcome is equally likely, so the bars are of equal height. Now suppose each of those bars has mass (a.k.a. probability mass), and further suppose we’ll place our wedge under the \\(x\\) axis such that the mass of the plot balances perfectly. Just by looking at the plot, if we put a wedge directly under the value of 3, there would be equivalent probability mass on either side, and the plot would balance perfectly. Notice that the value of 3 was the result of our expectation computation. \\(E(X)\\) can therefore be thought as the balancing point of the plot - the place that would perfectly balance the mass of probability for our random variable. That said, we shall need to learn a little bit more about the expectation to proceed on our statistical adventure. First let’s observe that it is possible to take the expectation of some function of a random variable. In other words, we may apply some function to the outcomes in the space of \\(X\\) and then calculate an expectation. For example, we might have \\[ f(X) = 2X \\] This function takes the outcomes in the space of \\(X\\) and multiplies each by 2. We shall calculate the expectation of this function of our random variable by \\[ E(f(X)) = \\sum_{i = 1}^{n} f(x_i)p(x_i) \\tag{2.5} \\] Let’s try it out in a couple examples. Example 2.2 We will again use the distribution from example 2.1. Here we will keep it simple (and also demonstrate a property of multiplication you may recall from algebra). Suppose our function \\(f(x)\\) takes every value in the space of \\(X\\) and returns the value one. In other words, \\[ f(X) = 1 \\] Let’s apply (2.5) to find the expectation of this function. First, let’s remind ourselves of the distribution of \\(X\\) and calculate \\(f(x)\\) for each outcome in it’s space \\[ p(x) = \\begin{cases} 1/5 &amp; x_1 = 1 &amp; f(1) = 1 \\\\ 1/5 &amp; x_2 = 2 &amp; f(2) = 1 \\\\ 1/5 &amp; x_3 = 3 &amp; f(3) = 1 \\\\ 1/5 &amp; x_4 = 4 &amp; f(4) = 1 \\\\ 1/5 &amp; x_5 = 5 &amp; f(5) = 1 \\end{cases} \\] Now let’s calculate the expectation of \\(f(X)\\). Watch the underlined values for a little bit of algebra review. \\[ \\begin{align} E(X) = \\sum_{i=1}^{5} f(x_i)p(x_i) &amp;= \\underline{1}\\:p(x_1)+\\underline{1}\\:p(x_2)+\\underline{1}\\:p(x_3)+\\underline{1}\\:p(x_4)+\\underline{1}\\:p(x_5) \\\\ &amp;= \\underline{1}\\:[p(x_1) + p(x_2) + p(x_3) + p(x_4) + p(x_5)] \\\\ &amp;= \\underline{1}\\:[\\frac{1}{5} + \\frac{1}{5} + \\frac{1}{5} + \\frac{1}{5} + \\frac{1}{5}] \\\\ &amp;= \\underline{1}(1) = 1 \\end{align} \\] The second line of this calculation was made possible by the distributive property of multiplication. That made the computation easier because we note that the sum of probabilities over the space of \\(X\\) is equal to 1 (see the third line; and see the properties of a discrete random variable at the start of this chapter). Example 2.3 Suppose our random variable \\(X\\) is again defined as in example 2.1. Now let’s use the function \\(f(x)= 2X\\). Then \\[ p(x) = \\begin{cases} 1/5 &amp; x_1 = 1 &amp; f(1) = 2(1) = 2 \\\\ 1/5 &amp; x_2 = 2 &amp; f(2) = 2(2) = 4 \\\\ 1/5 &amp; x_3 = 3 &amp; f(3) = 2(3) = 6 \\\\ 1/5 &amp; x_4 = 4 &amp; f(4) = 2(4) = 8 \\\\ 1/5 &amp; x_5 = 5 &amp; f(5) = 2(5) = 10 \\end{cases} \\] \\[ \\begin{align} E(2X) = \\sum_{i=1}^{5} f(x_i)p(x_i) &amp;= 2p(x_1)+4p(x_2)+6p(x_3)+8p(x_4)+10p(x_5) \\\\ &amp;= (2)\\frac{1}{5} + (4)\\frac{1}{5} + (6)\\frac{1}{5} + (8)\\frac{1}{5} + (10)\\frac{1}{5} \\\\ &amp;= \\frac{30}{5} = 6 \\end{align} \\] The astute observer might notice an interesting trend in the last two examples. In example 2.2, we saw that our function which converts the whole space of \\(X\\) into 1 had the expectation of 1. Indeed, looking back at the computation, we could perform that computation for any constant, e.g., if \\(a\\) is a constant, and \\(f(X) = a\\), then \\[ \\begin{align} E(a) = \\sum_{i=1}^{5} ap(x_i) &amp;= \\underline{a}\\:p(x_1)+\\underline{a}\\:p(x_2)+\\underline{a}\\:p(x_3)+\\underline{a}\\:p(x_4)+\\underline{a}\\:p(x_5) \\\\ &amp;= \\underline{a}\\:[p(x_1) + p(x_2) + p(x_3) + p(x_4) + p(x_5)] \\\\ &amp;= \\underline{a}\\:[\\frac{1}{5} + \\frac{1}{5} + \\frac{1}{5} + \\frac{1}{5} + \\frac{1}{5}] \\\\ &amp;= \\underline{a}(1) = a \\end{align} \\] In other words, we would get the value of our constant every time. Similarly, in 2.3, recall that the original expectation of \\(X\\) was 3. Our function \\(f(X) = 2X\\) had the expectation of \\(2\\times3 = 6\\). That is, we multiplied \\(X\\) by 2 and got back 2 times its expectation. Our last two observations highlight a couple of properties of the expectation. We’ll list a few below. The first two (or three) you might have deduced yourself. The fourth is new. Theorem 2.1 (Some Properties of the Expectation) If \\(a\\) and \\(b\\) are constants, then \\(E(b) = b\\) \\(E(aX) = aE(X)\\) More generally, \\(E(aX + b) = aE(X) + E(b)\\) If \\(X_1\\) and \\(X_2\\) are random variables, then \\(E(aX_1 + bX_2)\\) = \\(aE(X_1) + bE(X_2)\\) These are called properties because they apply to any random variable - assuming that random variable has an expectation (a more advanced topic that we won’t discuss in much depth in this book). They allow us to perform computations with expectations without knowing much about the actual distribution of the random variable. And, they make computations easier. As you will see later, they will help us draw certain conclusions about the populations we study. For now, let’s see how they work in practice and look at a more thorough example for the fourth. Example 2.4 Suppose we are given a random variable - \\(Z\\) - and \\(E(Z) = 7\\). What is the expectation of \\(4Z\\) \\(Z + 10\\) \\(1\\) \\(4Z+10\\) We will approach each of these in turn. Property 2 tells us that \\(E(aX) = aE(X)\\), and in this case \\(a = 4\\) \\[ E(4Z) = 4E(Z) = 4(7) = 28 \\] Note that property 3 also applies here with \\(b = 0\\). Property 3 tells us that \\(E(aX + b) = aE(X) + b\\), so in this case we have \\(a = 1\\) and \\(b = 10\\) \\[ E(Z + 10) = (1)E(Z) + 10 = 7 + 10 = 17 \\] The first property tells us that \\(E(c) = c\\), so \\[ E(1) = 1 \\] Note that property 3 also applies here with \\(a = 0\\) and \\(b = 1\\). Similar to number 2, property 3 applies with \\(a = 4\\) and \\(b = 10\\) \\[ E(4Z + 10) = E(4Z) + 10 = 4E(Z) + 10 = 28 + 10 = 38 \\] Do you see how that made things a bit easier? We didn’t have to go through the computation of the expectation of our function of \\(Z\\). I bet you can tell by now I like to avoid long computations! Now let’s turn our attention towards property 4. Example 2.5 Suppose we flip two coins - where a flip of \\(H\\) yields a value of 1 and a flip of \\(T\\) yields a value of zero. Call these two random variables \\(X_1\\) and \\(X_2\\), respectively, and further assume that they are independent (recall what that means?). What is the expectation of their sum? To demonstrate why property four is so useful, we’ll do this the hard way and then the easy way. First note that the pmf of \\(X_1\\) and \\(X_2\\) is \\[ p(x_1) = \\begin{cases} 1/2 &amp; x_1 = 1 \\\\ 1/2 &amp; x_1 = 0 \\\\ \\end{cases} \\\\ p(x_2) = \\begin{cases} 1/2 &amp; x_2 = 1 \\\\ 1/2 &amp; x_2 = 0 \\\\ \\end{cases} \\] Without the use of property 4, these pmfs won’t suffice to calculate the expectation of \\(X_1 + X_2\\). We’ll have to figure out the pmf of the sum of the two random variables, a.k.a., \\(Z = X_1 + X_2\\). Note that there are four possible outcomes in the sample space of our two coin flips = \\({HH, HT, TH, TT}\\). One outcome in this sample space that would lead to a sum of two - \\({HH}\\). In this case, both values of \\(X_1\\) and \\(X_2\\) would be 1 so \\[ X_1 + X_2 = 1 + 1 = 2 \\] Because our random variables are independent, we can find the probability of this outcome using the law of multiplication for independent events \\[ P(X_1 = 1 \\cap X_2 = 1) = P(X_1 = 1)P(X_2 = 1) = (1/2)(1/2) = 1/4 \\] We can do this calculation for each of the other outcomes in the sample space of our coin flips. The table below breaks down each. It is most easily read from left to right. Find the value of the first flip in the first column. For example, the big top row has a first flip of 1 (Heads). Next scan to the right to see outcome of the second flip, which can either be a 1 or a 0. Scan further to the right to see the sum and probability of the outcome. For example, if we flipped a 1 on the first coin, and then a zero on the second coin, the sum of the two would be 1, and the probability would be 1/4. .cl-e0c7f556{}.cl-e0bedc46{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-e0c29354{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-e0c2b988{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e0c2b992{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e0c2b993{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e0c2b994{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e0c2b99c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}1st Flip2nd FlipSum (Z)Probability1121/4011/40111/4001/4 Looking over this table, we see that our new random variable \\(Z\\) has the space of \\({0,1,2}\\). There is only 1 way to get a 0 or 2, respectively, and each of those outcomes has a probability of \\(1/4\\). There are two ways to get a 1, so by the law of addition, this outcome in the space of \\(Z\\) has a probability of \\(1/4 + 1/4 = 1/2\\). In other words \\[ p(z) = \\begin{cases} 1/4 &amp; z = 0 \\\\ 1/2 &amp; z = 1 \\\\ 1/4 &amp; z = 2 \\end{cases} \\] Now, after that slog (you can imagine it being much worse), are in a position to calculate the expectation of \\(Z = X_1 + X_2\\) \\[ \\begin{align} E(a) = \\sum_{z}^{} zp(z) &amp;= 0\\:p(x_1)+1\\:p(x_2)+2\\:p(x_3) \\\\ &amp;= 0\\frac{1}{4} + 1\\frac{1}{2} + 2\\frac{1}{4} = \\frac{2}{4} + \\frac{2}{4} = 1 \\\\ \\end{align} \\] The usefulness of the fourth property now comes into view. Observe that \\(E(X_1) = E(X_2) = 1/2\\) (calculate this on your own; you can do it!). So, by property 4 \\[ E(X_1 + X_2) = E(X_1) + E(X_2) = 1/2 + 1/2 = 1 \\] Easier! We shall end this section with a word of caution. Some beginning statistics students may understandably read property 4 and think it applies to other arithmetic operations. For example they may think that \\(E(XY) = E(X)E(Y)\\). This is true in some circumstances but it is not true in general. The properties of the expectation apply specifically to the circumstances outlined by each property. You may get into trouble to go beyond that. 2.3.3 The variance of a discrete random variable We have just dedicated a bit of time to the expectation of a random variable. Another important thing we will want to know as statisticians is the variance of a random variable. The variance of a random variable \\(X\\) is referred to as \\(Var(X)\\) or symbolically as \\(\\sigma^2\\). The square root of the variance - \\(\\sigma\\) - is called the standard deviation. In terms of discrete random variables, the variance is calculated \\[ Var(X) = E((X-\\mu)^2) = \\sum_{x}^{} (x - \\mu)^2p(x) \\tag{2.6} \\] It can be thought of as a measure of how “spread” out the probability mass function is for a given random variable (a.k.a. the dispersion). That is, it will tend to be higher for variables that have probability mass spread over larger ranges of values. In the case of the variance calculation described here, we are measuring dispersion by how far values are spread out from the expectation or mean of the random variable. There are indeed many ways you can measure the dispersion of a random variable. New students, for example, often wonder why we don’t just calculate \\(E(X - \\mu)\\) instead of squaring the difference as in (2.6). Example 2.6 Let’s do a quick calculation to convince ourselves why this isn’t a good idea. Such a calculation will have the added bonus of helping us review properties of the expectation. First notice that \\(\\mu\\) is a constant, so \\[ E(X-\\mu) = E(X) - E(\\mu) = E(X) - \\mu \\] Now observe that \\(E(X) = \\mu\\), so \\[ E(X) - \\mu = \\mu - \\mu = 0 \\] \\(E(X - \\mu)\\) is zero for every random variable (that has an expectation) which is not a very good measure of dispersion! Example 2.7 With that minor point out of the way, let’s give the variance equation some run. Suppose our random variable \\(X\\) has the pmf \\[ p(x) = \\begin{cases} 1/8 &amp; x_1 = 0 \\\\ 3/4 &amp; x_2 = 1 \\\\ 1/8 &amp; x_3 = 2 \\end{cases} \\] Reviewing the equation for the variance, it is apparent we shall first need to calculate \\(\\mu\\) for \\(X\\) before we can do any calculation of the variance. Knowing what we know about the expected value and the balancing point of probability mass, I think we would be pretty safe to guess that the expectation of \\(X\\) is 1. Nevertheless, let’s calculate it really quick \\[ \\begin{align} E(X) = \\sum_{i=1}^{3} x_ip(x_i) &amp;= 0\\frac{1}{8}+ 1\\frac{3}{4} + 2\\frac{1}{8} \\\\ &amp;= \\frac{6}{8} + \\frac{2}{8} = 1 \\end{align} \\] Our intuition was indeed correct - \\(E(X) = 1\\). We shall now turn our attention to calculating the variance of \\(X\\) \\[ \\begin{align} Var(X) = \\sum_{i=1}^{3} (x_1-\\mu)^2p(x_i) &amp;= (x_1-\\mu)^2p(x_1) + (x_2-\\mu)^2p(x_2) + (x_3-\\mu)^2p(x_3) \\\\ &amp;= (0 - 1)^2\\frac{1}{8} + (1-1)^2\\frac{1}{8} + (2-1)^2\\frac{1}{8} \\\\ &amp;= (-1)^2\\frac{1}{8} + 0 + 1^2\\frac{1}{8} = \\frac{1}{4} \\end{align} \\] Thus, \\(Var(X) = 1/4\\). We should stop here and take a breath. In the last two sections, you’ve calculated your very first mean and variance of a random variable. Congratulations! These concepts may seem a little disconnected from our work as behavioral scientists, but they will make some wonderful things clear later on. I promise. Now let’s get back to work. Just like the expectation, we can also calculate the variance of some function of \\(X\\). First calculate \\(E(f(X))\\), referred to here as \\(\\mu_f\\), then \\[ Var(X) = E((f(X)-\\mu_f)^2) = \\sum_{x}^{} (f(x) - \\mu_f)^2p(x) \\tag{2.7} \\] To demonstrate how this equation works we’ll use the same pmf as example 2.7. Suppose we want to find the variance of \\(f(X) = 2X\\). As with calculation of the expecation in the last section, let’s first multiply all the outcomes in the space of \\(X\\) by 2 \\[ p(x) = \\begin{cases} 1/8 &amp; x_1 = 0 &amp; f(x_1) = (2)0 = 0 \\\\ 3/4 &amp; x_2 = 1 &amp; f(x_2) = (2)1 = 2\\\\ 1/8 &amp; x_3 = 2 &amp; f(x_3) = (2)2 = 4 \\end{cases} \\] Next, in order to calculate \\(Var(2X)\\), we will first need \\(E(2X)\\). I’ll just go ahead and give you the fact that \\(E(2X) = 2\\), but you should calculate this for yourself as well (you have all the tools, skill, and excitement to do it!). Next we’ll apply our new equation \\[ \\begin{align} Var(2X) = \\sum_{i=1}^{3} (f(x_i)-\\mu_f)^2p(x_i) &amp;= (f(x_1)-\\mu_f)^2p(x_1) + (f(x_2)-\\mu_f)^2p(x_2) + (f(x_3)-\\mu_f)^2p(x_3) \\\\ &amp;= (0 - 2)^2\\frac{1}{8} + (2-2)^2\\frac{1}{8} + (4-2)^2\\frac{1}{8} \\\\ &amp;= (-2)^2\\frac{1}{8} + 0 + 2^2\\frac{1}{8} = 1 \\end{align} \\] Observe that the variance of our random variable \\(2X\\) is larger than \\(X\\). To understand why, let’s take a look at the probability mass functions of \\(X\\) and \\(2X\\). The top plot below is the pmf of \\(X\\), with the vertical line representing \\(E(X)\\). The height of each bar represents the probability mass assigned to a particular value on the \\(x\\)-axis - in the space of \\(X\\). For example, \\(1/8\\) mass sits on top of the value 0. The bottom plot is the pmf of \\(2X\\) - as if we created a new random variable as \\(x\\) but with each of the outcomes multiplied by 2. That is, for example, instead of \\(1/8\\) probability sitting on top of 2, it now sits on top of 4. In this case, the vertical line represents \\(E(2X)\\). Note how the mass for \\(2X\\) now sits on the values of 0, 2, and 4 (the outcomes after we have applied \\(f(x)\\)). And, it’s pmf is spread out further around its expectation. It is dispersed more widely than the probability mass of \\(X\\). Indeed, this is an intuitive (although maybe not perfectly exact) way of thinking about variance: A random variable with a larger variance will tend to have it’s probability mass spread out more. Therefore, it is not surprising that the \\(Var(X) &lt; Var(2X)\\). In fact, this is precisely the sort of behavior we would want out of a measure of variance! We shall observe one more thing about our last two examples. In the first example we found \\(Var(X) = 1/4\\) and in the second example, we found \\(Var(2X) = 1\\). Indeed, you may not have noticed this quickly (I didn’t when I first learned this stuff), but \\[ \\begin{align} Var(2X) &amp;= 1 \\\\ &amp;= 4\\frac{1}{4} \\\\ &amp;= 2^2\\frac{1}{4} \\\\ &amp;= 2^2Var(X) \\end{align} \\] Because \\(Var(X) = 1/4\\). I might have lost you there. In words, it looks like the variance of \\(2X\\) is simply \\(2^2\\) times the variance of \\(X\\). If we did this a few more times, you might begin to think that, given some value \\(c\\), the variance of \\(cX\\) is just \\(c^2Var(X)\\). Indeed this is true in general (for random variables that have a variance). Like the expectation, the variance also has some useful properties. They are listed below: Theorem 2.1 (Some Properties of the Variance) If \\(a\\) and \\(b\\) are constants, then \\(Var(b) = 0\\) \\(Var(aX) = a^2Var(X)\\) More generally, \\(Var(aX + b) = a^2Var(X) + Var(b) = a^2Var(X) + 0\\) If \\(X_1\\) and \\(X_2\\) are random variables, and they are independent, then \\(Var(aX_1 + bX_2)\\) = \\(a^2Var(X_1) + b^2Var(X_2)\\) Let’s do some examples to get a better grasp on these. Example 2.8 Suppose we have a random variable \\(X\\) and we are seeking to determine the the variance of \\(f(X) = b\\) - e.g., \\(Var(b)\\). This function takes any value in the space of \\(X\\) and returns a constant \\(b\\). First take a moment to recall that \\(E(f(X)) = E(b) = b\\) (Property 1 of the expectation). So then our variance equation boils down to: \\[ Var(X) = E((f(X)-\\mu_f)^2) = \\sum_{x}^{} (b - b)^2p(x) = 0 \\] Indeed, we could have just used property 1 of the variance and concluded the same thing - \\(Var(b) = 0\\). As an aside: within the field of statistics, random variables with zero variance are called degenerate. These variables return a constant every time because all their probability sits on exactly one value. The next example will elucidate a bit of property 3. Specifically, we unpack why adding a constant value to a random variable doesn’t change it’s variance. That is to say, if we have \\(a = 1\\) and \\(b=10\\), then \\[ Var(aX+b) = (1)Var(X) + 0 = Var(X) \\] Example 2.9 Suppose we have a random variable \\(X\\) with \\(E(X) = 1/2\\), \\(Var(X) = 1/4\\), and pmf \\[ p(x) = \\begin{cases} 1/2 &amp; x = 0 \\\\ 1/2 &amp; x = 1 \\end{cases} \\] Take a moment to calculate the expectation and variance of this random variable yourself. Now let’s say we would like to calculate \\(Var(X + 5)\\). Looking at property 3 of the expectation, we have \\(a = 1\\) and \\(b = 5\\). Therefore \\[ Var(X + 5) = Var(X) + 0 = 1/4 \\] Why didn’t adding a constant to \\(X\\) change it’s variance? As is often the case, it’s easier to see why by visualizing some pmfs. In the plot below, you will see two pmfs. The first (blueish) is the pmf of \\(X\\), and the second (redish) is the pmf of \\(X+5\\) - as if we created a new random variable and pmf by adding 5 to each outcome of \\(X\\). The vertical lines represent \\(E(X)\\) and \\(E(X+5)\\) respectively. Do you see how the plot of \\(X+5\\) is simply the pmf of \\(X\\) shifted to the right? We can see the bars of \\(X+%\\) are still equivalently “spread” out but at a new location on the number line. Indeed, the values in the \\(X+5\\) pmf are just as close to their expectation compared to \\(X\\), so the variance calculation would not turn out differently (I invite you to convince yourself of this by using equation (2.7). Let’s do a few more examples using properties 1 through 3, and then we’ll move on to the fourth. Example 2.10 Given a random variable \\(X\\) with \\(Var(X) = 10\\) what is \\(Var(5X)\\) \\(Var(3)\\) \\(Var(2X + 7)\\) Let’s unpack each in turn. Property 2 says that \\(Var(aX) = a^2Var(X)\\), so \\[ Var(5X) = 25Var(X) = 250 \\] Property 1 says that \\(Var(b) = 0\\), so this one is \\(0\\) Property 3 says that \\(Var(aX + b) = a^2Var(X) + 0\\), so \\[ Var(2X + 7) = 4Var(X) + 0 = \\] The last three examples should give you a good sense of how to use properties 1 through 3 of the variance. They can each be useful and save us the rigmarole of the variance calculation. We now turn our attention to the fourth property. Notice how this property is very similar to the fourth property of the expectation - except there is an additional caveat. Our random variables must be independent. Example 2.11 We will again work with the random variables in example 2.5. Recall that \\(Z\\) was the sum of two random variables \\(X_1\\) and \\(X_2\\) - each having an expectation of 1/2. In this example, we determined the distribution and expectation of \\(Z\\). From property 4 of the expectation we found that \\(E(Z) = E(X_1 + X_2) = 1/2 + 1/2 = 1\\). Now let’s determine the variance of this random variable. Recall that the variables \\(X_1\\) and \\(X_2\\) are independent. This means that property 4 applies, and we shall use it. First let’s calculate the variance of \\(X_1\\) (which will also be the variance of \\(X_2\\)) \\[ \\begin{align} Var(X_1) = \\sum_{x}^{} &amp;= (0-\\frac{1}{2})^2p(0) + (1-\\frac{1}{2})^2p(x_2) \\\\ &amp;= 0 + \\frac{1}{4} \\\\ &amp;= \\frac{1}{4} \\end{align} \\] And, therefore, by property 4, the variance of \\(Z\\) is \\[ Var(Z) = Var(X_1 + X_2) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} \\] Now consider the circumstance where we multiply \\(X_1\\) by four before adding it to \\(X_2\\). Then the variance of the sum of \\(4X_1\\) and \\(X_2\\) would be \\[ Var(4X_1 + X_2) = 16Var(X_1) + Var(X_2) = (16) \\frac{1}{4} + 1/4 = \\frac{17}{4} \\] by property 4. And now you see the usefulness of the fourth property of the variance. We only needed the expectations and variances of our simpler variables \\(X_1\\) and \\(X_2\\) to do more complicated computations. In this case, we didn’t need to know the distribution or expectation of \\(4X_1 + X_2\\) (although I bet you could figure it out by following the procedure of example 2.5). We only need to know that they are independent. As you will see, many of the statistical procedures we will learn will assume independence to take advantage of this particular property of the variance. We close out this section with a final word of caution and a reference to the bigger picture. First the word of caution: The properties of the variance must be applied exactly how they are described. For example, without more information (which we will not persue here) it would be difficult to tell what \\(Var(X_1X_2)\\) would be. Now the bigger picture: We have spent the last two sections of this chapter talking about the expectation and variance of random variables. In our research practice, we typically do not know the expectation and variance of the population (random variable) we are trying to study. Statistics is the mathematical science of figuring these things out. Over the years some very smart people have invented some wonderful ways to study random variables. Understanding the expectation and variance shall help us develop a deeper understanding how to study random variables. We’re on the journey to learn the beginnings of this wonderful field. 2.4 Some common discrete distributions As we have seen from the previous sections, there are an infinitude of probability mass functions/discrete distributions. Nevertheless, as behavioral researchers we tend to encounter certain types of random variables and discrete distributions more frequently. In this section, we shall discuss three commonly occurring distributions of discrete random variables: The Bernoulli distribution, The binomial distribution, and the Poisson distribution. As you will see, these distributions are closely related. 2.4.1 The Bernoulli distribution A random variable - \\(X\\) - with a Bernoulli distribution (here referred to as a Bernoulli random variable) can take two outcomes - 1 or 0 - where 1 is typically mapped to a “success” or outcome of interest. We’ve already seen an example of a Bernoulli random variable - a coin flip with pmf \\[ p(x) = \\begin{cases} 1/2 &amp; x = 1 \\\\ 1/2 &amp; x = 0 \\end{cases} \\] In general, however, the probability of a success does not have to be 1/2. We can choose any probability \\(p\\) with \\(0 \\leq p \\leq 1\\), such that the pmf of our random variable \\(X\\) is \\[ p(x) = \\begin{cases} p &amp; x = 1 \\\\ 1-p &amp; x = 0 \\end{cases} \\tag{2.8} \\] That is to say, we assign \\(p\\) as the probability of success and then assign the remaining probability \\(1-p\\) to a non-success. Notice how the probabilities add to 1, e.g., \\[ p + (1-p) = 1 \\] Here we refer to \\(p\\) as a parameter of the pmf of \\(X\\). A parameter is a value that is necessary to fully specify a distribution and determine a probability from it. In this case, we need the value of \\(p\\) in order to determine the probability of a success or non-success, respectively. For example, if \\(p = 1/4\\), then the pmf of the random variable \\(X\\) is \\[ p(x) = \\begin{cases} 1/4 &amp; x = 1 \\\\ 1-1/4 = 3/4 &amp; x = 0 \\end{cases} \\] As we have seen from the previous two sections, it helps to have quick ways of calculating expectations and variances. For each of the common distributions, we will therefore present or derive equations for each. As you know, in order to get the variance, we first need to expectation. With regards to a Bernoulli random variable (\\(X\\)) with some parameter \\(p\\), the expectation is \\[ E(X) = \\sum_{x} xp(x) = 1(p) + 0(1-p) = p \\tag{2.9} \\] In words, the expectation of a Bernoulli random variable is the probability of a success. As you recall from our previous discussion, now that we have the expectation of our random variable, we have enough information to calculate the variance. This is a little more complicated, although certainly within our reach. I’ll try my best to breakdown the steps. Recall the definition of the Bernoulli distribution: \\[ p(x) = \\begin{cases} p &amp; x = 1 \\\\ 1-p &amp; x = 0 \\end{cases} \\] Now let’s calculate the variance with this as reference. \\[ \\begin{align} Var(X) = \\sum_{x} (x - \\mu)^2p(x) &amp;= (x-p)^2p(x) \\\\ &amp;= (1-p)^2p + (0-p)^2(1-p) \\\\ &amp;= (1-p)^2p + p^2(1-p) \\\\ &amp;= p(1-p)^2 + p^2(1-p) \\\\ &amp;= p(1-p)(1-p) + p(p)(1-p) \\end{align} \\] Note that we just rearranged terms from the 3rd to 4th line. On the 5th line we wrote out the squares as multiplications. Now, we’ll stop here and point something out. This equation has two terms - one before the + and one after. Each of these terms has a common factor of \\(p(1-p)\\). We’re going to use the magic of the distributive property. Watch the underlines below. \\[ \\begin{align} &amp;= \\underline{p(1-p)}(1-p) + p\\underline{(p)(1-p)} \\\\ &amp;= \\underline{p(1-p)}[(1-p) + p] \\\\ &amp;= p(1-p)[1-p+p] \\\\ &amp;= p(1-p)[1] \\\\ &amp;= p(1-p) \\end{align} \\tag{2.10} \\] The variance of a Bernoulli random variable is therefore \\(p(1-p)\\). These equations help us quickly calculate important aspects of Bernoulli distributions, but they will also help us derive the expectation and variance of our next important distribution. An example will kick us off. Example 2.12 Suppose an individual who really likes lottery tickets has a .01 chance of winning any money from their favorite scratcher. They purchase 100 lottery tickets. What is the average number of winning lottery tickets we would expect from such a large purchase? First, observe that we are talking about a Bernoulli random variable (100 of them in fact) with \\(p = 0.01\\). Our pmf is therefore \\[ p(x) = \\begin{cases} .01 &amp; x = 1 \\\\ .99 &amp; x = 0 \\end{cases} \\] From our derivations above, we also know that the expectation of our random variable is .01. We also know that (for the sake of completeness) its variance is \\[ p(1-p) = .01(1-.01) = 0.0099 \\] We are, however, asked to determine the expectation of 100 scratchers. You can think about each lottery ticket as a single random variable. That is we have a set of 100 random variables \\[ \\{X_1, X_2, X_3,...,X_{100}\\} \\] The key observation here is that their sum represents the number of successful lottery tickets. That is, if we added them all up, e.g., \\[ \\sum_{i-1}^{100} X_i \\] then the sum of their outcomes would be the number of winning lotto tickets. For example, if we had 3 winners, we would get three ones and 97 zeros. The total sum would be 3. Indeed, we know how to calculate the expectation of a sum of random variables thanks to the properties of the expectation. \\[ \\begin{align} E(\\sum_{i=1}^{100} X_i) &amp;= E(X_1) + E(X_2) + E(X_3) + ... + E(X_{100}) \\\\ &amp;= p + p + p + ... + p \\\\ &amp;= 100p \\\\ &amp;= 100(.01) = 1 \\\\ \\end{align} \\] The lotto ticket enthusiast shall expect to have 1 winning ticket on average. That’s not a stupendously lucrative hobby, but who says hobbies have to be profitable. To close out this example, let’s think about \\(\\sum_{i=1}^{100} X_i\\). The sum of these random variables is itself a random variable. Call it \\(Z\\). Note that we have already looked at similar variables as in examples 2.5 and 1.22. Although, we were able to determine the expectation of this variable \\(Z\\) using our knowledge of the expectation and Bernoulli random variables, we do not know its pmf. Well, you’re in for a treat! We shall next discuss the pmf of this random variable (and others like it). Fair warning: This is one of my favorite pmfs of all time. 2.4.2 The binomial distribution The binomial distribution is the pmf of the sum of \\(n\\) Bernoulli random variables. In our last example, we saw an example of a binomial distribution with 100 Bernoulli random variables, and we were already equipped to calculate its expectation (and variance if independence is assumed). Instead of throwing the whole distribution at you at once, we’ll work up from a single example. Example 2.13 Suppose we have three independent Bernoulli random variables - \\(X_1\\), \\(X_2\\), and \\(X_3\\) - with with \\(p = 1/3\\), and \\(1-p = 1-1/3 = 2/3\\). We would like to know the probability that they sum to the value of 1. In order to get a value of 1 from our sum, we will need exactly 1 success and 2 non-successes at the resolution of our random process (e.g. flipping three coins and hoping for heads). There are three such combinations of successes and non-successes that will yield this result. I’ve listed each in a set below. Note that the order of the digits in each outcome corresponds to variables 1, 2, and 3, respectively. \\[ \\{1\\,0\\,0, 0\\,1\\,0, 0\\,0\\,1\\} \\] Each of these outcomes in our three-variable sample space would result in a value of 1. For example, with regards to the first element, we have \\(1 + 0 + 0 = 1\\). The event \\(Y = \\{1\\,0\\,0, 0\\,1\\,0, 0\\,0\\,1\\}\\) therefore fully specifies the scenario where our Bernoulli random variables sum to 1. We have learned all the tools to calculate the probability of this event. We will first need the probability of each of the outcomes in the set \\(Y\\). Our random variables are independent, so we can multiply \\(P(X_1 = x_1)\\), \\(P(X_2 = x_2)\\), and \\(P(X_3 = x_3)\\) to determine the probability of each of the outcomes in \\(Y\\). \\[ P(\\{1\\,0\\,0\\})=(P(X_1 = 1)P(X_2 = 0)P(X_3 = 0) = 1/3 \\times 2/3 \\times 2/3 = 4/27 \\\\ P(\\{0\\,1\\,0\\})=(P(X_1 = 0)P(X_2 = 1)P(X_3 = 0) = 2/3 \\times 1/3 \\times 2/3 = 4/27 \\\\ P(\\{0\\,0\\,1\\})=(P(X_1 = 0)P(X_2 = 0)P(X_3 = 1) = 2/3 \\times 2/3 \\times 1/3 = 4/27 \\] Therefore each has the exact same probability, and by the law of addition, \\(P(Y) = 4/27 + 4/27 + 4/27 = 12/27 = 4/9\\). Importantly, note how each of the combinations has exactly the same probability. This is because each has the same number of successes and the same number of non-successes. A little algebra helps to see this. Let’s take a closer look at the combination - \\(\\{0\\,1\\,0\\}\\). \\[ \\begin{align} P(\\{1\\,0\\,0\\})=(P(X_1 = 0)P(X_2 = 1)P(X_3 = 0) &amp;= (1-p)p(1-p) \\\\ &amp;= p(1-p)(1-p) \\\\ &amp;= p^1(1-p)^2 \\\\ &amp;= (1/3)^1(1-1/3)^2 \\end{align} \\] Note that the second line from the top is just a rearrangement of factors thanks to the commutative property of multiplication. Looking carefully, we see that we have \\(p^1\\) because we have 1 success, and we have \\((1-p)^2\\) because we have two non-successes. You could rearrange the factors of the other combinations and get the same result. That is to say, each outcome has the same probability because they have the same number of successes and non-successes. There are a couple of important things to note from the last example. If we are looking to determine the pmf of a random variable \\(Z\\) - where \\(Z\\) is the sum of \\(n\\) Bernoulli random variables (e.g., \\(Z = X_1 + X_2 +...+X_n\\)) - we will need two pieces of information. The first thing we will need is the number of ways we can get a given value of \\(Z\\). In the example above, we found that there were three different ways to get a value of 1. We enumerated them by hand, but you can see that this process would be incredibly cumbersome for large \\(n\\)’s. The second thing we shall need is the probability of single outcome where \\(Z=z\\). We only need the probability of 1, because each possible combination will have the same number of successes and non-success, and therefore, the same probability. In our last example, we had three possible combinations - each with a probability of 4/27, and observe \\[ \\frac{4}{27} + \\frac{4}{27} + \\frac{4}{27} = 3\\times\\frac{4}{27} \\] as we calculated before. We shall discuss each of these - the number and the probability - in turn. First we will discuss the number of combinations, and to do this we will need a little bit of review. Specifically, we will need to know how to calculate a factorial. The factorial looks a little intimidating, but it’s not too hard to unpack. Given some integer \\(n\\), a factorial is defined as \\[ n! = n(n-1)(n-2)...(1) \\] so for example, if \\(n = 3\\) then, \\[ 3! = 3\\cdot2\\cdot1 = 6 \\] We multiply \\(3\\) by all the integers less than \\(3\\) - all the way down to 1. Now you may recall (or be wondering) what we do with \\(0!\\). A zero factorial is defined to be 1, so \\[ 0! = 1 \\] With that brief review out of the way, we now return to calculating the number of combinations. Supposing we have \\(n\\) Bernoulli random variables, and \\(z\\) successes, then the number of combinations resulting in the value \\(z\\) is calculated \\[ [Number\\: of \\: Combinations]=\\frac{n!}{z!(n-z)!} \\] This equation is often symbolically referenced as \\({n \\choose z}\\). That is \\[ {n \\choose z} = \\frac{n!}{z!(n-z)!} \\tag{2.11} \\] Example 2.14 Returning to our previous example with \\(n = 3\\), we can now quickly calculate the number of ways to get a sum of 1. \\[ \\begin{align} {3 \\choose 1} &amp;= \\frac{3!}{1!(3-1)!} \\\\ &amp;= \\frac{3 \\cdot 2 \\cdot 1}{1 \\cdot 2 \\cdot 1} \\end{align} \\] Then if we cancel out common multiples in the numerator and denominator, we get \\[ \\frac{3 \\cdot 2 \\cdot 1}{1 \\cdot 2 \\cdot 1} = \\frac{3}{1} = 3 \\] which is exactly what we were looking for. Now that we know how to calculate the number of combinations necessary for \\(z\\) successes, we shall turn our attention to the probability of a single combination. Looking back at example 2.13, a single outcome had one success and two non-successes. The probability of a single was therefore \\[ p(1-p)(1-p) = p(1-p)^2 \\] Generally speaking, we can calculate the probability of a single combination of \\(z\\) successes and \\(n-z\\) non-successes by \\[ p^z(1-p)^{n-z} \\tag{2.12} \\] In our example, we had \\(z = 1\\) and \\(n-z = 3-1 = 2\\), so we had \\(p(1-p)^2\\) for each of the outcomes. Indeed, this agrees with our algebraic manipulation at the end of example 2.13. Now we are ready to put it all together. The pmf of the sum of \\(n\\) Bernoulli random variables, each with probability of success \\(p\\), is \\[ p(x) = \\begin{cases} {n \\choose z} p^z(1-p)^{n-z} &amp; z = 0,1,...,n \\\\ 0 &amp; Everywhere\\: else \\end{cases} \\tag{2.13} \\] The \\({n \\choose z}\\) gives us the number of ways to get a value of z, and the \\(p^z(1-p)^{n-z}\\) gives us the probability for a single way. If we plugged our first example into this equation, then we get \\[ 3 \\times \\frac{4}{27} = \\frac{4}{9} \\] as we observed before when we calculated the probability by hand. The binomial distribution is often referenced as \\(B(n,p)\\) - where \\(n\\) and \\(p\\) are parameters. To indicate the distribution of \\(X\\) we would write \\(X \\sim B(n,p)\\), meaning \\(X\\) is a random variable with a binomial distribution, a.k.a., a binomial random variable. Not all distributions have a special symbol, but when they do we will also sometimes use the shorthand above. We now turn our attention to the expectation and variance of a binomial random variable. Our random variable - call it \\(Z\\) - is just the sum of \\(n\\) indepndent Bernoulli random variables with parameter \\(p\\), so the expectation is calculated as \\[ E(Z) = E(\\sum_{i=1}^{n}X_i) \\] And each of the \\(X_i\\)’s has \\(E(X_i) = p\\), so we would sum \\(p\\) a total of \\(n\\) times. That is, \\[ E(Z) = E(\\sum_{i=1}^{n}X_i) = \\sum_{i=1}^{n}p = np \\tag{2.14} \\] where \\(\\sum_{i=1}^{n}p\\) means that for each \\(X_i\\) we add another \\(p\\), and as such, \\(E(X) = np\\). The logic behind the variance is very similar. Our Bernoulli random variables are independent, so we can apply property 4 of the variance. Recall that the variance of a Bernoulli random variable is \\(p(1-p)\\), so \\[ Var(Z) = Var(\\sum_{i=1}^{n}X_i) = \\sum_{i=1}^{n}p(1-p) = np(1-p) \\tag{2.15} \\] For each random variable, we add a \\(p(1-p\\), so \\(Var(Z) = np(1-p)\\). Let’s close this section out with a few examples of the binomial distribution. Example 2.15 Suppose we are flipping 4 fair coins - each with \\(p=1/2\\) probability of flipping heads. Further suppose that we assign \\(H\\) the value of 1 and \\(T\\) the value of zero, and sum the flipped outcomes. Our new random variable has a space of \\(\\{0,1,2,3,4\\}\\). Let’s call this new (binomial) random variable \\(Z\\). In this case, what is the probability of flipping zero heads - \\(P(Z = 0)\\)? What is the probability of flipping at least 1 head - \\(P(Z \\geq 1)\\)? What are the expectation and variance of this new random variable \\(Z\\). For the first part of the question, we have \\(n = 4\\), \\(p = 1/2\\), and \\(z = 0\\), so let’s plug these into the pmf for a binomial distribution. \\[ \\begin{align} P(Z = 0) &amp;= {4 \\choose 0} 1/2^0(1-1/2)^{4-0} \\\\ &amp;= \\frac{4!}{0!(4-0)!}(1/2)^0(1-1/2)^{4-0} \\\\ &amp;= \\frac{4 \\cdot 3 \\cdot 2 \\cdot 1}{1 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1}(1/2)^0(1-1/2)^{4-0} \\end{align} \\] Notice how the factors are the same for the top and bottom of \\(\\frac{4 \\cdot 3 \\cdot 2 \\cdot 1}{1 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1}\\), so we can cancel out all the common factors and get 1. Then \\[ \\begin{align} &amp;= (1)(1/2)^0(1-1/2)^{4-0} \\\\ &amp;= (1)(1/2)^0(1/2)^{4} \\\\ &amp;= (1/2)^{4} = 1/16 \\end{align} \\] where the last step follows because a number raised to the 0th power is 1. Therefore, \\(P(Z = 0) = 1/16\\). The second part follows from this first part of the question. Let’s call the space of our random variable \\(S = \\{0,1,2,3,4\\}\\). Note that the set \\(\\{Z \\geq 1\\} = \\{1,2,3,4\\}\\) is the complement of \\(\\{Z = 0\\}\\). Therefore, we can use equation (1.11). \\[ P(Z \\geq 1) = 1 - P(Z = 0) = 1 - 1/16 = 15/16 \\] Finally the expectation and variance of this random variable is \\[ E(Z) = np = 4(1/2) = 2 Var(Z) = np(1-p) = 4(1/2)(1-1/2) = 1 \\] respectively. Example 2.16 Suppose there are 30 students in a particularly difficult course at College of Learning (a preeminent college). Over many many years, researchers at the college determined that the probability of passing this very difficult course is about .25. What is the probability of 0 students passing the course, and what is the probability of at least 1 student passing the course? What is the expectation of the number of students passing? Here we have \\(p = .25\\) and \\(n=30\\). Our binomial distribution therefore looks like this: \\[ p(x) = \\begin{cases} {30 \\choose z} .25^z(1-.25)^{30-z} &amp; z = 0,1,...,30 \\\\ 0 &amp; Everywhere\\: else \\end{cases} \\tag{2.13} \\] With regards to the first part of the question, we have \\(z = 0\\), and the probability is calculated \\[ \\begin{align} P(Z = 0) &amp;= {30 \\choose 0} .25^0(1-.25)^{30-0} \\\\ &amp;=\\frac{30!}{0!(30-0)!}(.75)^{30} \\\\ &amp;=\\frac{30!}{30!}(.75)^{30} = .00019 \\end{align} \\] \\(P(Z=0) = 0.00019\\) (after some rounding), and we can proceed with the second part as we did in the last example. \\[ P(Z \\geq 1) = 1 - P(Z=0) = 1 - 0.00019 = 0.99981 \\] Finally, the expectation of our random variable is \\(E(Z) = .25(30) = 7.5\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
